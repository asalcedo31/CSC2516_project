{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pruning_units.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "b031iPnkif-H"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "g0u7P1-w6lxN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ]
    },
    {
      "metadata": {
        "id": "yqrwA42L7mgF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision as tv\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "\n",
        "\n",
        "class DatasetManager:\n",
        "    \n",
        "    def __init__(self, dataset = 'cifar10', percent_data = 10.0, percent_val = 20.0, data_path = './data'):\n",
        "        \n",
        "        # 'dataset' can be 'hymenoptera', 'cifar10', or 'cifar100'.\n",
        "        # 'percent_data' is the percentage of the full training set to be used.\n",
        "        # 'percent_val' is the percentage of the *loaded* training set to be used as validation data.\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.data_path = data_path\n",
        "        self.percent_data = percent_data\n",
        "        self.percent_val = percent_val\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "            \n",
        "        elif self.dataset == 'cifar10' or self.dataset == 'cifar100':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def ImportDataset(self, batch_size=5):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "        \n",
        "            self.trainset = tv.datasets.ImageFolder(root=self.data_path,\n",
        "                             transform=self.transform)\n",
        "        \n",
        "        # todo\n",
        "        \n",
        "        elif self.dataset == 'cifar10':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR10(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR10(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "        \n",
        "        elif self.dataset == 'cifar100':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR100(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR100(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "             \n",
        "        self.SplitData();\n",
        "        self.GenerateLoaders();\n",
        "                \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def SplitData(self):\n",
        "        \n",
        "        len_full = self.trainset.__len__()\n",
        "        len_train = int(np.round(len_full*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.trainset = torch.utils.data.random_split(self.trainset, (len_full-len_train, len_train))\n",
        "        \n",
        "        len_val = int(np.round(len_train*self.percent_val/100.0))\n",
        "        len_train = len_train - len_val\n",
        "        \n",
        "        self.valset, self.trainset = torch.utils.data.random_split(self.trainset, (len_val, len_train))\n",
        "         \n",
        "        len_full_test = self.testset.__len__()\n",
        "        len_test = int(np.round(len_full_test*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.testset = torch.utils.data.random_split(self.testset, (len_full_test-len_test, len_test))\n",
        "\n",
        "        print('\\nFull training set size: {}'.format(len_full))\n",
        "        print('Full test set size: {}'.format(len_full_test))\n",
        "        print('\\nActive training set size: {}'.format(len_train))\n",
        "        print('Active validation set size: {}'.format(len_val))\n",
        "        print('Active test set size: {}'.format(len_test))\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def GenerateLoaders(self):\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.val_loader = torch.utils.data.DataLoader(self.valset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)          \n",
        "            \n",
        "        return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fHyU0vpTK2-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "195ce7fa-096c-416e-bab7-cf23698823cf"
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "\n",
        "dat = DatasetManager('cifar10', 10.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 4000\n",
            "Active validation set size: 1000\n",
            "Active test set size: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SLDvMNwVEPgV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruning Functions"
      ]
    },
    {
      "metadata": {
        "id": "Ye1vK3FmREgq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Class that contains various settings pertaining to how filters are pruned\n",
        "\n",
        "# Constants that define possible pruning metrics\n",
        "WEIGHT_NORM = 1\n",
        "ACT_NORM = 2\n",
        "\n",
        "class UnitPruningSettings:\n",
        "    \n",
        "    def __init__(self, idx_layer, idx_filter, N_prune = 1, p = 2, pruning_metric = WEIGHT_NORM):\n",
        "        \n",
        "        self.N_prune = N_prune # Number of filters allowed to be pruned in one pass\n",
        "        self.idx_filter = idx_filter # Indices of the N_prune filters\n",
        "        self.idx_layer = idx_layer # Current layer under consideration\n",
        "        self.p = p # p-norm to use when computing which filters to remove\n",
        "        self.pruning_metric = pruning_metric\n",
        "        \n",
        "        return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HnBts6uMEU9t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# These functions were adapted from https://github.com/jacobgil/pytorch-pruning/blob/master/prune.py\n",
        "\n",
        "def replace_layers(model, i, idx, layers):\n",
        "\tif i in idx:\n",
        "\t\treturn layers[idx.index(i)]\n",
        "\treturn model[i]\n",
        "\n",
        "# # Function to prune a given convolution layer in the model provided.\n",
        "# # Input \"idx_layers\" is the global index of the convolution layer to be pruned.\n",
        "# # Input \"prune_settings\" is a data structure containing information on how pruning is performed.\n",
        "# def PruneConvLayers(model, prune_settings):\n",
        "    \n",
        "#     # Strategy: in order to prune a particular layer, the output of the previous layer \n",
        "#     # and the inputs to the next layer must also be altered accordingly.\n",
        "\t\n",
        "#     # Extract pruning settings for convenience\n",
        "#     # Note that \"N_prune\" *consecutive* filters will get pruned\n",
        "# #     N_prune = prune_settings.N_prune\n",
        "#     N_prune = 1\n",
        "#     idx_filter = prune_settings.idx_filter\n",
        "#     idx_layer = prune_settings.idx_layer\n",
        "    \n",
        "#     if idx_layer >= len(model.features._modules.items()):\n",
        "#         return\n",
        "        \n",
        "#     # Extract the layer of the model currently being pruned\n",
        "#     _, conv = list(model.features._modules.items())[idx_layer]\n",
        "# #     _, conv = model.features._modules.items()(idx_layer)\n",
        "    \n",
        "#     if idx_filter >= conv.out_channels:\n",
        "#         return\n",
        "\n",
        "        \n",
        "#     # To keep track of the succeeding convolution layer\n",
        "#     next_conv = None\n",
        "#     offset = 1\n",
        "    \n",
        "#     # Figure out how many layers after this one are NOT conv layers, in order to skip pruning them\n",
        "#     while idx_layer + offset < len(model.features._modules.items()):\n",
        "        \n",
        "#         res =  list(model.features._modules.items())[idx_layer + offset]\n",
        "#         if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "#             next_name, next_conv = res\n",
        "#             break\n",
        "#         offset = offset + 1\n",
        "    \n",
        "#     # Create a new, replacement conv layer to remove a given number of filters.\n",
        "#     # The rest of its settings should remain the same as the original conv layer.\n",
        "#     new_conv = torch.nn.Conv2d(in_channels = conv.in_channels,\n",
        "#                                out_channels = conv.out_channels - N_prune,\n",
        "# \t\t\t                   kernel_size = conv.kernel_size,\n",
        "#                                stride = conv.stride,\n",
        "#                                padding = conv.padding,\n",
        "#                                dilation = conv.dilation,\n",
        "#                                groups = conv.groups,\n",
        "#                                bias = True)\n",
        "    \n",
        "#     new_conv.bias = conv.bias\n",
        "    \n",
        "#     # Copy over the weights to the new conv layer, except the ones corresponding to the filter to be removed\n",
        "#     old_weights = conv.weight.data.cpu().numpy()\n",
        "#     new_weights = new_conv.weight.data.cpu().numpy()\n",
        "\n",
        "#     # This copies the set of filters up to and excluding the filters to be removed\n",
        "#     print(idx_filter)\n",
        "#     print(conv.weight.data.size())\n",
        "#     new_weights[: idx_filter, :, :, :] = old_weights[: idx_filter, :, :, :]\n",
        "\n",
        "#     # This copies the filters after and excluding the filters to be removed\n",
        "#     new_weights[idx_filter :, :, :, :] = old_weights[idx_filter + N_prune :, :, :, :]\n",
        "\n",
        "#     # Update weight data of the new conv layer\n",
        "#     new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "# #     new_conv.weight.data = torch.from_numpy(new_weights)\n",
        "\n",
        "#     # Now do the same thing for biases\n",
        "#     old_biases = conv.bias.data.cpu().numpy()\n",
        "\n",
        "#     new_biases = np.zeros(shape = (old_biases.shape[0] - N_prune), dtype = np.float32)\n",
        "#     new_biases[:idx_filter] = old_biases[:idx_filter]\n",
        "#     new_biases[idx_filter :] = old_biases[idx_filter + N_prune :]\n",
        "#     new_conv.bias.data = torch.from_numpy(new_biases).cuda()\n",
        "# #     new_conv.bias.data = torch.from_numpy(new_biases)\n",
        "    \n",
        "#     # If there is a succeeding conv layer, adjust its input units and weights accordingly\n",
        "#     if next_conv != None:\n",
        "        \n",
        "#         next_new_conv = torch.nn.Conv2d(in_channels = next_conv.in_channels - N_prune,\n",
        "#                                         out_channels =  next_conv.out_channels,\n",
        "#                                         kernel_size = next_conv.kernel_size,\n",
        "#                                         stride = next_conv.stride,\n",
        "#                                         padding = next_conv.padding,\n",
        "#                                         dilation = next_conv.dilation,\n",
        "#                                         groups = next_conv.groups,\n",
        "#                                         bias = True)\n",
        "        \n",
        "#         next_new_conv.bias = next_conv.bias\n",
        "\n",
        "#         old_weights = next_conv.weight.data.cpu().numpy()\n",
        "#         new_weights = next_new_conv.weight.data.cpu().numpy()\n",
        "\n",
        "#         new_weights[:, : idx_filter, :, :] = old_weights[:, : idx_filter, :, :]\n",
        "#         new_weights[:, idx_filter : , :, :] = old_weights[:, idx_filter + N_prune :, :, :]\n",
        "#         next_new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "# #         next_new_conv.weight.data = torch.from_numpy(new_weights)\n",
        "\n",
        "#         next_new_conv.bias.data = next_conv.bias.data\n",
        "\n",
        "#         # Update the actual model by replacing the existing filters with the new ones\n",
        "#         features = torch.nn.Sequential(\n",
        "#                 *(replace_layers(model.features, i, [idx_layer, idx_layer + offset], \\\n",
        "#                     [new_conv, next_new_conv]) for i, _ in enumerate(model.features)))\n",
        "#         del model.features\n",
        "#         del conv\n",
        "\n",
        "#         model.features = features\n",
        "    \n",
        "#     else:\n",
        "\n",
        "#         # This is the last conv layer. This affects the first linear layer of the classifier.\n",
        "#         model.features = torch.nn.Sequential(\n",
        "#                 *(replace_layers(model.features, i, [idx_layer], \\\n",
        "#                     [new_conv]) for i, _ in enumerate(model.features)))\n",
        "#         idx_layer = 0\n",
        "#         old_linear_layer = None\n",
        "\n",
        "#         for _, module in model.classifier._modules.items():\n",
        "#             if isinstance(module, torch.nn.Linear):\n",
        "#                 old_linear_layer = module\n",
        "#                 break\n",
        "#             idx_layer = idx_layer + 1\n",
        "\n",
        "#         if old_linear_layer == None:\n",
        "#             raise BaseException(\"No linear layer found in classifier.\")\n",
        "#         params_per_input_channel = old_linear_layer.in_features / conv.out_channels\n",
        "\n",
        "#         new_linear_layer = \\\n",
        "#             torch.nn.Linear(int(old_linear_layer.in_features - params_per_input_channel), \n",
        "#                 old_linear_layer.out_features)\n",
        "\n",
        "#         old_weights = old_linear_layer.weight.data.cpu().numpy()\n",
        "#         new_weights = new_linear_layer.weight.data.cpu().numpy()\t \t\n",
        "\n",
        "#         new_weights[:, : int(idx_filter * params_per_input_channel)] = \\\n",
        "#             old_weights[:, : int(idx_filter * params_per_input_channel)]\n",
        "#         new_weights[:, int(idx_filter * params_per_input_channel) :] = \\\n",
        "#             old_weights[:, int((idx_filter + N_prune) * params_per_input_channel) :]\n",
        "\n",
        "#         new_linear_layer.bias.data = old_linear_layer.bias.data\n",
        "\n",
        "#         new_linear_layer.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "# #         new_linear_layer.weight.data = torch.from_numpy(new_weights)\n",
        "\n",
        "#         classifier = torch.nn.Sequential(\n",
        "#             *(replace_layers(model.classifier, i, [idx_layer], \\\n",
        "#                 [new_linear_layer]) for i, _ in enumerate(model.classifier)))\n",
        "\n",
        "#         del model.classifier\n",
        "#         del next_conv\n",
        "#         del conv\n",
        "#         model.classifier = classifier\n",
        "        \n",
        "#     return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to prune a given convolution layer in the model provided.\n",
        "# Input \"idx_layers\" is the global index of the convolution layer to be pruned.\n",
        "# Input \"prune_settings\" is a data structure containing information on how pruning is performed.\n",
        "def PruneConvLayers(model, prune_settings):\n",
        "    \n",
        "    # Strategy: in order to prune a particular layer, the output of the previous layer \n",
        "    # and the inputs to the next layer must also be altered accordingly.\n",
        "\t\n",
        "    # Extract pruning settings for convenience\n",
        "    # Note that \"N_prune\" *consecutive* filters will get pruned\n",
        "    N_prune = prune_settings.N_prune\n",
        "#     N_prune = 1\n",
        "    idx_filter = prune_settings.idx_filter\n",
        "    idx_layer = prune_settings.idx_layer\n",
        "    \n",
        "    if idx_layer >= len(model.features._modules.items()):\n",
        "        return\n",
        "        \n",
        "    # Extract the layer of the model currently being pruned\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    \n",
        "\n",
        "    # In case the list of target filters to delete has out-of-range entries, detect and ignore them\n",
        "    del_filters = []\n",
        "    for kk in range(0, len(idx_filter)):\n",
        "        if idx_filter[kk] >= conv.out_channels:\n",
        "            del_filters.extend(kk)\n",
        "    \n",
        "    if (len(del_filters) > 0):\n",
        "        idx_filter = np.delete(idx_filter, del_filters, 0)\n",
        "        N_prune = len(idx_filter)\n",
        "        prune_settings.N_prune = N_prune\n",
        "        print(\"[WARNING] Encountered an out-of-range target filter; it will be ignored.\")\n",
        "    \n",
        "        \n",
        "    # To keep track of the succeeding convolution layer\n",
        "    next_conv = None\n",
        "    offset = 1\n",
        "    \n",
        "    # Figure out how many layers after this one are NOT conv layers, in order to skip pruning them\n",
        "    while idx_layer + offset < len(model.features._modules.items()):\n",
        "        \n",
        "        res =  list(model.features._modules.items())[idx_layer + offset]\n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            next_name, next_conv = res\n",
        "            break\n",
        "        offset = offset + 1\n",
        "    \n",
        "    # Create a new, replacement conv layer to remove a given number of filters.\n",
        "    # The rest of its settings should remain the same as the original conv layer.\n",
        "    new_conv = torch.nn.Conv2d(in_channels = conv.in_channels,\n",
        "                               out_channels = conv.out_channels - N_prune,\n",
        "\t\t\t                   kernel_size = conv.kernel_size,\n",
        "                               stride = conv.stride,\n",
        "                               padding = conv.padding,\n",
        "                               dilation = conv.dilation,\n",
        "                               groups = conv.groups,\n",
        "                               bias = True)\n",
        "    \n",
        "    new_conv.bias = conv.bias\n",
        "    \n",
        "    # Copy over the weights to the new conv layer, except the ones corresponding to the filter to be removed\n",
        "    old_weights = conv.weight.data.cpu().numpy()\n",
        "    new_weights = new_conv.weight.data.cpu().numpy()\n",
        "\n",
        "    # Copy over the set of filters, excluding the ones to be removed\n",
        "    new_weights_temp = np.copy(old_weights)\n",
        "    new_weights_temp = np.delete(new_weights_temp, idx_filter, 0)\n",
        "    new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "    # Update weight data of the new conv layer\n",
        "    new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "    \n",
        "    # Now do the same thing for biases\n",
        "    old_biases = conv.bias.data.cpu().numpy()\n",
        "    new_biases = np.zeros(shape=(old_biases.shape[0] - N_prune), dtype=np.float32)\n",
        "    \n",
        "    new_biases_temp = np.copy(old_biases)\n",
        "    new_biases_temp = np.delete(new_biases_temp, idx_filter, 0)\n",
        "    new_biases[:] = new_biases_temp[:]\n",
        "        \n",
        "    new_conv.bias.data = torch.from_numpy(new_biases).cuda()\n",
        "    \n",
        "    # If there is a succeeding conv layer, adjust its input units and weights accordingly\n",
        "    if next_conv != None:\n",
        "        \n",
        "        next_new_conv = torch.nn.Conv2d(in_channels = next_conv.in_channels - N_prune,\n",
        "                                        out_channels =  next_conv.out_channels,\n",
        "                                        kernel_size = next_conv.kernel_size,\n",
        "                                        stride = next_conv.stride,\n",
        "                                        padding = next_conv.padding,\n",
        "                                        dilation = next_conv.dilation,\n",
        "                                        groups = next_conv.groups,\n",
        "                                        bias = True)\n",
        "        \n",
        "        next_new_conv.bias = next_conv.bias\n",
        "\n",
        "        old_weights = next_conv.weight.data.cpu().numpy()\n",
        "        new_weights = next_new_conv.weight.data.cpu().numpy()\n",
        "        \n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_filter, 1)\n",
        "        new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "        next_new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        # Now do the same thing for biases\n",
        "        next_new_conv.bias.data = next_conv.bias.data\n",
        "\n",
        "        # Update the actual model by replacing the existing filters with the new ones\n",
        "        features = torch.nn.Sequential(\n",
        "                *(replace_layers(model.features, i, [idx_layer, idx_layer + offset], \\\n",
        "                    [new_conv, next_new_conv]) for i, _ in enumerate(model.features)))\n",
        "        del model.features\n",
        "        del conv\n",
        "\n",
        "        model.features = features\n",
        "    \n",
        "    else:\n",
        "\n",
        "        # This is the last conv layer. This affects the first linear layer of the classifier.\n",
        "        model.features = torch.nn.Sequential(*(replace_layers(model.features, i, [idx_layer], [new_conv]) for i, _ in enumerate(model.features)))\n",
        "        idx_layer = 0\n",
        "        old_linear_layer = None\n",
        "\n",
        "        for _, module in model.classifier._modules.items():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                old_linear_layer = module\n",
        "                break\n",
        "            idx_layer = idx_layer + 1\n",
        "\n",
        "        if old_linear_layer == None:\n",
        "            raise BaseException(\"No linear layer found in classifier.\")\n",
        "            \n",
        "        params_per_input_channel = int(old_linear_layer.in_features/conv.out_channels)\n",
        "\n",
        "        new_linear_layer = torch.nn.Linear(old_linear_layer.in_features - N_prune*params_per_input_channel, \n",
        "                                           old_linear_layer.out_features)\n",
        "\n",
        "        old_weights = old_linear_layer.weight.data.cpu().numpy()\n",
        "        new_weights = new_linear_layer.weight.data.cpu().numpy()\t \t\n",
        "\n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        idx_expanded = np.zeros(shape=(N_prune*params_per_input_channel))\n",
        "        \n",
        "        for kk in range(0, len(idx_filter)):\n",
        "            idx_expanded[kk*params_per_input_channel:kk*params_per_input_channel+params_per_input_channel] = np.arange(idx_filter[kk]*params_per_input_channel, idx_filter[kk]*params_per_input_channel + params_per_input_channel)\n",
        "\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_expanded.astype(int), 1)\n",
        "        new_weights[:, :] = new_weights_temp[:, :]\n",
        "        \n",
        "        new_linear_layer.bias.data = old_linear_layer.bias.data\n",
        "        new_linear_layer.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        classifier = torch.nn.Sequential(*(replace_layers(model.classifier, i, [idx_layer], [new_linear_layer]) for i, _ in enumerate(model.classifier)))\n",
        "\n",
        "        del model.classifier\n",
        "        del next_conv\n",
        "        del conv\n",
        "        model.classifier = classifier\n",
        "        \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MnKIkiz8QZXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "71c9689b-6550-4a87-c3e8-cf6f61e23164"
      },
      "cell_type": "code",
      "source": [
        "# Test pruning\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.train()\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10, 12, 15, 16, 21), \n",
        "                                     N_prune=5, p=2, pruning_metric=WEIGHT_NORM)\n",
        "# prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10), \n",
        "#                                      N_prune=1, p=2, pruning_metric=WEIGHT_NORM)\n",
        "\n",
        "t0 = time.time()\n",
        "model = PruneConvLayers(model, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "Pruning took 3.9255526065826416 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yn_VPoXdbmPg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Function"
      ]
    },
    {
      "metadata": {
        "id": "kvekshfziL_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, dat, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    \n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                dataloader = dat.train_loader\n",
        "                dataset_size = dat.trainset.__len__()\n",
        "                \n",
        "                model.train()  # Set model to training mode\n",
        "                \n",
        "            else:\n",
        "                \n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                dataloader = dat.val_loader\n",
        "                dataset_size = dat.valset.__len__()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloader:\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vpIOd-wiTYf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline Model Setup"
      ]
    },
    {
      "metadata": {
        "id": "6w6D3bRfbrvQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = models.vgg16(pretrained=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_5AJjslGcFHi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Baseline Model"
      ]
    },
    {
      "metadata": {
        "id": "9s7W8AMicIlK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "9b47ea12-4748-4f4d-8236-87fe2e7efa24"
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "Epoch 1/25\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ecba4ebf2561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-49276ab71e4d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dat, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "b031iPnkif-H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruned Model Setup - Single Filter"
      ]
    },
    {
      "metadata": {
        "id": "IbwrbOviikWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a55f15d5-2f8f-48fa-c5fc-06e759029776"
      },
      "cell_type": "code",
      "source": [
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(28, 10, 1)\n",
        "\n",
        "t0 = time.time()\n",
        "model = PruneConvLayers(model, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning took 1.4709546566009521 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Sg4OKhtHjCrj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Pruned Model - Single Filter"
      ]
    },
    {
      "metadata": {
        "id": "AKXgtuR0jJf2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2366
        },
        "outputId": "c10dc6be-5e8a-4268-cbaf-42b02313244b"
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 2.8876 Acc: 0.0875\n",
            "val Loss: 2.3552 Acc: 0.1900\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 2.4806 Acc: 0.1275\n",
            "val Loss: 2.2240 Acc: 0.1800\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 2.3176 Acc: 0.1900\n",
            "val Loss: 2.1939 Acc: 0.2000\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 2.2381 Acc: 0.1875\n",
            "val Loss: 2.1577 Acc: 0.2000\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 2.1509 Acc: 0.2350\n",
            "val Loss: 2.1055 Acc: 0.1900\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 2.1236 Acc: 0.2425\n",
            "val Loss: 2.0256 Acc: 0.2600\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 1.8307 Acc: 0.3250\n",
            "val Loss: 1.8670 Acc: 0.2900\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 1.7686 Acc: 0.3500\n",
            "val Loss: 1.9131 Acc: 0.3200\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 1.7535 Acc: 0.3400\n",
            "val Loss: 1.8303 Acc: 0.3400\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 1.6652 Acc: 0.4000\n",
            "val Loss: 1.8468 Acc: 0.3600\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 1.6122 Acc: 0.4050\n",
            "val Loss: 1.7168 Acc: 0.4300\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 1.5730 Acc: 0.4050\n",
            "val Loss: 1.8190 Acc: 0.3100\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 1.6161 Acc: 0.4000\n",
            "val Loss: 1.7363 Acc: 0.3600\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 1.4759 Acc: 0.4650\n",
            "val Loss: 1.6957 Acc: 0.4200\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 1.5155 Acc: 0.4475\n",
            "val Loss: 1.8601 Acc: 0.3300\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "train Loss: 1.5061 Acc: 0.4800\n",
            "val Loss: 1.8389 Acc: 0.3500\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "train Loss: 1.5602 Acc: 0.4375\n",
            "val Loss: 1.7337 Acc: 0.3400\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "train Loss: 1.5198 Acc: 0.4625\n",
            "val Loss: 1.7519 Acc: 0.3600\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n",
            "train Loss: 1.4680 Acc: 0.4800\n",
            "val Loss: 1.7563 Acc: 0.3100\n",
            "\n",
            "Epoch 19/24\n",
            "----------\n",
            "train Loss: 1.4432 Acc: 0.5250\n",
            "val Loss: 1.6784 Acc: 0.4000\n",
            "\n",
            "Epoch 20/24\n",
            "----------\n",
            "train Loss: 1.4341 Acc: 0.4700\n",
            "val Loss: 1.8538 Acc: 0.2700\n",
            "\n",
            "Epoch 21/24\n",
            "----------\n",
            "train Loss: 1.5188 Acc: 0.4375\n",
            "val Loss: 1.8839 Acc: 0.3100\n",
            "\n",
            "Epoch 22/24\n",
            "----------\n",
            "train Loss: 1.4668 Acc: 0.4550\n",
            "val Loss: 1.7525 Acc: 0.3800\n",
            "\n",
            "Epoch 23/24\n",
            "----------\n",
            "train Loss: 1.5095 Acc: 0.4525\n",
            "val Loss: 1.8141 Acc: 0.3200\n",
            "\n",
            "Epoch 24/24\n",
            "----------\n",
            "train Loss: 1.4857 Acc: 0.4775\n",
            "val Loss: 1.7037 Acc: 0.4900\n",
            "\n",
            "Training complete in 14m 40s\n",
            "Best val Acc: 0.490000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1slyZpAjY0yQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Functions to Compute Pruning Metrics"
      ]
    },
    {
      "metadata": {
        "id": "ryg9vNTsY8Oc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "3c9901cb-cb63-493d-ed7f-f91ed9e9e326"
      },
      "cell_type": "code",
      "source": [
        "# Function to compute the p-norm of weights in all filters of a given layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DWeightNorms(model, idx_layer, p):\n",
        "    \n",
        "    # Extract the layer of the model currently being considered\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    \n",
        "    weights = conv.weight.data\n",
        "\n",
        "    # Compute norms of each filter\n",
        "    norms = weights.norm(p, dim=2)\n",
        "    norms = norms.norm(p, dim=2)\n",
        "    norms = norms.norm(p, dim=1)\n",
        "    \n",
        "    return norms\n",
        "\n",
        "# Test\n",
        "norms_1 = ComputeConv2DWeightNorms(model, 0, 1)\n",
        "norms_2 = ComputeConv2DWeightNorms(model, 0, 2)\n",
        "norms_inf = ComputeConv2DWeightNorms(model, 0, float('inf'))\n",
        "\n",
        "print(norms_1.size())\n",
        "# print(norms_1)\n",
        "\n",
        "# Compute values and indices of the k smallest norms\n",
        "k = 2\n",
        "n1_botk, ind_n1_botk = torch.topk(norms_1, k, 0, largest=False, sorted=True, out=None)\n",
        "print(n1_botk)\n",
        "print(len(ind_n1_botk))\n",
        "print(ind_n1_botk)\n",
        "\n",
        "\n",
        "# Function to compute the p-norm of activations of all filters of a given layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DActNorms(model, idx_layer, p):\n",
        "\n",
        "    \n",
        "    \n",
        "    return\n",
        "    \n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64])\n",
            "tensor([1.0097, 1.9274])\n",
            "2\n",
            "tensor([54, 50])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RvJela053WOj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to iterate through all conv2D layers of the network and determine \n",
        "# filters to be pruned, and then carry out the pruning.\n",
        "def PruneAllConv2DLayers(model, prune_settings):\n",
        "    \n",
        "    # Extract pruning settings for convenience\n",
        "    # Note that \"N_prune\" *consecutive* filters will get pruned\n",
        "    N_prune = prune_settings.N_prune\n",
        "    p = prune_settings.p\n",
        "    pruning_metric = prune_settings.pruning_metric\n",
        "    \n",
        "    # Find the N_prune filters to remove\n",
        "    ii = 1\n",
        "    while ii < len(model.features._modules.items()):\n",
        "        \n",
        "        res = list(model.features._modules.items())[ii]\n",
        "        \n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            # Compute values and indices of the N_prune smallest norms\n",
        "            if pruning_metric == WEIGHT_NORM:\n",
        "                norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "            elif pruning_metric == ACT_NORM:\n",
        "                norms = ComputeConv2DActNorms(model, ii, p)\n",
        "            \n",
        "            n_botk, ind_botk = torch.topk(norms, N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            \n",
        "            # Prune filters one at a time (todo: prune all at once)\n",
        "            prune_settings.idx_layer = ii\n",
        "            prune_settings.idx_filter = ind_botk.cpu().numpy()\n",
        "            \n",
        "            model = PruneConvLayers(model, prune_settings)\n",
        "            \n",
        "#             jj = 0\n",
        "#             while jj < len(ind_botk):\n",
        "#                 print(ind_botk)\n",
        "#                 prune_settings.idx_filter = ind_botk[jj]\n",
        "# #                 print(\"Pruning filter {} in layer {}\".format(ind_botk[jj], ii))\n",
        "#                 model = PruneConvLayers(model, prune_settings)\n",
        "#                 jj = jj + 1\n",
        "                \n",
        "        ii = ii + 1\n",
        "    \n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_QHn-nWjih5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruned Model Setup - Several Filters"
      ]
    },
    {
      "metadata": {
        "id": "wLvxYCUmeh3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "f39d9757-e8a4-4055-c47d-369c7b96262b"
      },
      "cell_type": "code",
      "source": [
        "# Test pruning all layers\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.train()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(28, 10, N_prune = 4, p = 2, pruning_metric = WEIGHT_NORM)\n",
        "\n",
        "t0 = time.time()\n",
        "model = PruneAllConv2DLayers(model, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "[WARNING] Encountered an out-of-range target filter; it will be ignored.\n",
            "Pruning took 4.356287717819214 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SHixvYRUjpgl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Pruned Model - Several Filters"
      ]
    },
    {
      "metadata": {
        "id": "5c5f6i-1jxFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "743db61d-f301-47db-ae29-a1034d45080d"
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "Epoch 1/25\n",
            "----------\n",
            "train Loss: 5.1571 Acc: 0.0925\n",
            "val Loss: 2.8524 Acc: 0.0700\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss: 2.5442 Acc: 0.1225\n",
            "val Loss: 2.5077 Acc: 0.1800\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-ecba4ebf2561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-49276ab71e4d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dat, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "eWF7Rkytr0D8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Iterative Pruning"
      ]
    },
    {
      "metadata": {
        "id": "hphM-tuDr8Yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "a3c10f91-df3b-4c1a-a56c-d7535594a7da"
      },
      "cell_type": "code",
      "source": [
        "# ====== Model setup ======\n",
        "\n",
        "model = models.vgg16(pretrained=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# ====== Pruning setup ======\n",
        "\n",
        "N_prune = 4\n",
        "p = 2\n",
        "prune_settings = UnitPruningSettings(0, 0, N_prune, p)\n",
        "\n",
        "\n",
        "# ====== Begin training ======\n",
        "\n",
        "N_iter_outer = 5\n",
        "N_iter_inner = 5\n",
        "\n",
        "# Import data\n",
        "dat = DatasetManager(dataset='cifar10', percent_data=1.0, percent_val=20.0)\n",
        "dat.ImportDataset(batch_size=5)\n",
        "\n",
        "for ii in range(0, N_iter_outer):\n",
        "    \n",
        "    print(\"\\n------ Outer iteration {}/{} ------\".format(ii+1, N_iter_outer))\n",
        "#     t0 = time.time()\n",
        "\n",
        "    # ------ Prune current model ------\n",
        "    \n",
        "    model = PruneAllConv2DLayers(model, prune_settings)\n",
        "    new_model = copy.deepcopy(model)\n",
        "    model = new_model\n",
        "\n",
        "    # ------ Train current model ------\n",
        "    \n",
        "#     # Import data\n",
        "#     dat = DatasetManager(dataset='cifar10', percent_data=1.0, percent_val=20.0)\n",
        "#     dat.ImportDataset(batch_size=5)\n",
        "    \n",
        "    # Update optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=N_iter_inner)\n",
        "        \n",
        "#     print (\"Pruning took {} s\".format(time.time() - t0))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "------ Outer iteration 1/5 ------\n",
            "Epoch 1/5\n",
            "----------\n",
            "train Loss: 4.2705 Acc: 0.0650\n",
            "val Loss: 2.9538 Acc: 0.0400\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}