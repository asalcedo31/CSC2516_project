{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pruning_units.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Yn_VPoXdbmPg",
        "_5AJjslGcFHi",
        "b031iPnkif-H",
        "Sg4OKhtHjCrj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "g0u7P1-w6lxN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ]
    },
    {
      "metadata": {
        "id": "yqrwA42L7mgF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision as tv\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "\n",
        "\n",
        "class DatasetManager:\n",
        "    \n",
        "    def __init__(self, dataset = 'cifar10', percent_data = 10.0, percent_val = 20.0, data_path = './data'):\n",
        "        \n",
        "        # 'dataset' can be 'hymenoptera', 'cifar10', or 'cifar100'.\n",
        "        # 'percent_data' is the percentage of the full training set to be used.\n",
        "        # 'percent_val' is the percentage of the *loaded* training set to be used as validation data.\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.data_path = data_path\n",
        "        self.percent_data = percent_data\n",
        "        self.percent_val = percent_val\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "            \n",
        "        elif self.dataset == 'cifar10' or self.dataset == 'cifar100':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def ImportDataset(self, batch_size=5):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "        \n",
        "            self.trainset = tv.datasets.ImageFolder(root=self.data_path,\n",
        "                             transform=self.transform)\n",
        "        \n",
        "        # todo\n",
        "        \n",
        "        elif self.dataset == 'cifar10':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR10(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR10(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "        \n",
        "        elif self.dataset == 'cifar100':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR100(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR100(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "             \n",
        "        self.SplitData();\n",
        "        self.GenerateLoaders();\n",
        "                \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def SplitData(self):\n",
        "        \n",
        "        len_full = self.trainset.__len__()\n",
        "        len_train = int(np.round(len_full*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.trainset = torch.utils.data.random_split(self.trainset, (len_full-len_train, len_train))\n",
        "        \n",
        "        len_val = int(np.round(len_train*self.percent_val/100.0))\n",
        "        len_train = len_train - len_val\n",
        "        \n",
        "        self.valset, self.trainset = torch.utils.data.random_split(self.trainset, (len_val, len_train))\n",
        "         \n",
        "        len_full_test = self.testset.__len__()\n",
        "        len_test = int(np.round(len_full_test*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.testset = torch.utils.data.random_split(self.testset, (len_full_test-len_test, len_test))\n",
        "\n",
        "        print('\\nFull training set size: {}'.format(len_full))\n",
        "        print('Full test set size: {}'.format(len_full_test))\n",
        "        print('\\nActive training set size: {}'.format(len_train))\n",
        "        print('Active validation set size: {}'.format(len_val))\n",
        "        print('Active test set size: {}'.format(len_test))\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def GenerateLoaders(self):\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.val_loader = torch.utils.data.DataLoader(self.valset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)          \n",
        "            \n",
        "        return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fHyU0vpTK2-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "966c27ca-5cfc-4b11-a7e2-afd245948c65"
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "\n",
        "dat = DatasetManager('cifar10', 10.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 170205184/170498071 [00:43<00:00, 3373051.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 4000\n",
            "Active validation set size: 1000\n",
            "Active test set size: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SLDvMNwVEPgV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruning Functions"
      ]
    },
    {
      "metadata": {
        "id": "Ye1vK3FmREgq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Class that contains various settings pertaining to how filters are pruned\n",
        "class UnitPruningSettings:\n",
        "    \n",
        "    def __init__(self, idx_layer, idx_filter, N_prune = 1, p = 2):\n",
        "        \n",
        "        self.N_prune = N_prune # Number of filters allowed to be pruned in one pass\n",
        "        self.idx_filter = idx_filter # Indices of the N_prune filters\n",
        "        self.idx_layer = idx_layer # Current layer under consideration\n",
        "        self.p = p # p-norm to use when computing which filters to remove\n",
        "        \n",
        "        return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HnBts6uMEU9t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# These functions were adapted from https://github.com/jacobgil/pytorch-pruning/blob/master/prune.py\n",
        "\n",
        "def replace_layers(model, i, idx, layers):\n",
        "\tif i in idx:\n",
        "\t\treturn layers[idx.index(i)]\n",
        "\treturn model[i]\n",
        "\n",
        "# Function to prune a given convolution layer in the model provided.\n",
        "# Input \"idx_layers\" is the global index of the convolution layer to be pruned.\n",
        "# Input \"prune_settings\" is a data structure containing information on how pruning is performed.\n",
        "def PruneConvLayers(model, prune_settings):\n",
        "    \n",
        "    # Strategy: in order to prune a particular layer, the output of the previous layer \n",
        "    # and the inputs to the next layer must also be altered accordingly.\n",
        "\t\n",
        "    # Extract pruning settings for convenience\n",
        "    # Note that \"N_prune\" *consecutive* filters will get pruned\n",
        "#     N_prune = prune_settings.N_prune\n",
        "    N_prune = 1\n",
        "    idx_filter = prune_settings.idx_filter\n",
        "    idx_layer = prune_settings.idx_layer\n",
        "    \n",
        "    # Extract the layer of the model currently being pruned\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "#     _, conv = model.features._modules.items()(idx_layer)\n",
        "    \n",
        "    # To keep track of the succeeding convolution layer\n",
        "    next_conv = None\n",
        "    offset = 1\n",
        "    \n",
        "    # Figure out how many layers after this one are NOT conv layers, in order to skip pruning them\n",
        "    while idx_layer + offset < len(model.features._modules.items()):\n",
        "        \n",
        "        res =  list(model.features._modules.items())[idx_layer + offset]\n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            next_name, next_conv = res\n",
        "            break\n",
        "        offset = offset + 1\n",
        "    \n",
        "    # Create a new, replacement conv layer to remove a given number of filters.\n",
        "    # The rest of its settings should remain the same as the original conv layer.\n",
        "    new_conv = torch.nn.Conv2d(in_channels = conv.in_channels,\n",
        "                               out_channels = conv.out_channels - N_prune,\n",
        "\t\t\t                   kernel_size = conv.kernel_size,\n",
        "                               stride = conv.stride,\n",
        "                               padding = conv.padding,\n",
        "                               dilation = conv.dilation,\n",
        "                               groups = conv.groups,\n",
        "                               bias = True)\n",
        "    \n",
        "    new_conv.bias = conv.bias\n",
        "    \n",
        "    # Copy over the weights to the new conv layer, except the ones corresponding to the filter to be removed\n",
        "    old_weights = conv.weight.data.cpu().numpy()\n",
        "    new_weights = new_conv.weight.data.cpu().numpy()\n",
        "\n",
        "    # This copies the set of filters up to and excluding the filters to be removed\n",
        "    new_weights[: idx_filter, :, :, :] = old_weights[: idx_filter, :, :, :]\n",
        "\n",
        "    # This copies the filters after and excluding the filters to be removed\n",
        "    new_weights[idx_filter :, :, :, :] = old_weights[idx_filter + N_prune :, :, :, :]\n",
        "\n",
        "    # Update weight data of the new conv layer\n",
        "#     new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "    new_conv.weight.data = torch.from_numpy(new_weights)\n",
        "\n",
        "    # Now do the same thing for biases\n",
        "    old_biases = conv.bias.data.cpu().numpy()\n",
        "\n",
        "    new_biases = np.zeros(shape = (old_biases.shape[0] - N_prune), dtype = np.float32)\n",
        "    new_biases[:idx_filter] = old_biases[:idx_filter]\n",
        "    new_biases[idx_filter :] = old_biases[idx_filter + N_prune :]\n",
        "#     new_conv.bias.data = torch.from_numpy(new_biases).cuda()\n",
        "    new_conv.bias.data = torch.from_numpy(new_biases)\n",
        "    \n",
        "    # If there is a succeeding conv layer, adjust its input units and weights accordingly\n",
        "    if next_conv != None:\n",
        "        \n",
        "        next_new_conv = torch.nn.Conv2d(in_channels = next_conv.in_channels - N_prune,\n",
        "                                        out_channels =  next_conv.out_channels,\n",
        "                                        kernel_size = next_conv.kernel_size,\n",
        "                                        stride = next_conv.stride,\n",
        "                                        padding = next_conv.padding,\n",
        "                                        dilation = next_conv.dilation,\n",
        "                                        groups = next_conv.groups,\n",
        "                                        bias = True)\n",
        "        \n",
        "        next_new_conv.bias = next_conv.bias\n",
        "\n",
        "        old_weights = next_conv.weight.data.cpu().numpy()\n",
        "        new_weights = next_new_conv.weight.data.cpu().numpy()\n",
        "\n",
        "        new_weights[:, : idx_filter, :, :] = old_weights[:, : idx_filter, :, :]\n",
        "        new_weights[:, idx_filter : , :, :] = old_weights[:, idx_filter + N_prune :, :, :]\n",
        "#         next_new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "        next_new_conv.weight.data = torch.from_numpy(new_weights)\n",
        "\n",
        "        next_new_conv.bias.data = next_conv.bias.data\n",
        "\n",
        "        # Update the actual model by replacing the existing filters with the new ones\n",
        "        features = torch.nn.Sequential(\n",
        "                *(replace_layers(model.features, i, [idx_layer, idx_layer + offset], \\\n",
        "                    [new_conv, next_new_conv]) for i, _ in enumerate(model.features)))\n",
        "        del model.features\n",
        "        del conv\n",
        "\n",
        "        model.features = features\n",
        "    \n",
        "    else:\n",
        "\n",
        "        # This is the last conv layer. This affects the first linear layer of the classifier.\n",
        "        model.features = torch.nn.Sequential(\n",
        "                *(replace_layers(model.features, i, [idx_layer], \\\n",
        "                    [new_conv]) for i, _ in enumerate(model.features)))\n",
        "        idx_layer = 0\n",
        "        old_linear_layer = None\n",
        "\n",
        "        for _, module in model.classifier._modules.items():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                old_linear_layer = module\n",
        "                break\n",
        "            idx_layer = idx_layer + 1\n",
        "\n",
        "        if old_linear_layer == None:\n",
        "            raise BaseException(\"No linear layer found in classifier.\")\n",
        "        params_per_input_channel = old_linear_layer.in_features / conv.out_channels\n",
        "\n",
        "        new_linear_layer = \\\n",
        "            torch.nn.Linear(int(old_linear_layer.in_features - params_per_input_channel), \n",
        "                old_linear_layer.out_features)\n",
        "\n",
        "        old_weights = old_linear_layer.weight.data.cpu().numpy()\n",
        "        new_weights = new_linear_layer.weight.data.cpu().numpy()\t \t\n",
        "\n",
        "        new_weights[:, : int(idx_filter * params_per_input_channel)] = \\\n",
        "            old_weights[:, : int(idx_filter * params_per_input_channel)]\n",
        "        new_weights[:, int(idx_filter * params_per_input_channel) :] = \\\n",
        "            old_weights[:, int((idx_filter + N_prune) * params_per_input_channel) :]\n",
        "\n",
        "        new_linear_layer.bias.data = old_linear_layer.bias.data\n",
        "\n",
        "#         new_linear_layer.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "        new_linear_layer.weight.data = torch.from_numpy(new_weights)\n",
        "\n",
        "        classifier = torch.nn.Sequential(\n",
        "            *(replace_layers(model.classifier, i, [idx_layer], \\\n",
        "                [new_linear_layer]) for i, _ in enumerate(model.classifier)))\n",
        "\n",
        "        del model.classifier\n",
        "        del next_conv\n",
        "        del conv\n",
        "        model.classifier = classifier\n",
        "        \n",
        "    return model\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MnKIkiz8QZXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1160
        },
        "outputId": "d918b936-329e-4f86-8e68-7f1c06284655"
      },
      "cell_type": "code",
      "source": [
        "# Test pruning\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.train()\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(28, 10, 1)\n",
        "\n",
        "t0 = time.time()\n",
        "model = PruneConvLayers(model, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.torch/models/vgg16-397923af.pth\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "2547712it [00:00, 25456629.93it/s]\u001b[A\n",
            "12214272it [00:00, 32677392.33it/s]\u001b[A\n",
            "19587072it [00:00, 39226650.06it/s]\u001b[A\n",
            "26484736it [00:00, 45053929.43it/s]\u001b[A\n",
            "31784960it [00:00, 45784712.39it/s]\u001b[A\n",
            "38797312it [00:00, 51101246.77it/s]\u001b[A\n",
            "48824320it [00:00, 59910976.07it/s]\u001b[A\n",
            "57548800it [00:00, 66119314.51it/s]\u001b[A\n",
            "66174976it [00:00, 70873167.42it/s]\u001b[A\n",
            "74604544it [00:01, 74337518.02it/s]\u001b[A\n",
            "84066304it [00:01, 72106890.28it/s]\u001b[A\n",
            "93757440it [00:01, 78030950.78it/s]\u001b[A\n",
            "104054784it [00:01, 84142701.83it/s]\u001b[A\n",
            "112902144it [00:01, 85160872.30it/s]\u001b[A\n",
            "121962496it [00:01, 86715579.01it/s]\u001b[A\n",
            "130859008it [00:01, 86276254.90it/s]\u001b[A\n",
            "139649024it [00:01, 75768614.52it/s]\u001b[A\n",
            "148078592it [00:01, 78134040.57it/s]\u001b[A\n",
            "157065216it [00:02, 81314383.15it/s]\u001b[A\n",
            "165412864it [00:02, 75706618.39it/s]\u001b[A\n",
            "173211648it [00:02, 72029681.04it/s]\u001b[A\n",
            "182870016it [00:02, 77967071.14it/s]\u001b[A\n",
            "192643072it [00:02, 82991088.71it/s]\u001b[A\n",
            "202588160it [00:02, 87320604.09it/s]\u001b[A\n",
            "212869120it [00:02, 91445710.26it/s]\u001b[A\n",
            "222855168it [00:02, 93815571.02it/s]\u001b[A\n",
            "232415232it [00:02, 93812547.42it/s]\u001b[A\n",
            "241926144it [00:02, 88635064.12it/s]\u001b[A\n",
            "251691008it [00:03, 91115840.72it/s]\u001b[A\n",
            "260923392it [00:03, 90676879.18it/s]\u001b[A\n",
            "270082048it [00:03, 88518230.29it/s]\u001b[A\n",
            "279314432it [00:03, 89615880.52it/s]\u001b[A\n",
            "288718848it [00:03, 90894821.00it/s]\u001b[A\n",
            "297852928it [00:03, 90058780.10it/s]\u001b[A\n",
            "306896896it [00:03, 88006713.20it/s]\u001b[A\n",
            "316375040it [00:03, 89925288.96it/s]\u001b[A\n",
            "325402624it [00:03, 70755169.68it/s]\u001b[A\n",
            "335503360it [00:04, 77733883.78it/s]\u001b[A\n",
            "345300992it [00:04, 82862069.00it/s]\u001b[A\n",
            "355319808it [00:04, 87383180.00it/s]\u001b[A\n",
            "365051904it [00:04, 90131144.43it/s]\u001b[A\n",
            "375496704it [00:04, 93986640.06it/s]\u001b[A\n",
            "385179648it [00:04, 90280370.00it/s]\u001b[A\n",
            "394444800it [00:04, 89677080.19it/s]\u001b[A\n",
            "404176896it [00:04, 91824909.29it/s]\u001b[A\n",
            "413491200it [00:04, 90150352.14it/s]\u001b[A\n",
            "422608896it [00:05, 82297564.53it/s]\u001b[A\n",
            "432513024it [00:05, 86694069.35it/s]\u001b[A\n",
            "441384960it [00:05, 84201324.22it/s]\u001b[A\n",
            "451305472it [00:05, 88192709.77it/s]\u001b[A\n",
            "460292096it [00:05, 80044817.41it/s]\u001b[A\n",
            "469000192it [00:05, 82024650.07it/s]\u001b[A\n",
            "477536256it [00:05, 82993033.86it/s]\u001b[A\n",
            "486604800it [00:05, 85148792.23it/s]\u001b[A\n",
            "495738880it [00:05, 86819118.11it/s]\u001b[A\n",
            "504954880it [00:06, 88351138.69it/s]\u001b[A\n",
            "514408448it [00:06, 90110275.22it/s]\u001b[A\n",
            "523624448it [00:06, 90707916.95it/s]\u001b[A\n",
            "533405696it [00:06, 92715264.16it/s]\u001b[A\n",
            "542720000it [00:06, 89534691.20it/s]\u001b[A\n",
            "551731200it [00:06, 82942614.14it/s]\u001b[A\n",
            "553433881it [00:06, 84151156.79it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Pruning took 1.234318494796753 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yn_VPoXdbmPg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Function"
      ]
    },
    {
      "metadata": {
        "id": "kvekshfziL_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, dat, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    \n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                dataloader = dat.train_loader\n",
        "                dataset_size = dat.trainset.__len__()\n",
        "                \n",
        "                model.train()  # Set model to training mode\n",
        "                \n",
        "            else:\n",
        "                \n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                dataloader = dat.val_loader\n",
        "                dataset_size = dat.valset.__len__()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloader:\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vpIOd-wiTYf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline Model Setup"
      ]
    },
    {
      "metadata": {
        "id": "6w6D3bRfbrvQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_5AJjslGcFHi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Baseline Model"
      ]
    },
    {
      "metadata": {
        "id": "9s7W8AMicIlK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2366
        },
        "outputId": "819f6eba-2b6e-49b7-910a-30fb1c5a4213"
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 2.1566 Acc: 0.2325\n",
            "val Loss: 2.0811 Acc: 0.2800\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 2.1084 Acc: 0.2500\n",
            "val Loss: 1.9544 Acc: 0.3100\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 2.0179 Acc: 0.2650\n",
            "val Loss: 2.0884 Acc: 0.2400\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 1.9010 Acc: 0.3075\n",
            "val Loss: 2.0639 Acc: 0.2800\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 1.8655 Acc: 0.3150\n",
            "val Loss: 1.8821 Acc: 0.2800\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 1.7540 Acc: 0.3675\n",
            "val Loss: 1.9376 Acc: 0.3300\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 1.5314 Acc: 0.4350\n",
            "val Loss: 1.9539 Acc: 0.2900\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 1.4907 Acc: 0.4600\n",
            "val Loss: 1.6439 Acc: 0.3300\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 1.3321 Acc: 0.5350\n",
            "val Loss: 1.7334 Acc: 0.3200\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 1.3800 Acc: 0.4975\n",
            "val Loss: 1.7698 Acc: 0.3800\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 1.2999 Acc: 0.5225\n",
            "val Loss: 1.7705 Acc: 0.3300\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 1.3612 Acc: 0.5050\n",
            "val Loss: 1.7626 Acc: 0.3600\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 1.3147 Acc: 0.5400\n",
            "val Loss: 1.5745 Acc: 0.3800\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 1.1530 Acc: 0.5825\n",
            "val Loss: 1.5845 Acc: 0.3900\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 1.1775 Acc: 0.6200\n",
            "val Loss: 1.8330 Acc: 0.3400\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "train Loss: 1.1696 Acc: 0.6000\n",
            "val Loss: 1.7229 Acc: 0.3800\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "train Loss: 1.1687 Acc: 0.5875\n",
            "val Loss: 1.7418 Acc: 0.3000\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "train Loss: 1.1214 Acc: 0.6100\n",
            "val Loss: 1.7388 Acc: 0.3300\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n",
            "train Loss: 1.2336 Acc: 0.5450\n",
            "val Loss: 1.7153 Acc: 0.3200\n",
            "\n",
            "Epoch 19/24\n",
            "----------\n",
            "train Loss: 1.1425 Acc: 0.6025\n",
            "val Loss: 1.7685 Acc: 0.3000\n",
            "\n",
            "Epoch 20/24\n",
            "----------\n",
            "train Loss: 1.1471 Acc: 0.5950\n",
            "val Loss: 1.6418 Acc: 0.3600\n",
            "\n",
            "Epoch 21/24\n",
            "----------\n",
            "train Loss: 1.1494 Acc: 0.6000\n",
            "val Loss: 1.7840 Acc: 0.3700\n",
            "\n",
            "Epoch 22/24\n",
            "----------\n",
            "train Loss: 1.1295 Acc: 0.5900\n",
            "val Loss: 1.8160 Acc: 0.3200\n",
            "\n",
            "Epoch 23/24\n",
            "----------\n",
            "train Loss: 1.2116 Acc: 0.5875\n",
            "val Loss: 1.7614 Acc: 0.3200\n",
            "\n",
            "Epoch 24/24\n",
            "----------\n",
            "train Loss: 1.1677 Acc: 0.5800\n",
            "val Loss: 1.7376 Acc: 0.3500\n",
            "\n",
            "Training complete in 14m 33s\n",
            "Best val Acc: 0.390000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b031iPnkif-H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruned Model Setup - Single Filter"
      ]
    },
    {
      "metadata": {
        "id": "IbwrbOviikWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a55f15d5-2f8f-48fa-c5fc-06e759029776"
      },
      "cell_type": "code",
      "source": [
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(28, 10, 1)\n",
        "\n",
        "t0 = time.time()\n",
        "model = PruneConvLayers(model, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning took 1.4709546566009521 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Sg4OKhtHjCrj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Pruned Model - Single Filter"
      ]
    },
    {
      "metadata": {
        "id": "AKXgtuR0jJf2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2366
        },
        "outputId": "c10dc6be-5e8a-4268-cbaf-42b02313244b"
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 2.8876 Acc: 0.0875\n",
            "val Loss: 2.3552 Acc: 0.1900\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 2.4806 Acc: 0.1275\n",
            "val Loss: 2.2240 Acc: 0.1800\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 2.3176 Acc: 0.1900\n",
            "val Loss: 2.1939 Acc: 0.2000\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 2.2381 Acc: 0.1875\n",
            "val Loss: 2.1577 Acc: 0.2000\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 2.1509 Acc: 0.2350\n",
            "val Loss: 2.1055 Acc: 0.1900\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 2.1236 Acc: 0.2425\n",
            "val Loss: 2.0256 Acc: 0.2600\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 1.8307 Acc: 0.3250\n",
            "val Loss: 1.8670 Acc: 0.2900\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 1.7686 Acc: 0.3500\n",
            "val Loss: 1.9131 Acc: 0.3200\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 1.7535 Acc: 0.3400\n",
            "val Loss: 1.8303 Acc: 0.3400\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 1.6652 Acc: 0.4000\n",
            "val Loss: 1.8468 Acc: 0.3600\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 1.6122 Acc: 0.4050\n",
            "val Loss: 1.7168 Acc: 0.4300\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 1.5730 Acc: 0.4050\n",
            "val Loss: 1.8190 Acc: 0.3100\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 1.6161 Acc: 0.4000\n",
            "val Loss: 1.7363 Acc: 0.3600\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 1.4759 Acc: 0.4650\n",
            "val Loss: 1.6957 Acc: 0.4200\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 1.5155 Acc: 0.4475\n",
            "val Loss: 1.8601 Acc: 0.3300\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "train Loss: 1.5061 Acc: 0.4800\n",
            "val Loss: 1.8389 Acc: 0.3500\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "train Loss: 1.5602 Acc: 0.4375\n",
            "val Loss: 1.7337 Acc: 0.3400\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "train Loss: 1.5198 Acc: 0.4625\n",
            "val Loss: 1.7519 Acc: 0.3600\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n",
            "train Loss: 1.4680 Acc: 0.4800\n",
            "val Loss: 1.7563 Acc: 0.3100\n",
            "\n",
            "Epoch 19/24\n",
            "----------\n",
            "train Loss: 1.4432 Acc: 0.5250\n",
            "val Loss: 1.6784 Acc: 0.4000\n",
            "\n",
            "Epoch 20/24\n",
            "----------\n",
            "train Loss: 1.4341 Acc: 0.4700\n",
            "val Loss: 1.8538 Acc: 0.2700\n",
            "\n",
            "Epoch 21/24\n",
            "----------\n",
            "train Loss: 1.5188 Acc: 0.4375\n",
            "val Loss: 1.8839 Acc: 0.3100\n",
            "\n",
            "Epoch 22/24\n",
            "----------\n",
            "train Loss: 1.4668 Acc: 0.4550\n",
            "val Loss: 1.7525 Acc: 0.3800\n",
            "\n",
            "Epoch 23/24\n",
            "----------\n",
            "train Loss: 1.5095 Acc: 0.4525\n",
            "val Loss: 1.8141 Acc: 0.3200\n",
            "\n",
            "Epoch 24/24\n",
            "----------\n",
            "train Loss: 1.4857 Acc: 0.4775\n",
            "val Loss: 1.7037 Acc: 0.4900\n",
            "\n",
            "Training complete in 14m 40s\n",
            "Best val Acc: 0.490000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1slyZpAjY0yQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Functions to Compute Filter Metrics"
      ]
    },
    {
      "metadata": {
        "id": "ryg9vNTsY8Oc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "1c1e8eec-c220-4964-f72d-feaf62b8720e"
      },
      "cell_type": "code",
      "source": [
        "# Function to compute the p-norm of weights in all filters of a given layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DFilterNorms(model, idx_layer, p):\n",
        "    \n",
        "    # Extract the layer of the model currently being considered\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    \n",
        "    weights = conv.weight.data\n",
        "\n",
        "    # Compute norms of each filter\n",
        "    norms = weights.norm(p, dim=2)\n",
        "    norms = norms.norm(p, dim=2)\n",
        "    norms = norms.norm(p, dim=1)\n",
        "    \n",
        "    return norms\n",
        "\n",
        "# Test\n",
        "norms_1 = ComputeConv2DFilterNorms(model, 0, 1)\n",
        "norms_2 = ComputeConv2DFilterNorms(model, 0, 2)\n",
        "norms_inf = ComputeConv2DFilterNorms(model, 0, float('inf'))\n",
        "\n",
        "print(norms_1.size())\n",
        "# print(norms_1)\n",
        "\n",
        "# Compute values and indices of the k smallest norms\n",
        "k = 2\n",
        "n1_botk, ind_n1_botk = torch.topk(norms_1, k, 0, largest=False, sorted=True, out=None)\n",
        "print(n1_botk)\n",
        "print(len(ind_n1_botk))\n",
        "print(ind_n1_botk)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64])\n",
            "tensor([1.0097, 1.9274])\n",
            "2\n",
            "tensor([54, 50])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RvJela053WOj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to iterate through all conv2D layers of the network and determine \n",
        "# filters to be pruned, and then carry out the pruning.\n",
        "def PruneAllConv2DLayers(model, prune_settings):\n",
        "    \n",
        "    # Extract pruning settings for convenience\n",
        "    # Note that \"N_prune\" *consecutive* filters will get pruned\n",
        "    N_prune = prune_settings.N_prune\n",
        "    p = prune_settings.p\n",
        "    \n",
        "    # Find the N_prune filters to remove\n",
        "    ii = 0\n",
        "    while ii < len(model.features._modules.items()):\n",
        "        \n",
        "        res = list(model.features._modules.items())[ii]\n",
        "        \n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            # Compute values and indices of the N_prune smallest norms\n",
        "            norms = ComputeConv2DFilterNorms(model, ii, p)\n",
        "            n_botk, ind_botk = torch.topk(norms, N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            \n",
        "            # Prune filters one at a time (todo: prune all at once)\n",
        "            prune_settings.idx_layer = ii\n",
        "            \n",
        "            jj = 0\n",
        "            while jj < len(ind_botk):\n",
        "                prune_settings.idx_filter = ind_botk[jj]\n",
        "                print(\"Pruning filter {} in layer {}\".format(ind_botk[jj], ii))\n",
        "                model = PruneConvLayers(model, prune_settings)\n",
        "                jj = jj + 1\n",
        "                \n",
        "        ii = ii + 1\n",
        "    \n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_QHn-nWjih5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruned Model Setup - Several Filters"
      ]
    },
    {
      "metadata": {
        "id": "wLvxYCUmeh3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "ce7d8c43-8aec-45cb-8e9e-9b5df7f629a7"
      },
      "cell_type": "code",
      "source": [
        "# Test pruning all layers\n",
        "\n",
        "model = models.vgg16(pretrained=False)\n",
        "model.train()\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(28, 10, N_prune = 2, p = 2)\n",
        "\n",
        "t0 = time.time()\n",
        "model = PruneAllConv2DLayers(model, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning filter 0 in layer 0\n",
            "Pruning filter 60 in layer 0\n",
            "Pruning filter 5 in layer 2\n",
            "Pruning filter 16 in layer 2\n",
            "Pruning filter 47 in layer 5\n",
            "Pruning filter 28 in layer 5\n",
            "Pruning filter 42 in layer 7\n",
            "Pruning filter 54 in layer 7\n",
            "Pruning filter 4 in layer 10\n",
            "Pruning filter 29 in layer 10\n",
            "Pruning filter 10 in layer 12\n",
            "Pruning filter 238 in layer 12\n",
            "Pruning filter 169 in layer 14\n",
            "Pruning filter 218 in layer 14\n",
            "Pruning filter 371 in layer 17\n",
            "Pruning filter 283 in layer 17\n",
            "Pruning filter 374 in layer 19\n",
            "Pruning filter 91 in layer 19\n",
            "Pruning filter 86 in layer 21\n",
            "Pruning filter 377 in layer 21\n",
            "Pruning filter 26 in layer 24\n",
            "Pruning filter 480 in layer 24\n",
            "Pruning filter 23 in layer 26\n",
            "Pruning filter 358 in layer 26\n",
            "Pruning filter 351 in layer 28\n",
            "Pruning filter 131 in layer 28\n",
            "Pruning took 3.5376181602478027 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SHixvYRUjpgl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Pruned Model - Several Filters"
      ]
    },
    {
      "metadata": {
        "id": "5c5f6i-1jxFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "ff6d25ac-8812-4769-fbff-b3f6286ed3b9"
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "Epoch 0/24\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}