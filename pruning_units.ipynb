{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pruning_units.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "tYqrMVdpA1NX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision as tv\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BhzbK8ntAmnN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Computation Routines"
      ]
    },
    {
      "metadata": {
        "id": "NuhlcxSIA94G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ]
    },
    {
      "metadata": {
        "id": "YC8SpzkJA8gK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DatasetManager:\n",
        "    \n",
        "    def __init__(self, dataset = 'cifar10', percent_data = 10.0, percent_val = 20.0, data_path = './data'):\n",
        "        \n",
        "        # 'dataset' can be 'hymenoptera', 'cifar10', or 'cifar100'.\n",
        "        # 'percent_data' is the percentage of the full training set to be used.\n",
        "        # 'percent_val' is the percentage of the *loaded* training set to be used as validation data.\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.data_path = data_path\n",
        "        self.percent_data = percent_data\n",
        "        self.percent_val = percent_val\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "            \n",
        "        elif self.dataset == 'cifar10' or self.dataset == 'cifar100':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def ImportDataset(self, batch_size=5):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "        \n",
        "            self.trainset = tv.datasets.ImageFolder(root=self.data_path,\n",
        "                             transform=self.transform)\n",
        "        \n",
        "        # todo\n",
        "        \n",
        "        elif self.dataset == 'cifar10':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR10(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR10(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "        \n",
        "        elif self.dataset == 'cifar100':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR100(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR100(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "             \n",
        "        self.SplitData();\n",
        "        self.GenerateLoaders();\n",
        "                \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def SplitData(self):\n",
        "        \n",
        "        len_full = self.trainset.__len__()\n",
        "        len_train = int(np.round(len_full*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.trainset = torch.utils.data.random_split(self.trainset, (len_full-len_train, len_train))\n",
        "        \n",
        "        len_val = int(np.round(len_train*self.percent_val/100.0))\n",
        "        len_train = len_train - len_val\n",
        "        \n",
        "        self.valset, self.trainset = torch.utils.data.random_split(self.trainset, (len_val, len_train))\n",
        "         \n",
        "        len_full_test = self.testset.__len__()\n",
        "        len_test = int(np.round(len_full_test*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.testset = torch.utils.data.random_split(self.testset, (len_full_test-len_test, len_test))\n",
        "\n",
        "        print('\\nFull training set size: {}'.format(len_full))\n",
        "        print('Full test set size: {}'.format(len_full_test))\n",
        "        print('\\nActive training set size: {}'.format(len_train))\n",
        "        print('Active validation set size: {}'.format(len_val))\n",
        "        print('Active test set size: {}\\n'.format(len_test))\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def GenerateLoaders(self):\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.val_loader = torch.utils.data.DataLoader(self.valset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)          \n",
        "            \n",
        "        return\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUJWD_oDBKHD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training function"
      ]
    },
    {
      "metadata": {
        "id": "34G6XlaFBMUW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, dat, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    \n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                dataloader = dat.train_loader\n",
        "                dataset_size = dat.trainset.__len__()\n",
        "                \n",
        "                model.train()  # Set model to training mode\n",
        "                \n",
        "            else:\n",
        "                \n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                dataloader = dat.val_loader\n",
        "                dataset_size = dat.valset.__len__()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloader:\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if training\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z7C3UubBBXFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruning functions"
      ]
    },
    {
      "metadata": {
        "id": "jAZVF5O-BZNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Constants that define possible pruning metrics\n",
        "WEIGHT_NORM = 1\n",
        "WEIGHT_GRAD = 2\n",
        "WEIGHT_GRAD_AND_NORM = 3\n",
        "ACT_NORM = 4\n",
        "\n",
        "# Class that contains various settings pertaining to how filters are pruned\n",
        "class UnitPruningSettings:\n",
        "    \n",
        "    def __init__(self, idx_layer=0, idx_filter=0, N_prune=1, P_prune=10, p=2, pruning_metric=WEIGHT_NORM):\n",
        "        \n",
        "        # EITHER N_prune OR P_prune will be used to decide how many filters to prune.\n",
        "        # If one is non-positive, the other is used.\n",
        "        # If neither is non-positive, priority is given to P_prune.\n",
        "        # If both are non-positive, no pruning will happen.\n",
        "\n",
        "        self.N_prune = N_prune # Number of filters allowed to be pruned in one pass\n",
        "        self.P_prune = P_prune; # Percent of filters of the current layer to prune\n",
        "        \n",
        "        self.idx_filter = idx_filter # Indices of the N_prune filters\n",
        "        self.idx_layer = idx_layer # Current layer under consideration\n",
        "        self.p = p # p-norm to use when computing which filters to remove\n",
        "        self.pruning_metric = pruning_metric\n",
        "        \n",
        "        # Various statistics will be stored and computed to keep track of how the network changes\n",
        "        \n",
        "        # Number of filters per layer in the original network\n",
        "        self.filters_per_layer_orig = []\n",
        "        \n",
        "        # Number of filters per layer after pruning - this gets updated every time the network is pruned\n",
        "        self.filters_per_layer_after = []\n",
        "        \n",
        "        # Time taken to prune in sec (running total, updated every time pruning happens)\n",
        "        self.prune_time = 0.0\n",
        "        \n",
        "        return\n",
        "    \n",
        "    # Function to print the current pruning state of the model. Verbose can be 0, 1, or 2.\n",
        "    def PrintPruningStatistics(self, verbose=1):\n",
        "    \n",
        "        if verbose == 0:\n",
        "            return\n",
        "        \n",
        "        print(\"Total number of filters before pruning: {}\".format(sum(self.filters_per_layer_orig)))\n",
        "        print(\"Total number of filters after pruning: {}\".format(sum(self.filters_per_layer_after)))\n",
        "    \n",
        "        return\n",
        "\n",
        "    \n",
        "# The following functions were adapted from https://github.com/jacobgil/pytorch-pruning/blob/master/prune.py\n",
        "\n",
        "def replace_layers(model, i, idx, layers):\n",
        "\tif i in idx:\n",
        "\t\treturn layers[idx.index(i)]\n",
        "\treturn model[i]\n",
        "\n",
        "\n",
        "# Function to prune a given convolution layer in the model provided.\n",
        "# Input \"idx_layers\" is the global index of the convolution layer to be pruned.\n",
        "# Input \"prune_settings\" is a data structure containing information on how pruning is performed.\n",
        "def PruneConvLayers(model, prune_settings):\n",
        "    \n",
        "    # Strategy: in order to prune a particular layer, the output of the previous layer \n",
        "    # and the inputs to the next layer must also be altered accordingly.\n",
        "\t\n",
        "    # Extract pruning settings for convenience\n",
        "    # Note that \"N_prune\" *consecutive* filters will get pruned\n",
        "    N_prune = prune_settings.N_prune\n",
        "    idx_filter = prune_settings.idx_filter\n",
        "    idx_layer = prune_settings.idx_layer\n",
        "    \n",
        "    if idx_layer >= len(model.features._modules.items()):\n",
        "        return\n",
        "        \n",
        "    # Extract the layer of the model currently being pruned\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    \n",
        "\n",
        "    # In case the list of target filters to delete has out-of-range entries, detect and ignore them\n",
        "    del_filters = []\n",
        "    for kk in range(0, len(idx_filter)):\n",
        "        if idx_filter[kk] >= conv.out_channels:\n",
        "            del_filters.extend(kk)\n",
        "    \n",
        "    if (len(del_filters) > 0):\n",
        "        idx_filter = np.delete(idx_filter, del_filters, 0)\n",
        "        N_prune = len(idx_filter)\n",
        "        prune_settings.N_prune = N_prune\n",
        "        print(\"[WARNING] Encountered an out-of-range target filter; it will be ignored.\")\n",
        "    \n",
        "    # Record pruning statistics\n",
        "#     prune_settings.filters_per_layer_orig.extend(conv.out_channels)\n",
        "#     prune_settings.filters_per_layer_after.extend(conv.out_channels - N_prune)\n",
        "    \n",
        "    prune_settings.filters_per_layer_orig[idx_layer] = conv.out_channels\n",
        "    prune_settings.filters_per_layer_after[idx_layer] = conv.out_channels - N_prune\n",
        "    \n",
        "        \n",
        "    # To keep track of the succeeding convolution layer\n",
        "    next_conv = None\n",
        "    offset = 1\n",
        "    \n",
        "    # Figure out how many layers after this one are NOT conv layers, in order to skip pruning them\n",
        "    while idx_layer + offset < len(model.features._modules.items()):\n",
        "        \n",
        "        res =  list(model.features._modules.items())[idx_layer + offset]\n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            next_name, next_conv = res\n",
        "            break\n",
        "        offset = offset + 1\n",
        "    \n",
        "    # Create a new, replacement conv layer to remove a given number of filters.\n",
        "    # The rest of its settings should remain the same as the original conv layer.\n",
        "    new_conv = torch.nn.Conv2d(in_channels = conv.in_channels,\n",
        "                               out_channels = conv.out_channels - N_prune,\n",
        "\t\t\t                   kernel_size = conv.kernel_size,\n",
        "                               stride = conv.stride,\n",
        "                               padding = conv.padding,\n",
        "                               dilation = conv.dilation,\n",
        "                               groups = conv.groups,\n",
        "                               bias = True)\n",
        "    \n",
        "    new_conv.bias = conv.bias\n",
        "    \n",
        "    # Copy over the weights to the new conv layer, except the ones corresponding to the filter to be removed\n",
        "    old_weights = conv.weight.data.cpu().numpy()\n",
        "    new_weights = new_conv.weight.data.cpu().numpy()\n",
        "\n",
        "    # Copy over the set of filters, excluding the ones to be removed\n",
        "    new_weights_temp = np.copy(old_weights)\n",
        "    new_weights_temp = np.delete(new_weights_temp, idx_filter, 0)\n",
        "    new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "    # Update weight data of the new conv layer\n",
        "    new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "    \n",
        "    # Now do the same thing for biases\n",
        "    old_biases = conv.bias.data.cpu().numpy()\n",
        "    new_biases = np.zeros(shape=(old_biases.shape[0] - N_prune), dtype=np.float32)\n",
        "    \n",
        "    new_biases_temp = np.copy(old_biases)\n",
        "    new_biases_temp = np.delete(new_biases_temp, idx_filter, 0)\n",
        "    new_biases[:] = new_biases_temp[:]\n",
        "        \n",
        "    new_conv.bias.data = torch.from_numpy(new_biases).cuda()\n",
        "    \n",
        "    # If there is a succeeding conv layer, adjust its input units and weights accordingly\n",
        "    if next_conv != None:\n",
        "        \n",
        "        next_new_conv = torch.nn.Conv2d(in_channels = next_conv.in_channels - N_prune,\n",
        "                                        out_channels =  next_conv.out_channels,\n",
        "                                        kernel_size = next_conv.kernel_size,\n",
        "                                        stride = next_conv.stride,\n",
        "                                        padding = next_conv.padding,\n",
        "                                        dilation = next_conv.dilation,\n",
        "                                        groups = next_conv.groups,\n",
        "                                        bias = True)\n",
        "        \n",
        "        next_new_conv.bias = next_conv.bias\n",
        "\n",
        "        old_weights = next_conv.weight.data.cpu().numpy()\n",
        "        new_weights = next_new_conv.weight.data.cpu().numpy()\n",
        "        \n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_filter, 1)\n",
        "        new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "        next_new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        # Now do the same thing for biases\n",
        "        next_new_conv.bias.data = next_conv.bias.data\n",
        "\n",
        "        # Update the actual model by replacing the existing filters with the new ones\n",
        "        features = torch.nn.Sequential(\n",
        "                *(replace_layers(model.features, i, [idx_layer, idx_layer + offset], \\\n",
        "                    [new_conv, next_new_conv]) for i, _ in enumerate(model.features)))\n",
        "        del model.features\n",
        "        del conv\n",
        "\n",
        "        model.features = features\n",
        "    \n",
        "    else:\n",
        "\n",
        "        # This is the last conv layer. This affects the first linear layer of the classifier.\n",
        "        model.features = torch.nn.Sequential(*(replace_layers(model.features, i, [idx_layer], [new_conv]) for i, _ in enumerate(model.features)))\n",
        "        idx_layer = 0\n",
        "        old_linear_layer = None\n",
        "\n",
        "        for _, module in model.classifier._modules.items():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                old_linear_layer = module\n",
        "                break\n",
        "            idx_layer = idx_layer + 1\n",
        "\n",
        "        if old_linear_layer == None:\n",
        "            raise BaseException(\"No linear layer found in classifier.\")\n",
        "            \n",
        "        params_per_input_channel = int(old_linear_layer.in_features/conv.out_channels)\n",
        "\n",
        "        new_linear_layer = torch.nn.Linear(old_linear_layer.in_features - N_prune*params_per_input_channel, \n",
        "                                           old_linear_layer.out_features)\n",
        "\n",
        "        old_weights = old_linear_layer.weight.data.cpu().numpy()\n",
        "        new_weights = new_linear_layer.weight.data.cpu().numpy()\t \t\n",
        "\n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        idx_expanded = np.zeros(shape=(N_prune*params_per_input_channel))\n",
        "        \n",
        "        for kk in range(0, len(idx_filter)):\n",
        "            idx_expanded[kk*params_per_input_channel:kk*params_per_input_channel+params_per_input_channel] = np.arange(idx_filter[kk]*params_per_input_channel, idx_filter[kk]*params_per_input_channel + params_per_input_channel)\n",
        "\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_expanded.astype(int), 1)\n",
        "        new_weights[:, :] = new_weights_temp[:, :]\n",
        "        \n",
        "        new_linear_layer.bias.data = old_linear_layer.bias.data\n",
        "        new_linear_layer.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        classifier = torch.nn.Sequential(*(replace_layers(model.classifier, i, [idx_layer], [new_linear_layer]) for i, _ in enumerate(model.classifier)))\n",
        "\n",
        "        del model.classifier\n",
        "        del next_conv\n",
        "        del conv\n",
        "        model.classifier = classifier\n",
        "        \n",
        "    return model\n",
        "\n",
        "\n",
        "# Function to compute the p-norm of weights in all filters of a given layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DWeightNorms(model, idx_layer, p):\n",
        "    \n",
        "    # Extract the layer of the model currently being considered\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    \n",
        "    weights = conv.weight.data\n",
        "\n",
        "    # Compute norms of each filter\n",
        "    norms = weights.norm(p, dim=2)\n",
        "    norms = norms.norm(p, dim=2)\n",
        "    norms = norms.norm(p, dim=1)\n",
        "    \n",
        "    return norms\n",
        "\n",
        "\n",
        "# Function to compute the p-norm of activations of all filters of a given layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DWeightGrads(model, idx_layer, p):\n",
        "    \n",
        "    return\n",
        "\n",
        "\n",
        "# Function to compute the p-norm of activations of all filters of a given layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DWeightNormsAndGrads(model, idx_layer, p):\n",
        "    \n",
        "    return\n",
        "\n",
        "\n",
        "# Function to compute the p-norm of activations of all filters of a given layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DActNorms(model, idx_layer, p):\n",
        "    \n",
        "    return\n",
        "\n",
        "\n",
        "# Function to iterate through all conv2D layers of the network and determine \n",
        "# filters to be pruned, and then carry out the pruning.\n",
        "def PruneAllConv2DLayers(model, prune_settings):\n",
        "    \n",
        "    # Extract pruning settings for convenience\n",
        "    # Note that \"N_prune\" *consecutive* filters will get pruned\n",
        "    N_prune = prune_settings.N_prune\n",
        "    P_prune = prune_settings.P_prune\n",
        "    p = prune_settings.p\n",
        "    pruning_metric = prune_settings.pruning_metric\n",
        "    \n",
        "    # Count number of prunable layers for preallocation\n",
        "    N_layers = len(model.features._modules.items())       \n",
        "    prune_settings.filters_per_layer_orig = np.zeros(shape=(1, N_layers)).ravel()\n",
        "    prune_settings.filters_per_layer_after = np.zeros(shape=(1, N_layers)).ravel()\n",
        "\n",
        "    \n",
        "    # Find the N_prune filters to remove\n",
        "    ii = 1\n",
        "    while ii < len(model.features._modules.items()):\n",
        "        \n",
        "        res = list(model.features._modules.items())[ii]\n",
        "        \n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            # Compute values and indices of the N_prune smallest norms\n",
        "            if pruning_metric == WEIGHT_NORM:\n",
        "                norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "            elif pruning_metric == ACT_NORM:\n",
        "                norms = ComputeConv2DActNorms(model, ii, p)\n",
        "                \n",
        "                \n",
        "            if (P_prune >= 0):\n",
        "                _, conv = list(model.features._modules.items())[ii]\n",
        "                N_prune = int(conv.out_channels*P_prune/100.0)\n",
        "                prune_settings.N_prune = N_prune\n",
        "            \n",
        "            n_botk, ind_botk = torch.topk(norms, N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            \n",
        "            # Prune filters one at a time (todo: prune all at once)\n",
        "            prune_settings.idx_layer = ii\n",
        "            prune_settings.idx_filter = ind_botk.cpu().numpy()\n",
        "            \n",
        "            model = PruneConvLayers(model, prune_settings)\n",
        "                \n",
        "        ii = ii + 1\n",
        "            \n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cCEASFCwB3uP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Pruning"
      ]
    },
    {
      "metadata": {
        "id": "LmuhhazDB6d7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2a7188e0-a697-442b-d79d-d7734f3352bb"
      },
      "cell_type": "code",
      "source": [
        "# Test pruning\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.train()\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10, 12, 15, 16, 21), \n",
        "                                     N_prune=5, p=2, pruning_metric=WEIGHT_NORM)\n",
        "# prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10), \n",
        "#                                      N_prune=1, p=2, pruning_metric=WEIGHT_NORM)\n",
        "\n",
        "N_layers = len(model.features._modules.items())       \n",
        "prune_settings.filters_per_layer_orig = np.zeros(shape=(1, N_layers)).ravel()\n",
        "prune_settings.filters_per_layer_after = np.zeros(shape=(1, N_layers)).ravel()\n",
        "\n",
        "t0 = time.time()\n",
        "model = PruneConvLayers(model, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning took 5.523540735244751 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0tfmvixSCzAH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Setup Routines"
      ]
    },
    {
      "metadata": {
        "id": "fF2ZhlgCC31h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline Model Setup"
      ]
    },
    {
      "metadata": {
        "id": "GeD0hZfpDHbr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_baseline = models.vgg16(pretrained=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_baseline = model_baseline.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model_baseline.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lc0YTkyADA63",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruned Model Setup"
      ]
    },
    {
      "metadata": {
        "id": "cr4wgE6eDKND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a81c8cef-039b-4f2a-ee1f-22593ef407ca"
      },
      "cell_type": "code",
      "source": [
        "# Test pruning all layers\n",
        "\n",
        "model_pruned = models.vgg16(pretrained=True)\n",
        "model_pruned.train()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_pruned = model_pruned.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model_pruned.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(28, 10, N_prune = 4, p = 2, pruning_metric = WEIGHT_NORM)\n",
        "\n",
        "t0 = time.time()\n",
        "model_pruned = PruneAllConv2DLayers(model_pruned, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning took 5.252957582473755 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "coPfbZmXE3T0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Driver Routines"
      ]
    },
    {
      "metadata": {
        "id": "DS7qbZ0JE6Fs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Iterative Pruning"
      ]
    },
    {
      "metadata": {
        "id": "E1q-h_KTE-kE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3627
        },
        "outputId": "e06a8f87-a398-4387-ca28-6bdd5b111fd0"
      },
      "cell_type": "code",
      "source": [
        "# ====== Dataset setup ======\n",
        "\n",
        "percent_data = 2.0\n",
        "percent_val = 20.0\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "# ====== Model setup ======\n",
        "\n",
        "model = models.vgg16(pretrained=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# ====== Pruning setup ======\n",
        "\n",
        "N_prune = 4\n",
        "P_prune = 5\n",
        "p = 2\n",
        "prune_settings = UnitPruningSettings(N_prune=N_prune, P_prune=P_prune, p=p)\n",
        "\n",
        "\n",
        "# ====== Begin training ======\n",
        "\n",
        "N_iter_outer = 5\n",
        "N_iter_inner = 5\n",
        "\n",
        "# Import data\n",
        "dat = DatasetManager(dataset='cifar10', percent_data=percent_data, percent_val=percent_val)\n",
        "dat.ImportDataset(batch_size=batch_size)\n",
        "\n",
        "for ii in range(0, N_iter_outer):\n",
        "    \n",
        "    print(\"\\n------ Outer iteration {}/{} ------\".format(ii+1, N_iter_outer))\n",
        "#     t0 = time.time()\n",
        "\n",
        "    # ------ Prune current model ------\n",
        "        \n",
        "    model = PruneAllConv2DLayers(model, prune_settings)\n",
        "    new_model = copy.deepcopy(model)\n",
        "    model = new_model\n",
        "    prune_settings.PrintPruningStatistics(1)\n",
        "\n",
        "    # ------ Train current model ------\n",
        "    \n",
        "    # Import data\n",
        "    dat = DatasetManager(dataset='cifar10', percent_data=percent_data, percent_val=percent_val)\n",
        "    dat.ImportDataset(batch_size=batch_size)\n",
        "    \n",
        "    # Update optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=N_iter_inner)\n",
        "        \n",
        "#     print (\"Pruning took {} s\".format(time.time() - t0))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 800\n",
            "Active validation set size: 200\n",
            "Active test set size: 200\n",
            "\n",
            "------ Outer iteration 1/5 ------\n",
            "Total number of filters before pruning: 4160.0\n",
            "Total number of filters after pruning: 3959.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 800\n",
            "Active validation set size: 200\n",
            "Active test set size: 200\n",
            "Epoch 1/5\n",
            "----------\n",
            "train Loss: 3.5196 Acc: 0.1025\n",
            "val Loss: 2.5544 Acc: 0.0650\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "train Loss: 2.5066 Acc: 0.1025\n",
            "val Loss: 2.4330 Acc: 0.1000\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "train Loss: 2.4020 Acc: 0.1300\n",
            "val Loss: 2.2630 Acc: 0.1450\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "train Loss: 2.2994 Acc: 0.1425\n",
            "val Loss: 2.1951 Acc: 0.2250\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "train Loss: 2.2389 Acc: 0.1663\n",
            "val Loss: 2.2959 Acc: 0.1800\n",
            "\n",
            "Training complete in 2m 13s\n",
            "Best val Acc: 0.225000\n",
            "\n",
            "------ Outer iteration 2/5 ------\n",
            "Total number of filters before pruning: 3959.0\n",
            "Total number of filters after pruning: 3764.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 800\n",
            "Active validation set size: 200\n",
            "Active test set size: 200\n",
            "Epoch 1/5\n",
            "----------\n",
            "train Loss: 2.2531 Acc: 0.1825\n",
            "val Loss: 2.2129 Acc: 0.2150\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "train Loss: 2.2678 Acc: 0.1750\n",
            "val Loss: 2.2028 Acc: 0.2100\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "train Loss: 2.2336 Acc: 0.1913\n",
            "val Loss: 2.1451 Acc: 0.1550\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "train Loss: 2.1931 Acc: 0.1862\n",
            "val Loss: 2.1794 Acc: 0.2100\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "train Loss: 2.1692 Acc: 0.1988\n",
            "val Loss: 2.2143 Acc: 0.2400\n",
            "\n",
            "Training complete in 2m 7s\n",
            "Best val Acc: 0.240000\n",
            "\n",
            "------ Outer iteration 3/5 ------\n",
            "Total number of filters before pruning: 3764.0\n",
            "Total number of filters after pruning: 3581.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 800\n",
            "Active validation set size: 200\n",
            "Active test set size: 200\n",
            "Epoch 1/5\n",
            "----------\n",
            "train Loss: 2.1608 Acc: 0.1787\n",
            "val Loss: 2.2107 Acc: 0.2000\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "train Loss: 2.1851 Acc: 0.2025\n",
            "val Loss: 2.1072 Acc: 0.2400\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "train Loss: 2.1211 Acc: 0.2038\n",
            "val Loss: 2.0345 Acc: 0.2650\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "train Loss: 2.0662 Acc: 0.2350\n",
            "val Loss: 2.1568 Acc: 0.2200\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "train Loss: 2.0641 Acc: 0.2400\n",
            "val Loss: 1.9448 Acc: 0.3150\n",
            "\n",
            "Training complete in 1m 59s\n",
            "Best val Acc: 0.315000\n",
            "\n",
            "------ Outer iteration 4/5 ------\n",
            "Total number of filters before pruning: 3581.0\n",
            "Total number of filters after pruning: 3404.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 800\n",
            "Active validation set size: 200\n",
            "Active test set size: 200\n",
            "Epoch 1/5\n",
            "----------\n",
            "train Loss: 2.0511 Acc: 0.2612\n",
            "val Loss: 2.0618 Acc: 0.2350\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "train Loss: 2.0910 Acc: 0.2225\n",
            "val Loss: 2.1452 Acc: 0.2000\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "train Loss: 2.0455 Acc: 0.2412\n",
            "val Loss: 2.0062 Acc: 0.2150\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "train Loss: 2.0744 Acc: 0.2150\n",
            "val Loss: 2.1014 Acc: 0.2150\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "train Loss: 2.0229 Acc: 0.2475\n",
            "val Loss: 2.0659 Acc: 0.2100\n",
            "\n",
            "Training complete in 1m 56s\n",
            "Best val Acc: 0.235000\n",
            "\n",
            "------ Outer iteration 5/5 ------\n",
            "Total number of filters before pruning: 3404.0\n",
            "Total number of filters after pruning: 3242.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 800\n",
            "Active validation set size: 200\n",
            "Active test set size: 200\n",
            "Epoch 1/5\n",
            "----------\n",
            "train Loss: 2.0935 Acc: 0.2213\n",
            "val Loss: 2.1880 Acc: 0.1900\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "train Loss: 2.0807 Acc: 0.2213\n",
            "val Loss: 2.0850 Acc: 0.2550\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "train Loss: 2.0269 Acc: 0.2338\n",
            "val Loss: 2.1231 Acc: 0.1700\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "train Loss: 2.0474 Acc: 0.2325\n",
            "val Loss: 2.0294 Acc: 0.2500\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "train Loss: 2.0300 Acc: 0.2137\n",
            "val Loss: 2.0818 Acc: 0.2350\n",
            "\n",
            "Training complete in 1m 50s\n",
            "Best val Acc: 0.255000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UQlUKtFYD5pc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Routines"
      ]
    },
    {
      "metadata": {
        "id": "VASgSU9KD-81",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Baseline Model"
      ]
    },
    {
      "metadata": {
        "id": "yNR6vmDhEBT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model_baseline.train()\n",
        "\n",
        "model_baseline = train_model(model_baseline, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_wR099MzELCI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Pruned Model"
      ]
    },
    {
      "metadata": {
        "id": "0eNeKM87EN6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}