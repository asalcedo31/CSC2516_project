{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clean_pruning",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asalcedo31/CSC2516_project/blob/master/clean_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zZ1NXrBF79lD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# set up\n"
      ]
    },
    {
      "metadata": {
        "id": "WcnGoj4y0dVu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.modules import Module\n",
        "import torchvision.models.vgg as tv_vgg\n",
        "import time\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fmjGlo3V0jCd",
        "colab_type": "code",
        "outputId": "74a538e1-a4a7-4f77-c8eb-9a11c01bb608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=5,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "# _,trainset = torch.utils.data.random_split(trainset,(49200,800))\n",
        "# _,trainset = torch.utils.data.random_split(trainset,(49995,5))\n",
        "# print(trainset.__len__())\n",
        "\n",
        "# train_data, val_data = torch.utils.data.random_split(trainset,(int(0.8*len(trainset)),int(0.2*len(trainset))))\n",
        "# print(train_data.__len__(),val_data.__len__() )\n",
        "\n",
        "# trainloader = torch.utils.data.DataLoader(train_data, batch_size=5,\n",
        "#                                           shuffle=True, num_workers=0)\n",
        "# valloader = torch.utils.data.DataLoader(val_data, batch_size=5,\n",
        "#                                           shuffle=True, num_workers=0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "15v78wBX0l32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# image_datasets= {'train': train_data,'val': val_data}\n",
        "# dataloaders = {'train': trainloader, 'val': valloader}\n",
        "\n",
        "# dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "# class_names = image_datasets['train'].classes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u6pkokkh0mx_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def freeze_layers(model_ft, exclude=[]):\n",
        "#   children = list(model_ft.named_children())\n",
        "  for name,param in model_ft.named_parameters():   \n",
        "    if(name not in  exclude):\n",
        "      param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PENRAKM0pLT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def countNonZeroWeights(model):\n",
        "    nonzeros = 0\n",
        "    weights = 0\n",
        "    for name,param in model.named_parameters():\n",
        "        if param is not None:\n",
        "            nonzeros += torch.sum((param != 0).int()).data[0]\n",
        "            weights += torch.sum(param).data[0]\n",
        "    \n",
        "    return nonzeros, weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dyXlRGiw0raM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def set_threshold(model,prop=0.05):\n",
        "  for child in model.named_children():    \n",
        "    for child in child[1].named_children():\n",
        "#       print(child)\n",
        "      if type(child[1]) == MaskedLinear or type(child[1]) == MaskedConv: \n",
        "        child[1].set_threshold(prop=prop)\n",
        "        print(\"layer {}  new threshold {:.4f}\".format(child[0], child[1].threshold))        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ear7nRQY0wwC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model_prune(model, dloaders, dataset_sizes, criterion, optimizer, scheduler,prop=0.05, num_epochs=25, device='cuda',pruning='threshold'):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    print(len(dloaders['train']))\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "                data_idx = 0\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                data_idx = 1\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            i=0\n",
        "      \n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dloaders[phase]:               \n",
        "#                 print(\"batch {} phase {}\".format(i, phase))\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    if pruning == 'L0':\n",
        "                      loss = criterion(outputs, labels,model)\n",
        "                    else:\n",
        "                      loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        \n",
        "                        #HACKS REPLACE!!!!!!\n",
        "                        model.classifier[0].constrain_parameters()\n",
        "                        exp_flops, exp_l0 = model.classifier[0].count_expected_flops_and_l0()\n",
        "                i+=1\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                           \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            if epoch % 5 == 0 and phase == 'train': \n",
        "              if pruning == 'threshold':\n",
        "                set_threshold(model,prop=prop)\n",
        "              elif pruning == 'L0':\n",
        "                print(exp_flops.item(), exp_l0.item())\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FS6V4Olo00JW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Masked:\n",
        "  def make_mask(self, threshold,mask=None):\n",
        "    if mask is None:\n",
        "      print(\"new mask\",device)\n",
        "      self.mask = torch.ones(self.weight.size(), requires_grad=False).to(device)\n",
        "    else:\n",
        "      self.mask = mask      \n",
        "    self.zeros = torch.zeros(self.weight.size(), requires_grad=False).to(device)\n",
        "    self.threshold = threshold\n",
        "  def set_threshold(self,prop=0.05):\n",
        "    unique_weights = torch.unique(self.weight*self.mask)\n",
        "    mask_size = self.mask.reshape(-1).size()[0]\n",
        "#     mask_size = mask_size[0]*mask_size[1]\n",
        "    mask_nonzero = torch.sum(self.mask.view([mask_size]))\n",
        "    mask_total = mask_size\n",
        "    print('nonzero proportion: {:.4f}'.format(mask_nonzero/mask_total))\n",
        "    self.threshold = torch.max(torch.topk(torch.abs(unique_weights),int(prop*unique_weights.size()[0]),largest=False)[0])    \n",
        "  def make_threshold_mask(self):\n",
        "    self.mask = torch.where(torch.abs(self.weight) >= self.threshold,self.mask,self.zeros).to(device)\n",
        "#     self.mask.requires_grad_(requires_grad=False)\n",
        "  def mask_weight(self):\n",
        "    self.weight = torch.nn.Parameter(self.weight*self.mask).to(device) \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Zw6A3138FNi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# L0 pruning\n"
      ]
    },
    {
      "metadata": {
        "id": "QtGNlj9j04hZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MaskedLinear(torch.nn.Linear,Masked):\n",
        "  def __init__(self, in_features, out_features, bias=True, threshold=0.001,mask=None):\n",
        "    super(MaskedLinear, self).__init__(in_features,out_features)\n",
        "    self.make_mask(threshold,mask)\n",
        "  def forward(self, input):\n",
        "    self.make_threshold_mask()\n",
        "    self.mask_weight()\n",
        "#     print(self.mask[125:135,125:135])\n",
        "#     print(self.weight[125:135,125:135])\n",
        "    return F.linear(input, self.weight, self.bias)\n",
        "\n",
        "class MaskedConv(torch.nn.Conv2d,Masked):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, groups, bias=True,threshold=0.0001):\n",
        "    super(MaskedConv,self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "    self.make_mask(threshold)    \n",
        "  def forward(self, input):\n",
        "    self.mask_weight()\n",
        "    return F.conv2d(input, self.weight, self.bias, self.stride,\n",
        "                    self.padding, self.dilation, self.groups)\n",
        "\n",
        "limit_a, limit_b, epsilon = -.1, 1.1, 1e-6\n",
        "device='cuda'\n",
        "class LinearL0(Module):\n",
        "  \"\"\"Implementation of L0 regularization for the input units of a fully connected layer\"\"\"\n",
        "  def __init__(self, in_features, out_features, bias=True, weight_decay=1., droprate_init=0.5, temperature=2./3.,\n",
        "                 lamba=1., local_rep=False, **kwargs):\n",
        "        \"\"\"\n",
        "        :param in_features: Input dimensionality\n",
        "        :param out_features: Output dimensionality\n",
        "        :param bias: Whether we use a bias\n",
        "        :param weight_decay: Strength of the L2 penalty\n",
        "        :param droprate_init: Dropout rate that the L0 gates will be initialized to\n",
        "        :param temperature: Temperature of the concrete distribution\n",
        "        :param lamba: Strength of the L0 penalty\n",
        "        :param local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
        "        \"\"\"\n",
        "        super(LinearL0, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.prior_prec = weight_decay\n",
        "        self.weights = torch.nn.Parameter(torch.Tensor(in_features, out_features).to(device))\n",
        "        self.qz_loga = torch.Tensor(in_features).to(device)\n",
        "#         self.qz_loga = torch.nn.Parameter(torch.Tensor(in_features).to(device))\n",
        "        self.temperature = temperature\n",
        "        self.droprate_init = droprate_init if droprate_init != 0. else 0.5\n",
        "        self.lamba = lamba\n",
        "        self.use_bias = False\n",
        "        self.local_rep = local_rep\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
        "            self.use_bias = True\n",
        "        self.floatTensor = torch.FloatTensor if not torch.cuda.is_available() else torch.cuda.FloatTensor\n",
        "        self.reset_parameters()\n",
        "        print(self)\n",
        "  def reset_parameters(self):\n",
        "      torch.nn.init.kaiming_normal(self.weights, mode='fan_out')\n",
        "\n",
        "      self.qz_loga.data.normal_(math.log(1 - self.droprate_init) - math.log(self.droprate_init), 1e-2)\n",
        "\n",
        "      if self.use_bias:\n",
        "          self.bias.data.fill_(0)\n",
        "\n",
        "  def constrain_parameters(self, **kwargs):\n",
        "      self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
        "\n",
        "  def cdf_qz(self, x):\n",
        "      \"\"\"Implements the CDF of the 'stretched' concrete distribution\"\"\"\n",
        "      xn = (x - limit_a) / (limit_b - limit_a)\n",
        "      logits = math.log(xn) - math.log(1 - xn)\n",
        "      return F.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=epsilon, max=1 - epsilon).to(device)\n",
        "\n",
        "  def quantile_concrete(self, x):\n",
        "      \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "      y = F.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature).to(device)\n",
        "      return y * (limit_b - limit_a) + limit_a\n",
        "\n",
        "  def _reg_w(self):\n",
        "      \"\"\"Expected L0 norm under the stochastic gates, takes into account and re-weights also a potential L2 penalty\"\"\"\n",
        "      logpw_col = torch.sum(- (.5 * self.prior_prec * self.weights.pow(2)) - self.lamba, 1).to(device)\n",
        "      logpw = torch.sum((1 - self.cdf_qz(0)) * logpw_col).to(device)\n",
        "      logpb = 0 if not self.use_bias else - torch.sum(.5 * self.prior_prec * self.bias.pow(2)).to(device)\n",
        "      return logpw + logpb\n",
        "\n",
        "  def regularization(self):\n",
        "      return self._reg_w()\n",
        "\n",
        "  def count_expected_flops_and_l0(self):\n",
        "      \"\"\"Measures the expected floating point operations (FLOPs) and the expected L0 norm\"\"\"\n",
        "      # dim_in multiplications and dim_in - 1 additions for each output neuron for the weights\n",
        "      # + the bias addition for each neuron\n",
        "      # total_flops = (2 * in_features - 1) * out_features + out_features\n",
        "      ppos = torch.sum(1 - self.cdf_qz(0))\n",
        "      expected_flops = (2 * ppos - 1) * self.out_features\n",
        "      expected_l0 = ppos * self.out_features\n",
        "      if self.use_bias:\n",
        "          expected_flops += self.out_features\n",
        "          expected_l0 += self.out_features\n",
        "#       return expected_flops.data[0], expected_l0.data[0]\n",
        "      return expected_flops, expected_l0\n",
        "\n",
        "  def get_eps(self, size):\n",
        "      \"\"\"Uniform random numbers for the concrete distribution\"\"\"\n",
        "      eps = self.floatTensor(size).uniform_(epsilon, 1-epsilon).to(device)\n",
        "      eps = Variable(eps)\n",
        "      return eps\n",
        "\n",
        "  def sample_z(self, batch_size, sample=True):\n",
        "      \"\"\"Sample the hard-concrete gates for training and use a deterministic value for testing\"\"\"\n",
        "      if sample:\n",
        "          eps = self.get_eps(self.floatTensor(batch_size, self.in_features))\n",
        "          z = self.quantile_concrete(eps)\n",
        "          return F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "      else:  # mode\n",
        "          pi = F.sigmoid(self.qz_loga).view(1, self.in_features).expand(batch_size, self.in_features).to(device)\n",
        "          return F.hardtanh(pi * (limit_b - limit_a) + limit_a, min_val=0, max_val=1).to(device)\n",
        "\n",
        "  def sample_weights(self):\n",
        "      z = self.quantile_concrete(self.get_eps(self.floatTensor(self.in_features)))\n",
        "      mask = F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "      return mask.view(self.in_features, 1) * self.weights\n",
        "\n",
        "  def forward(self, input):\n",
        "      if self.local_rep or not self.training:\n",
        "          z = self.sample_z(input.size(0), sample=self.training)\n",
        "          xin = input.mul(z)\n",
        "          output = xin.mm(self.weights)\n",
        "      else:\n",
        "          weights = self.sample_weights()\n",
        "          output = input.mm(weights)\n",
        "      if self.use_bias:\n",
        "          output.add_(self.bias)\n",
        "      return output\n",
        "\n",
        "  def __repr__(self):\n",
        "      s = ('{name}({in_features} -> {out_features}, droprate_init={droprate_init}, '\n",
        "           'lamba={lamba}, temperature={temperature}, weight_decay={prior_prec}, '\n",
        "           'local_rep={local_rep}')\n",
        "      if not self.use_bias:\n",
        "          s += ', bias=False'\n",
        "      s += ')'\n",
        "      return s.format(name=self.__class__.__name__, **self.__dict__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJeDMbDw06i2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mask_network(network,layers_to_mask, threshold=0.002, linear_masking=None,random_init=False, bias=True,masks=None):\n",
        "  \"\"\"\"\n",
        "  replaces linear layers with masked linear layers\n",
        "  network is the initial sequential container\n",
        "  layers is a list of layers to mask\n",
        "  random init is a logical indicating whether to preserve the initial weights or to modify them\n",
        "  \"\"\"\n",
        "  for name,layer in network.named_children():   \n",
        "    if int(name) in layers_to_mask:\n",
        "      layer_mask = None\n",
        "      if masks is not None:\n",
        "        if name in masks:\n",
        "          layer_mask = masks.get(name)      \n",
        "      if type(layer)== torch.nn.Linear and linear_masking is None:\n",
        "        masked_layer = MaskedLinear(layer.in_features, layer.out_features, bias=bias,threshold=threshold,mask=layer_mask)\n",
        "      elif type(layer)== torch.nn.Linear and linear_masking =='L0':\n",
        "        masked_layer = LinearL0(layer.in_features, layer.out_features, bias=bias)\n",
        "      elif type(layer)== torch.nn.Conv2d:\n",
        "        masked_layer = MaskedConv(layer.in_channels, layer.out_channels, layer.kernel_size, layer.stride, layer.padding, layer.dilation,layer.groups, bias=bias, threshold=threshold)\n",
        "      if random_init != True:\n",
        "        masked_layer.weight = copy.deepcopy(layer.weight)\n",
        "        masked_layer.bias = copy.deepcopy(layer.bias)\n",
        "      network[int(name)] = masked_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VdH3hF8PMqRo",
        "colab_type": "code",
        "outputId": "6499b883-21a8-4926-f19f-9e5915647bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2069
        }
      },
      "cell_type": "code",
      "source": [
        "class VGG_L0(tv_vgg.VGG):\n",
        "  def regularization(self):\n",
        "    regularization = 0.\n",
        "    for layer in self.layers:\n",
        "        regularization += - (1. / self.N) * layer.regularization()\n",
        "    if torch.cuda.is_available():\n",
        "        regularization = regularization.cuda()\n",
        "    return regularization\n",
        "  \n",
        "  def regularize(self, N):\n",
        "    regularization = 0.\n",
        "    for child in self.named_children():    \n",
        "      for child in child[1].named_children():\n",
        "        if type(child[1]) == LinearL0:\n",
        "          regularization += - (1. / N) * child[1].regularization()          \n",
        "    if torch.cuda.is_available():\n",
        "        regularization = regularization.cuda()\n",
        "    return regularization\n",
        "  \n",
        "          \n",
        "\n",
        "def vgg16_L0(pretrained=False, **kwargs):\n",
        "  \"\"\"VGG 16-layer model (configuration \"D\")\n",
        "  Args:\n",
        "      pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "  \"\"\"\n",
        "  if pretrained:\n",
        "      kwargs['init_weights'] = False\n",
        "  model = VGG_L0(tv_vgg.make_layers(tv_vgg.cfg['D']), **kwargs)\n",
        "  if pretrained:\n",
        "      model.load_state_dict(model_zoo.load_url(tv_vgg.model_urls['vgg16']))\n",
        "  return model\n",
        "\n",
        "  \n",
        "\n",
        "def run_normal_training_with_L0_pruning(this_trainset):\n",
        "  print(this_trainset.__len__())  \n",
        "  _,mytrainset = torch.utils.data.random_split(this_trainset,(49200,800))\n",
        "  # _,trainset = torch.utils.data.random_split(trainset,(49995,5))\n",
        "  print(mytrainset.__len__())\n",
        "\n",
        "  mytrain_data, myval_data = torch.utils.data.random_split(mytrainset,(int(0.8*len(mytrainset)),int(0.2*len(mytrainset))))\n",
        "  print(mytrain_data.__len__(),myval_data.__len__() )\n",
        "\n",
        "  mytrainloader = torch.utils.data.DataLoader(mytrain_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  myvalloader = torch.utils.data.DataLoader(myval_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  mydataloaders = {'train': mytrainloader, 'val': myvalloader}\n",
        "  image_datasets= {'train': mytrain_data,'val': myval_data}\n",
        "  dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}  \n",
        "\n",
        "  model_ft = vgg16_L0(pretrained=True)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  freeze_layers(model_ft.features, exclude=[])\n",
        "  mask_network(model_ft.classifier,[0],linear_masking=\"L0\")\n",
        "\n",
        "  set_threshold(model_ft)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  def loss_function(outputs,targets, model):\n",
        "    loss = criterion(outputs,targets)\n",
        "    loss += model.regularize(640)\n",
        "    return loss\n",
        "\n",
        "\n",
        "  # Observe that all parameters are being optimized\n",
        "  optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "#   [print(p) for p in model_ft.parameters()]\n",
        "#   return\n",
        "\n",
        "  # Decay LR by a factor of 0.1 every 7 epochs\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "  model_ft = train_model_prune(model_ft, mydataloaders,dataset_sizes, loss_function, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=20, pruning=\"L0\")\n",
        "  \n",
        "run_normal_training_with_L0_pruning(trainset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "800\n",
            "640 160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LinearL0(25088 -> 4096, droprate_init=0.5, lamba=1.0, temperature=0.6666666666666666, weight_decay=1.0, local_rep=False)\n",
            "128\n",
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 133570.0876 Acc: 0.0813\n",
            "170955968.0 85482080.0\n",
            "val Loss: 133567.1133 Acc: 0.1375\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 133567.0071 Acc: 0.1922\n",
            "val Loss: 133566.5557 Acc: 0.3125\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 133566.5800 Acc: 0.3328\n",
            "val Loss: 133566.4395 Acc: 0.3250\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 133566.4303 Acc: 0.3641\n",
            "val Loss: 133566.2407 Acc: 0.4062\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 133566.2543 Acc: 0.4172\n",
            "val Loss: 133566.2256 Acc: 0.3875\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 133566.2024 Acc: 0.4344\n",
            "170955968.0 85482080.0\n",
            "val Loss: 133566.2197 Acc: 0.3750\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 133566.0262 Acc: 0.4859\n",
            "val Loss: 133566.3208 Acc: 0.3875\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 133565.9509 Acc: 0.4969\n",
            "val Loss: 133566.0093 Acc: 0.4563\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 133565.8813 Acc: 0.5281\n",
            "val Loss: 133566.0034 Acc: 0.5000\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 133565.8990 Acc: 0.5297\n",
            "val Loss: 133566.0176 Acc: 0.4813\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 133565.8463 Acc: 0.5484\n",
            "170955968.0 85482080.0\n",
            "val Loss: 133566.0068 Acc: 0.4688\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 133565.8168 Acc: 0.5547\n",
            "val Loss: 133566.0347 Acc: 0.4875\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 133565.8112 Acc: 0.5563\n",
            "val Loss: 133565.9980 Acc: 0.4688\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 133565.7839 Acc: 0.5578\n",
            "val Loss: 133565.9902 Acc: 0.4625\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 133565.7720 Acc: 0.5500\n",
            "val Loss: 133566.0825 Acc: 0.4250\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 133565.7417 Acc: 0.5516\n",
            "170955968.0 85482080.0\n",
            "val Loss: 133565.9409 Acc: 0.5375\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 133565.8114 Acc: 0.5375\n",
            "val Loss: 133565.9570 Acc: 0.5250\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 133565.7893 Acc: 0.5547\n",
            "val Loss: 133565.9883 Acc: 0.4875\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 133565.7928 Acc: 0.5609\n",
            "val Loss: 133565.9268 Acc: 0.5062\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 133565.7770 Acc: 0.5563\n",
            "val Loss: 133565.9595 Acc: 0.4813\n",
            "\n",
            "Training complete in 5m 17s\n",
            "Best val Acc: 0.537500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A0WkKWn3-X8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# run"
      ]
    },
    {
      "metadata": {
        "id": "gcCzS1xv6nZY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_normal_training_with_pruning(this_trainset):\n",
        "  _,mytrainset = torch.utils.data.random_split(this_trainset,(49200,800))\n",
        "\n",
        "  mytrain_data, myval_data = torch.utils.data.random_split(mytrainset,(int(0.8*len(mytrainset)),int(0.2*len(mytrainset))))\n",
        "  print(mytrain_data.__len__(),myval_data.__len__() )\n",
        "\n",
        "  mytrainloader = torch.utils.data.DataLoader(mytrain_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  myvalloader = torch.utils.data.DataLoader(myval_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  mydataloaders = {'train': mytrainloader, 'val': myvalloader}\n",
        "  image_datasets= {'train': mytrain_data,'val': myval_data}\n",
        "  dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "  model_ft = models.vgg16(pretrained=True)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  freeze_layers(model_ft.features, exclude=[])\n",
        "  mask_network(model_ft.classifier,[0],threshold=0.0001)\n",
        "  set_threshold(model_ft)\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()  \n",
        "     \n",
        "  # Observe that all parameters are being optimized\n",
        "  optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  # Decay LR by a factor of 0.1 every 7 epochs\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "  \n",
        "  \n",
        "  model_ft = train_model_prune(model_ft, mydataloaders,dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=2)\n",
        "  \n",
        "# run_normal_training_with_pruning(trainset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GShMvVfL0-mQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_meta_prune(model,trainset, outer_steps, num_samples=800, device='cuda'):\n",
        "  mask_dict = {'0':torch.ones(model.classifier[0].weight.size()).to(device)}\n",
        "  shuffled_train = torch.utils.data.RandomSampler(trainset)\n",
        "  train_sample_list = list(torch.utils.data.BatchSampler(shuffled_train,num_samples,False))\n",
        "  shuffled_train = [x for x in shuffled_train]\n",
        "  for i in range(outer_steps):\n",
        "#     train_sample = [trainset[j] for j in train_sample_list[i]] \n",
        "    \n",
        "#     print(len(train_sample))\n",
        "    _,train_sample = torch.utils.data.random_split(trainset,(49200,800))\n",
        "    train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "    valloader = torch.utils.data.DataLoader(val_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "    \n",
        "    subdataloaders = {'train': trainloader, 'val': valloader}\n",
        "    image_datasets= {'train': train_data,'val': val_data}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    \n",
        "    model_ft = models.vgg16(pretrained=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "    # freeze_layers(model_ft.features, exclude=['28.weight'])\n",
        "    freeze_layers(model_ft.features)   \n",
        "    mask_network(model_ft.classifier,[0],threshold=0.0001,masks=mask_dict)\n",
        "    model_ft = train_model_prune(model_ft, subdataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=10, prop=0.1)\n",
        "    mask_dict = {'0':model_ft.classifier[0].mask}\n",
        "#     set_threshold(model_ft)\n",
        "\n",
        "#     cost = meta_objective({'train':trainloader, 'val':valoader}, model, optimizer, inner_epochs)\n",
        "\n",
        "\n",
        "# model_ft = models.vgg16(pretrained=True)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model_ft = model_ft.to(device)\n",
        "\n",
        "# train_meta_prune(model_ft,trainset,15)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}