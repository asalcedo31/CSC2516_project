{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of clean_pruning",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asalcedo31/CSC2516_project/blob/master/clean_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zZ1NXrBF79lD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# set up\n"
      ]
    },
    {
      "metadata": {
        "id": "WcnGoj4y0dVu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.modules import Module\n",
        "import torchvision.models.vgg as tv_vgg\n",
        "import time\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import re\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fmjGlo3V0jCd",
        "colab_type": "code",
        "outputId": "ece92472-beb3-4653-8766-f01e0b2e3d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=5,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "# _,trainset = torch.utils.data.random_split(trainset,(49200,800))\n",
        "# _,trainset = torch.utils.data.random_split(trainset,(49995,5))\n",
        "# print(trainset.__len__())\n",
        "\n",
        "# train_data, val_data = torch.utils.data.random_split(trainset,(int(0.8*len(trainset)),int(0.2*len(trainset))))\n",
        "# print(train_data.__len__(),val_data.__len__() )\n",
        "\n",
        "# trainloader = torch.utils.data.DataLoader(train_data, batch_size=5,\n",
        "#                                           shuffle=True, num_workers=0)\n",
        "# valloader = torch.utils.data.DataLoader(val_data, batch_size=5,\n",
        "#                                           shuffle=True, num_workers=0)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "15v78wBX0l32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# image_datasets= {'train': train_data,'val': val_data}\n",
        "# dataloaders = {'train': trainloader, 'val': valloader}\n",
        "\n",
        "# dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "# class_names = image_datasets['train'].classes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u6pkokkh0mx_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def freeze_layers(model_ft, exclude=[]):\n",
        "#   children = list(model_ft.named_children())\n",
        "  for name,param in model_ft.named_parameters():   \n",
        "    if(name not in  exclude):\n",
        "      param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PENRAKM0pLT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def countNonZeroWeights(model):\n",
        "    nonzeros = 0\n",
        "    weights = 0\n",
        "    for name,param in model.named_parameters():\n",
        "        if param is not None:\n",
        "            nonzeros += torch.sum((param != 0).int()).data[0]\n",
        "            weights += torch.sum(param).data[0]\n",
        "    \n",
        "    return nonzeros, weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dyXlRGiw0raM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def set_threshold(model,prop=0.05):\n",
        "  for child in model.named_children():    \n",
        "    for child in child[1].named_children():\n",
        "#       print(child)\n",
        "      if type(child[1]) == MaskedLinear or type(child[1]) == MaskedConv: \n",
        "        child[1].set_threshold(prop=prop)\n",
        "        print(\"layer {}  new threshold {:.4f}\".format(child[0], child[1].threshold))        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ear7nRQY0wwC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model_prune(model, dloaders, dataset_sizes, criterion, optimizer, scheduler,prop=0.05, num_epochs=25, device='cuda',pruning='threshold'):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    print(len(dloaders['train']))\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "                data_idx = 0\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                data_idx = 1\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            i=0\n",
        "            \n",
        "#             print(dloaders[phase].__iter__().next())\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dloaders[phase]:               \n",
        "#                 print(\"batch {} phase {}\".format(i, phase))\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    if pruning == 'L0':\n",
        "                      loss = criterion(outputs, labels,model)\n",
        "                    else:\n",
        "                      loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        model.clamp_parameters()\n",
        "                        exp_flops, exp_l0 = model.get_exp_flops_l0()\n",
        "                i+=1\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                           \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            if epoch % 5 == 0 and phase == 'train': \n",
        "              if pruning == 'threshold':\n",
        "                set_threshold(model,prop=prop)\n",
        "              elif pruning == 'L0':\n",
        "                print(exp_flops.item(), exp_l0.item())\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return epoch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FS6V4Olo00JW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Masked:\n",
        "  def make_mask(self, threshold,mask=None):\n",
        "    if mask is None:\n",
        "      print(\"new mask\",device)\n",
        "      self.mask = torch.ones(self.weight.size(), requires_grad=False).to(device)\n",
        "    else:\n",
        "      self.mask = mask      \n",
        "    self.zeros = torch.zeros(self.weight.size(), requires_grad=False).to(device)\n",
        "    self.threshold = threshold\n",
        "  def set_threshold(self,prop=0.05):\n",
        "    unique_weights = torch.unique(self.weight*self.mask)\n",
        "    mask_size = self.mask.reshape(-1).size()[0]\n",
        "#     mask_size = mask_size[0]*mask_size[1]\n",
        "    mask_nonzero = torch.sum(self.mask.view([mask_size]))\n",
        "    mask_total = mask_size\n",
        "    print('nonzero proportion: {:.4f}'.format(mask_nonzero/mask_total))\n",
        "    self.threshold = torch.max(torch.topk(torch.abs(unique_weights),int(prop*unique_weights.size()[0]),largest=False)[0])    \n",
        "  def make_threshold_mask(self):\n",
        "    self.mask = torch.where(torch.abs(self.weight) >= self.threshold,self.mask,self.zeros).to(device)\n",
        "#     self.mask.requires_grad_(requires_grad=False)\n",
        "  def mask_weight(self):\n",
        "    self.weight = torch.nn.Parameter(self.weight*self.mask).to(device) \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Zw6A3138FNi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# L0 pruning\n"
      ]
    },
    {
      "metadata": {
        "id": "QtGNlj9j04hZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MaskedLinear(torch.nn.Linear,Masked):\n",
        "  def __init__(self, in_features, out_features, bias=True, threshold=0.001,mask=None):\n",
        "    super(MaskedLinear, self).__init__(in_features,out_features)\n",
        "    self.make_mask(threshold,mask)\n",
        "  def forward(self, input):\n",
        "    self.make_threshold_mask()\n",
        "    self.mask_weight()\n",
        "#     print(self.mask[125:135,125:135])\n",
        "#     print(self.weight[125:135,125:135])\n",
        "    return F.linear(input, self.weight, self.bias)\n",
        "\n",
        "class MaskedConv(torch.nn.Conv2d,Masked):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, groups, bias=True,threshold=0.0001):\n",
        "    super(MaskedConv,self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "    self.make_mask(threshold)    \n",
        "  def forward(self, input):\n",
        "    self.mask_weight()\n",
        "    return F.conv2d(input, self.weight, self.bias, self.stride,\n",
        "                    self.padding, self.dilation, self.groups)\n",
        "  \n",
        "limit_a, limit_b, epsilon = -.1, 1.1, 1e-6\n",
        "device='cuda'\n",
        "\n",
        "class LinearL0(Module):\n",
        "  \"\"\"Implementation of L0 regularization for the input units of a fully connected layer\"\"\"\n",
        "  def __init__(self, in_features, out_features, bias=True, weight_decay=1., droprate_init=0.5, temperature=2./3.,\n",
        "                 lamba=1., local_rep=False, qz_loga=None, **kwargs):\n",
        "        \"\"\"\n",
        "        :param in_features: Input dimensionality\n",
        "        :param out_features: Output dimensionality\n",
        "        :param bias: Whether we use a bias\n",
        "        :param weight_decay: Strength of the L2 penalty\n",
        "        :param droprate_init: Dropout rate that the L0 gates will be initialize d to\n",
        "        :param temperature: Temperature of the concrete distribution\n",
        "        :param lamba: Strength of the L0 penalty\n",
        "        :param local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
        "        \"\"\"\n",
        "        super(LinearL0, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.prior_prec = weight_decay\n",
        "        self.weight = torch.nn.Parameter(torch.Tensor(in_features, out_features).to(device))\n",
        "#         self.qz_loga = torch.Tensor(in_features).to(device)\n",
        "        self.qz_loga = torch.nn.Parameter(torch.Tensor(in_features).to(device))\n",
        "        self.temperature = temperature\n",
        "        self.droprate_init = droprate_init if droprate_init != 0. else 0.5\n",
        "        self.lamba = lamba\n",
        "        self.use_bias = False\n",
        "        self.local_rep = local_rep\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.Tensor(out_features).to(device))\n",
        "            self.use_bias = True\n",
        "        self.floatTensor = torch.FloatTensor if not torch.cuda.is_available() else torch.cuda.FloatTensor\n",
        "        self.reset_parameters()\n",
        "        if qz_loga is not None:\n",
        "          self.qz_loga = qz_loga\n",
        "\n",
        "  def reset_parameters(self):\n",
        "      torch.nn.init.kaiming_normal(self.weight, mode='fan_out')\n",
        "      self.qz_loga.data.normal_(math.log(1 - self.droprate_init) - math.log(self.droprate_init), 1e-2)\n",
        "\n",
        "      if self.use_bias:\n",
        "          self.bias.data.fill_(0)\n",
        "\n",
        "  def constrain_parameters(self, **kwargs):\n",
        "      self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
        "\n",
        "  def cdf_qz(self, x):\n",
        "      \"\"\"Implements the CDF of the 'stretched' concrete distribution\"\"\"\n",
        "      xn = (x - limit_a) / (limit_b - limit_a)\n",
        "      logits = math.log(xn) - math.log(1 - xn)\n",
        "      return F.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=epsilon, max=1 - epsilon).to(device)\n",
        "\n",
        "  def quantile_concrete(self, x):\n",
        "      \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "      y = F.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature).to(device)\n",
        "      return y * (limit_b - limit_a) + limit_a\n",
        "\n",
        "  def _reg_w(self):\n",
        "      \"\"\"Expected L0 norm under the stochastic gates, takes into account and re-weights also a potential L2 penalty\"\"\"\n",
        "      logpw_col = torch.sum(- (.5 * self.prior_prec * self.weight.pow(2)) - self.lamba, 1).to(device)\n",
        "      logpw = torch.sum((1 - self.cdf_qz(0)) * logpw_col).to(device)\n",
        "      logpb = 0 if not self.use_bias else - torch.sum(.5 * self.prior_prec * self.bias.pow(2)).to(device)\n",
        "      return logpw + logpb\n",
        "\n",
        "  def regularization(self):\n",
        "      return self._reg_w()\n",
        "\n",
        "  def count_expected_flops_and_l0(self):\n",
        "      \"\"\"Measures the expected floating point operations (FLOPs) and the expected L0 norm\"\"\"\n",
        "      # dim_in multiplications and dim_in - 1 additions for each output neuron for the weights\n",
        "      # + the bias addition for each neuron\n",
        "      # total_flops = (2 * in_features - 1) * out_features + out_features\n",
        "      ppos = torch.sum(1 - self.cdf_qz(0))\n",
        "      expected_flops = (2 * ppos - 1) * self.out_features\n",
        "      expected_l0 = ppos * self.out_features\n",
        "      if self.use_bias:\n",
        "          expected_flops += self.out_features\n",
        "          expected_l0 += self.out_features\n",
        "#       return expected_flops.data[0], expected_l0.data[0]\n",
        "      return expected_flops, expected_l0\n",
        "\n",
        "  def get_eps(self, size):\n",
        "      \"\"\"Uniform random numbers for the concrete distribution\"\"\"\n",
        "      eps = self.floatTensor(size).uniform_(epsilon, 1-epsilon).to(device)\n",
        "      eps = Variable(eps)\n",
        "      return eps\n",
        "\n",
        "  def sample_z(self, batch_size, sample=True):\n",
        "      \"\"\"Sample the hard-concrete gates for training and use a deterministic value for testing\"\"\"\n",
        "      if sample:\n",
        "          eps = self.get_eps(self.floatTensor(batch_size, self.in_features))\n",
        "          z = self.quantile_concrete(eps)\n",
        "          return F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "      else:  # mode\n",
        "          pi = F.sigmoid(self.qz_loga).view(1, self.in_features).expand(batch_size, self.in_features).to(device)\n",
        "          return F.hardtanh(pi * (limit_b - limit_a) + limit_a, min_val=0, max_val=1).to(device)\n",
        "\n",
        "  def sample_weights(self):\n",
        "      z = self.quantile_concrete(self.get_eps(self.floatTensor(self.in_features)))\n",
        "      mask = F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "      return mask.view(self.in_features, 1) * self.weight\n",
        "\n",
        "  def forward(self, input):\n",
        "      if self.local_rep or not self.training:\n",
        "          z = self.sample_z(input.size(0), sample=self.training)\n",
        "          xin = input.mul(z)\n",
        "          output = xin.mm(self.weight)\n",
        "      else:\n",
        "          weights = self.sample_weights()\n",
        "          output = input.mm(weights)\n",
        "      if self.use_bias:\n",
        "          output.add_(self.bias)\n",
        "      return output\n",
        "\n",
        "  def __repr__(self):\n",
        "      s = ('{name}({in_features} -> {out_features}, droprate_init={droprate_init}, '\n",
        "           'lamba={lamba}, temperature={temperature}, weight_decay={prior_prec}, '\n",
        "           'local_rep={local_rep}')\n",
        "      if not self.use_bias:\n",
        "          s += ', bias=False'\n",
        "      s += ')'\n",
        "      return s.format(name=self.__class__.__name__, **self.__dict__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJeDMbDw06i2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mask_network(network,layers_to_mask, n=None, threshold=0.002, linear_masking=None,random_init=False, bias=True,masks=None):\n",
        "  \"\"\"\"\n",
        "  replaces linear layers with masked linear layers\n",
        "  network is the initial sequential container\n",
        "  layers is a list of layers to mask\n",
        "  random init is a logical indicating whether to preserve the initial weights or to modify them\n",
        "  \"\"\"\n",
        "  network.masked_layers=[]\n",
        "  for name,layer in network.named_children():   \n",
        "    if int(name) in layers_to_mask:\n",
        "      layer_mask = None\n",
        "      if masks is not None:\n",
        "        if name in masks:\n",
        "          layer_mask = masks.get(name)      \n",
        "      if type(layer)== torch.nn.Linear and linear_masking is None:\n",
        "        masked_layer = MaskedLinear(layer.in_features, layer.out_features, bias=bias,threshold=threshold,masks=layer_mask)\n",
        "      elif type(layer)== torch.nn.Linear and linear_masking =='L0':\n",
        "        masked_layer = LinearL0(layer.in_features, layer.out_features, bias=bias, lamba=0.5/n,qz_loga=layer_mask)\n",
        "#         masked_layer = LinearL0(layer.in_features, layer.out_features, bias=bias)\n",
        "        network.masked_layers.append(masked_layer)\n",
        "      elif type(layer)== torch.nn.Conv2d:\n",
        "        masked_layer = MaskedConv(layer.in_channels, layer.out_channels, layer.kernel_size, layer.stride, layer.padding, layer.dilation,layer.groups, bias=bias, threshold=threshold)\n",
        "      if random_init != True and linear_masking != 'L0':\n",
        "        masked_layer.weight = copy.deepcopy(layer.weight)\n",
        "        masked_layer.bias = copy.deepcopy(layer.bias)\n",
        "      elif random_init != True and linear_masking == 'L0':\n",
        "        weight_copy = torch.transpose(copy.deepcopy(layer.weight),0,1)\n",
        "        masked_layer.weights = torch.nn.Parameter(weight_copy)\n",
        "        \n",
        "      network[int(name)] = masked_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-BTMME3oonFj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class L0_Meta_Objective():\n",
        "  def __init__(self,train_data,val_data):\n",
        "    self.trainloader = torch.utils.data.DataLoader(train_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "    \n",
        "    self.valloader = torch.utils.data.DataLoader(val_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "    \n",
        "    self.dataloaders = {'train':self.trainloader, 'val': self.valloader}\n",
        "    self.image_datasets= {'train': train_data,'val': val_data}\n",
        "    self.dataset_sizes = {x: len(self.image_datasets[x]) for x in ['train', 'val']}\n",
        "    self.model_ft = vgg16_L0(pretrained=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.model_ft = self.model_ft.to(device)    \n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "    freeze_layers(self.model_ft.features, exclude=[])    \n",
        "    \n",
        "  def add_L0_layers(self,layers_to_prune,initial_masks=None,n=None):    \n",
        "    mask_network(self.model_ft.classifier,layers_to_prune,n=n,linear_masking=\"L0\",masks=initial_masks)\n",
        "    self.model_ft.masked_layers = self.model_ft.classifier.masked_layers  \n",
        "  \n",
        "  def inner_loss_function(self,outputs,targets,model):\n",
        "      loss = self.criterion(outputs,targets)\n",
        "      loss += self.model_ft.regularize(200)\n",
        "      return loss\n",
        "        \n",
        "  def inner_train_loop(self, inner_epochs):    \n",
        "    # Observe that all parameters are being optimized\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    self.optimizer_ft = optim.Adam(self.model_ft.parameters(), lr=0.001)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(self.optimizer_ft, step_size=7, gamma=0.1)\n",
        "    final_mod = train_model_prune(self.model_ft, self.dataloaders,self.dataset_sizes, self.inner_loss_function, self.optimizer_ft, exp_lr_scheduler,\n",
        "                                    num_epochs=inner_epochs, pruning=\"L0\")\n",
        "    return final_mod\n",
        "    \n",
        "def get_initial_masks(model,layers_to_mask,droprate_init=0.5):\n",
        "    initial_masks = {}    \n",
        "    for name,layer in model.named_children():\n",
        "      if int(name) in layers_to_mask:\n",
        "        qz_loga = torch.nn.Parameter(torch.Tensor(layer.in_features).to(device))\n",
        "        qz_loga.data.normal_(math.log(1 - droprate_init) - math.log(droprate_init), 1e-2)\n",
        "        initial_masks[name] = qz_loga                             \n",
        "    return initial_masks\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VdH3hF8PMqRo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VGG_L0(tv_vgg.VGG):\n",
        "  def regularization(self):\n",
        "    regularization = 0.\n",
        "    for layer in self.layers:\n",
        "        regularization += - (1. / self.N) * layer.regularization()\n",
        "    if torch.cuda.is_available():\n",
        "        regularization = regularization.cuda()\n",
        "    return regularization\n",
        "  \n",
        "  def regularize(self, N):\n",
        "    regularization = 0.\n",
        "    for layer in self.masked_layers:\n",
        "          regularization += - (1. / N) * layer.regularization()          \n",
        "    if torch.cuda.is_available():\n",
        "        regularization = regularization.cuda()\n",
        "    return regularization\n",
        "    \n",
        "  def clamp_parameters(self):\n",
        "    for layer in self.masked_layers:\n",
        "      layer.constrain_parameters()\n",
        "  \n",
        "  def get_exp_flops_l0(self):\n",
        "    total_flops = 0\n",
        "    total_l0 = 0\n",
        "    for layer in self.masked_layers:\n",
        "      exp_flops, exp_l0 = layer.count_expected_flops_and_l0()\n",
        "      total_flops += exp_flops\n",
        "      total_l0 += exp_l0\n",
        "    return total_flops, total_l0  \n",
        "          \n",
        "\n",
        "def vgg16_L0(pretrained=False, **kwargs):\n",
        "  \"\"\"VGG 16-layer model (configuration \"D\")\n",
        "  Args:\n",
        "      pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "  \"\"\"\n",
        "  if pretrained:\n",
        "      kwargs['init_weights'] = False\n",
        "  model = VGG_L0(tv_vgg.make_layers(tv_vgg.cfg['D']), **kwargs)\n",
        "  if pretrained:\n",
        "      model.load_state_dict(model_zoo.load_url(tv_vgg.model_urls['vgg16']))\n",
        "  return model\n",
        "\n",
        "\n",
        "def run_normal_training_with_L0_pruning(this_trainset):\n",
        "  print(this_trainset.__len__())  \n",
        "  _,mytrainset = torch.utils.data.random_split(this_trainset,(49800,200))\n",
        "  # _,trainset = torch.utils.data.random_split(trainset,(49995,5))\n",
        "  print(mytrainset.__len__())\n",
        "\n",
        "  mytrain_data, myval_data = torch.utils.data.random_split(mytrainset,(int(0.8*len(mytrainset)),int(0.2*len(mytrainset))))\n",
        "  print(mytrain_data.__len__(),myval_data.__len__() )\n",
        "\n",
        "  mytrainloader = torch.utils.data.DataLoader(mytrain_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  myvalloader = torch.utils.data.DataLoader(myval_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  mydataloaders = {'train': mytrainloader, 'val': myvalloader}\n",
        "  image_datasets= {'train': mytrain_data,'val': myval_data}\n",
        "  dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}  \n",
        "\n",
        "  model_ft = vgg16_L0(pretrained=True)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  freeze_layers(model_ft.features, exclude=[])\n",
        "  mask_network(model_ft.classifier,[0,3,6],linear_masking=\"L0\", n=200,random_init=False)\n",
        "  model_ft.masked_layers = model_ft.classifier.masked_layers\n",
        "  \n",
        "  print(model_ft.classifier[0].weights.size())  \n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  def loss_function(outputs,targets, model):\n",
        "    loss = criterion(outputs,targets)\n",
        "    loss += model.regularize(200)\n",
        "    return loss\n",
        "\n",
        "\n",
        "  # Observe that all parameters are being optimized\n",
        "  optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "#   [print(p) for p in model_ft.parameters()]\n",
        "#   return\n",
        "\n",
        "  # Decay LR by a factor of 0.1 every 7 epochs\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "  MO = L0_Meta_Objective(mytrain_data,myval_data)\n",
        "  MO.add_L0_layers([0,3,6],n=200)\n",
        "\n",
        "#   model_ft = train_model_prune(model_ft, mydataloaders,dataset_sizes, loss_function, optimizer_ft, exp_lr_scheduler,\n",
        "#                        num_epochs=20, pruning=\"L0\")\n",
        "\n",
        "  model_ft = MO.inner_train_loop(20)\n",
        "  \n",
        "# run_normal_training_with_L0_pruning(trainset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YaBU663jnTiW",
        "colab_type": "code",
        "outputId": "116b829c-a2a4-48a3-d7ec-54ce28b760a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10693
        }
      },
      "cell_type": "code",
      "source": [
        "def gd_step(cost, params, lrate):\n",
        "    \"\"\"Perform one gradient descent step on the given cost function with learning\n",
        "    rate lrate. Returns a new set of parameters, and (IMPORTANT) does not modify\n",
        "    the input parameters.\"\"\"\n",
        "\n",
        "    cost_grad_fun = torch.autograd.grad(cost)\n",
        "    grads = cost_grad_fun(params)\n",
        "    opt_params = {}\n",
        "    for p in params.keys():\n",
        "        opt_params[p] = params[p] - grads[p]*lrate\n",
        "    return opt_params\n",
        "\n",
        "def set_params(model,inparams):\n",
        "  for name,layer in model.named_children():\n",
        "    for name_p, p in layer.named_parameters():\n",
        "      key = str(name)+'.'+name_p\n",
        "      if key in inparams.keys():\n",
        "#         print(layer._parameters)\n",
        "        layer._parameters[name_p] = inparams[key]\n",
        "#         print(key)\n",
        "#         print(inparams[key])\n",
        "  return\n",
        "\n",
        "\n",
        "\n",
        "#MAML attempt\n",
        "def train_meta_prune_L0(trainset, valset, layers_to_prune,outer_steps=5, inner_steps=5, num_samples=800, inner_lr=0.001,outer_lr=0.001, device='cuda'):  \n",
        "  shuffled_train = torch.utils.data.RandomSampler(trainset)\n",
        "  train_sample_list = list(torch.utils.data.BatchSampler(shuffled_train,num_samples,False))\n",
        "  shuffled_train = [x for x in shuffled_train]\n",
        "  #sample model for dimensions\n",
        "  model_ft = vgg16_L0(pretrained=True)\n",
        "  initial_masks = get_initial_masks(model_ft.classifier, [0])\n",
        "#   outer_optimizer = optim.RMSprop(initial_masks.values(), lr=0.001, momentum=0.9)\n",
        "#   print(initial_masks)\n",
        "  \n",
        "  for i in range(outer_steps):\n",
        "#     for phase in ['train','val']\n",
        "    train_sample = [trainset[j] for j in train_sample_list[i]] \n",
        "    train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "    \n",
        "#     outer_optimizer.zero_grad()\n",
        "    #meta obejctive object contains model and loaders\n",
        "    MO = L0_Meta_Objective(train_data,val_data)\n",
        "        \n",
        "    #make separate inital masks for each layer    \n",
        "    MO.add_L0_layers(layers_to_prune,initial_masks)\n",
        "    print(\"new outer\")\n",
        "    losses_ta\n",
        "    \n",
        "    for i in range(5):\n",
        "      print(\"i \",i)\n",
        "      inputs,labels = MO.trainloader.__iter__().next()\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      preds = MO.model_ft(inputs)\n",
        "      \n",
        "      loss =  F.cross_entropy(preds,labels)+MO.model_ft.regularize(640)\n",
        "      vars = dict(MO.model_ft.classifier.named_parameters())\n",
        "      grad = torch.autograd.grad(loss,vars.values())\n",
        "#       grad = torch.autograd.grad(loss,[MO.model_ft.classifier[0].weights,MO.model_ft.classifier[3].weight, MO.model_ft.classifier[6].weight, MO.model_ft.classifier[0].qz_loga] )\n",
        "      params = {}\n",
        "      \n",
        "      for p in zip(grad,vars.values(),vars.keys()):\n",
        "#         print(p[1]-p[0])\n",
        "        params[p[2]] = p[1] - inner_lr* p[0]\n",
        "      \n",
        "#       print(params['3.weight'])  \n",
        "#       params = dict(p[0]= p[1] - inner_lr* p[0], zip(grad,vars.values(),vars.keys()))\n",
        "      set_params( MO.model_ft.classifier,params)\n",
        "#       loss += self.model_ft.regularize(640)\n",
        "      print('final inner loss {:.4f}'.format(loss))\n",
        "    \n",
        "    inputs,labels = MO.dataloaders['train'].__iter__().next()\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    final_preds = MO.model_ft(inputs)\n",
        "    final_loss =  F.cross_entropy(final_preds,labels)+MO.model_ft.regularize(640)\n",
        "    print('final outer loss {:.4f}'.format(final_loss))\n",
        "    outer_grad = torch.autograd.grad(final_loss,initial_masks.values())\n",
        "    for p in zip(outer_grad,initial_masks.values(),initial_masks.keys()):\n",
        "#           print(p[1]-p[0])\n",
        "        initial_masks[p[2]] = torch.nn.Parameter(p[1] - outer_lr* p[0])\n",
        "\n",
        "# train_meta_prune_L0(trainset,testset,[0], outer_steps=10,inner_lr=0.1, outer_lr=0.01)\n",
        "\n",
        "\n",
        "def set_params_from_t(model,inparams):\n",
        "  for name,layer in model.named_children():\n",
        "    for name_p, p in layer.named_parameters():\n",
        "      key = str(name)+'.'+name_p\n",
        "      if key in inparams.keys():        \n",
        "        p_new = inparams[key].data\n",
        "#         p_new.requires_grad = True        \n",
        "#           model[int(name)].weight = p\n",
        "#         print(model[int(name)]._parameters[name_p])\n",
        "        model[int(name)]._parameters[name_p].data.copy_(p_new)\n",
        "#   print(model[6]._parameters[name_p])\n",
        "  return \n",
        "\n",
        "def meta_prune_reptile(trainset, valset, layers_to_prune,outer_steps=10, inner_steps=5, num_samples=200, inner_lr=0.001,outer_lr=0.001, device='cuda'):  \n",
        "  shuffled_train = torch.utils.data.RandomSampler(trainset)\n",
        "  train_sample_list = list(torch.utils.data.BatchSampler(shuffled_train,num_samples,False))\n",
        "  \n",
        "  train_sample = train_sample_list[0]\n",
        "  train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "  \n",
        "  MO = L0_Meta_Objective(train_data,val_data)\n",
        "  \n",
        "  #make separate inital masks for each layer    \n",
        "  MO.add_L0_layers(layers_to_prune,n=200)\n",
        "  initial_p = {}\n",
        "  for name, param in OrderedDict(MO.model_ft.classifier.named_parameters()).items():\n",
        "#     print(param.data)\n",
        "    i_p = torch.ones(param.data.size(),requires_grad=False).to(device)\n",
        "    i_p.copy_(param.data)\n",
        "#     i_p\n",
        "#     i_p.requires_grad = False\n",
        "    initial_p[name] = i_p\n",
        "#   print(initial_p)\n",
        "  for i in range(1,outer_steps):\n",
        "    print(\"new outer \", i)\n",
        "    train_sample = [trainset[j] for j in train_sample_list[i]] \n",
        "    train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "    MO = L0_Meta_Objective(train_data,val_data)\n",
        "    MO.add_L0_layers(layers_to_prune,n=200)\n",
        "    print('initial',initial_p['6.qz_loga'][125:130])\n",
        "#     print(dataloaders['train'].__iter__().next())\n",
        "#     set_params_from_t(MO.model_ft.classifier, initial_p)\n",
        "  \n",
        "#     print('weights',MO.model_ft.classifier[6].weight[125:130,125:130])\n",
        "    new_mod = MO.inner_train_loop(5)\n",
        "#     print('after', MO.model_ft.classifier[6].weight[125:130,125:130])\n",
        "#     print('initial after',initial_p['6.weight'][125:130,125:130])\n",
        "    new_p = OrderedDict(MO.model_ft.classifier.named_parameters())\n",
        "    \n",
        "    for name, p in initial_p.items():\n",
        "      update_p = (p - new_p[name].data)/outer_steps*outer_lr\n",
        "#       if name == '6.weight':\n",
        "#         print('p',p[125:130,125:130])\n",
        "#         print('update',update_p[125:130,125:130])\n",
        "      initial_p[name] = p+update_p.data\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "#   for i in range(outer_steps):\n",
        "meta_prune_reptile(trainset,testset,[0,3,6], outer_steps=20,inner_lr=0.1, outer_lr=0.1)    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new outer  1\n",
            "initial tensor([-0.0002, -0.0122,  0.0169, -0.0040, -0.0074], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 1304.4893 Acc: 0.1750\n",
            "204578928.0 102298656.0\n",
            "val Loss: 1292.5214 Acc: 0.2500\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.8639 Acc: 0.3438\n",
            "val Loss: 1281.4763 Acc: 0.2750\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1276.7361 Acc: 0.5813\n",
            "val Loss: 1273.8656 Acc: 0.3500\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1267.9896 Acc: 0.7563\n",
            "val Loss: 1265.4070 Acc: 0.2750\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1259.9328 Acc: 0.7438\n",
            "val Loss: 1257.2574 Acc: 0.3000\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.350000\n",
            "new outer  2\n",
            "initial tensor([ 0.0003, -0.0121,  0.0174, -0.0036, -0.0070], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1305.4203 Acc: 0.1125\n",
            "204580896.0 102299640.0\n",
            "val Loss: 1292.2144 Acc: 0.2000\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.9439 Acc: 0.3500\n",
            "val Loss: 1281.8355 Acc: 0.2500\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1277.0829 Acc: 0.5813\n",
            "val Loss: 1273.6596 Acc: 0.3250\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.2806 Acc: 0.7812\n",
            "val Loss: 1265.3886 Acc: 0.3250\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1260.0262 Acc: 0.7188\n",
            "val Loss: 1258.0989 Acc: 0.3000\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.325000\n",
            "new outer  3\n",
            "initial tensor([ 0.0004, -0.0117,  0.0180, -0.0031, -0.0066], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1304.7503 Acc: 0.1125\n",
            "204575600.0 102296992.0\n",
            "val Loss: 1292.8030 Acc: 0.0250\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.8318 Acc: 0.2563\n",
            "val Loss: 1281.5317 Acc: 0.3500\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1276.5573 Acc: 0.5750\n",
            "val Loss: 1273.0538 Acc: 0.2750\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1267.8377 Acc: 0.6625\n",
            "val Loss: 1264.9015 Acc: 0.3000\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1259.7753 Acc: 0.7125\n",
            "val Loss: 1256.6996 Acc: 0.3000\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.350000\n",
            "new outer  4\n",
            "initial tensor([ 0.0006, -0.0115,  0.0182, -0.0028, -0.0064], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1304.7106 Acc: 0.1188\n",
            "204576864.0 102297624.0\n",
            "val Loss: 1293.4020 Acc: 0.1000\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.9119 Acc: 0.3375\n",
            "val Loss: 1282.8558 Acc: 0.2000\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1277.0685 Acc: 0.5563\n",
            "val Loss: 1273.8682 Acc: 0.2000\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.4349 Acc: 0.7250\n",
            "val Loss: 1265.8577 Acc: 0.2000\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1260.0373 Acc: 0.7375\n",
            "val Loss: 1257.6236 Acc: 0.2500\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.250000\n",
            "new outer  5\n",
            "initial tensor([ 0.0014, -0.0108,  0.0185, -0.0021, -0.0057], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1305.4455 Acc: 0.0813\n",
            "204578192.0 102298288.0\n",
            "val Loss: 1292.1179 Acc: 0.3500\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1287.0855 Acc: 0.2750\n",
            "val Loss: 1282.0270 Acc: 0.2250\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1277.1056 Acc: 0.5687\n",
            "val Loss: 1273.3737 Acc: 0.3250\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.2062 Acc: 0.7313\n",
            "val Loss: 1265.1299 Acc: 0.3750\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1259.8687 Acc: 0.7688\n",
            "val Loss: 1257.0754 Acc: 0.4250\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.425000\n",
            "new outer  6\n",
            "initial tensor([ 0.0019, -0.0106,  0.0191, -0.0015, -0.0052], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1305.2723 Acc: 0.0688\n",
            "204578304.0 102298344.0\n",
            "val Loss: 1292.7249 Acc: 0.2000\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1287.2764 Acc: 0.3125\n",
            "val Loss: 1282.3092 Acc: 0.3500\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1277.2205 Acc: 0.5750\n",
            "val Loss: 1273.9144 Acc: 0.3250\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.6085 Acc: 0.7250\n",
            "val Loss: 1265.4181 Acc: 0.3750\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1260.3529 Acc: 0.7750\n",
            "val Loss: 1257.5963 Acc: 0.4000\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.400000\n",
            "new outer  7\n",
            "initial tensor([ 0.0020, -0.0102,  0.0196, -0.0007, -0.0044], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1305.0764 Acc: 0.1063\n",
            "204579136.0 102298760.0\n",
            "val Loss: 1293.5269 Acc: 0.0250\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1287.5414 Acc: 0.2875\n",
            "val Loss: 1282.1198 Acc: 0.2000\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1277.1085 Acc: 0.5813\n",
            "val Loss: 1273.7671 Acc: 0.2750\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.6418 Acc: 0.7000\n",
            "val Loss: 1265.9144 Acc: 0.2250\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1260.3501 Acc: 0.7250\n",
            "val Loss: 1257.9773 Acc: 0.2500\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.275000\n",
            "new outer  8\n",
            "initial tensor([ 0.0026, -0.0100,  0.0198, -0.0005, -0.0040], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1305.1034 Acc: 0.0875\n",
            "204577600.0 102297992.0\n",
            "val Loss: 1292.0618 Acc: 0.2500\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.8106 Acc: 0.3625\n",
            "val Loss: 1281.7072 Acc: 0.3250\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1276.7779 Acc: 0.5938\n",
            "val Loss: 1273.5487 Acc: 0.2750\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.1664 Acc: 0.6875\n",
            "val Loss: 1265.1713 Acc: 0.3750\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1259.7909 Acc: 0.8250\n",
            "val Loss: 1257.3497 Acc: 0.3500\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.375000\n",
            "new outer  9\n",
            "initial tensor([ 0.0030, -0.0096,  0.0203, -0.0002, -0.0033], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1305.0613 Acc: 0.1562\n",
            "204583456.0 102300920.0\n",
            "val Loss: 1292.2606 Acc: 0.4000\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1287.4441 Acc: 0.3688\n",
            "val Loss: 1282.0599 Acc: 0.3250\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1277.3929 Acc: 0.6313\n",
            "val Loss: 1274.3638 Acc: 0.2500\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.6497 Acc: 0.6813\n",
            "val Loss: 1265.6367 Acc: 0.3750\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1260.1995 Acc: 0.7812\n",
            "val Loss: 1256.9328 Acc: 0.4500\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.450000\n",
            "new outer  10\n",
            "initial tensor([ 0.0037, -0.0091,  0.0207,  0.0004, -0.0032], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1305.2172 Acc: 0.1063\n",
            "204580528.0 102299456.0\n",
            "val Loss: 1292.4921 Acc: 0.1500\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.8124 Acc: 0.3625\n",
            "val Loss: 1281.9719 Acc: 0.3250\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1276.9872 Acc: 0.5563\n",
            "val Loss: 1273.8270 Acc: 0.2750\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.4429 Acc: 0.6875\n",
            "val Loss: 1265.6353 Acc: 0.3500\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1259.9855 Acc: 0.7938\n",
            "val Loss: 1257.8634 Acc: 0.2500\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.350000\n",
            "new outer  11\n",
            "initial tensor([ 0.0041, -0.0085,  0.0213,  0.0006, -0.0027], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1304.6405 Acc: 0.1562\n",
            "204580432.0 102299408.0\n",
            "val Loss: 1293.3726 Acc: 0.1000\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1287.0773 Acc: 0.3250\n",
            "val Loss: 1283.0668 Acc: 0.0750\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1277.0781 Acc: 0.6250\n",
            "val Loss: 1274.8743 Acc: 0.1500\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.4104 Acc: 0.7438\n",
            "val Loss: 1266.0037 Acc: 0.2000\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1259.9379 Acc: 0.7812\n",
            "val Loss: 1258.8515 Acc: 0.2000\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.200000\n",
            "new outer  12\n",
            "initial tensor([ 0.0046, -0.0083,  0.0219,  0.0012, -0.0021], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1305.0615 Acc: 0.1000\n",
            "204579520.0 102298952.0\n",
            "val Loss: 1292.7394 Acc: 0.2000\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1287.0294 Acc: 0.3250\n",
            "val Loss: 1282.2070 Acc: 0.2250\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1277.1431 Acc: 0.5875\n",
            "val Loss: 1274.3516 Acc: 0.2250\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.5839 Acc: 0.7438\n",
            "val Loss: 1266.0022 Acc: 0.2750\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1260.2348 Acc: 0.7750\n",
            "val Loss: 1258.2554 Acc: 0.2500\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.275000\n",
            "new outer  13\n",
            "initial tensor([ 0.0053, -0.0079,  0.0224,  0.0019, -0.0017], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1304.3633 Acc: 0.1375\n",
            "204576272.0 102297328.0\n",
            "val Loss: 1292.1526 Acc: 0.2000\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.6860 Acc: 0.3625\n",
            "val Loss: 1281.6899 Acc: 0.2250\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1276.5722 Acc: 0.5875\n",
            "val Loss: 1273.2997 Acc: 0.3000\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.0613 Acc: 0.6500\n",
            "val Loss: 1265.4008 Acc: 0.2500\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1259.6585 Acc: 0.7313\n",
            "val Loss: 1257.3425 Acc: 0.2750\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.300000\n",
            "new outer  14\n",
            "initial tensor([ 0.0059, -0.0075,  0.0230,  0.0023, -0.0014], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1305.0909 Acc: 0.1188\n",
            "204579824.0 102299104.0\n",
            "val Loss: 1292.1674 Acc: 0.2000\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.6190 Acc: 0.2875\n",
            "val Loss: 1281.4218 Acc: 0.2500\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1276.5074 Acc: 0.5563\n",
            "val Loss: 1272.5782 Acc: 0.3250\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1267.9131 Acc: 0.6438\n",
            "val Loss: 1265.2833 Acc: 0.2750\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1259.5981 Acc: 0.7438\n",
            "val Loss: 1256.5082 Acc: 0.4250\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.425000\n",
            "new outer  15\n",
            "initial tensor([ 0.0062, -0.0070,  0.0233,  0.0030, -0.0009], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1304.6925 Acc: 0.0813\n",
            "204575088.0 102296736.0\n",
            "val Loss: 1292.5690 Acc: 0.1500\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.6296 Acc: 0.2938\n",
            "val Loss: 1281.9409 Acc: 0.1250\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1276.7578 Acc: 0.5938\n",
            "val Loss: 1273.5555 Acc: 0.2250\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.1672 Acc: 0.6875\n",
            "val Loss: 1265.5756 Acc: 0.1250\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1259.7915 Acc: 0.7438\n",
            "val Loss: 1257.2787 Acc: 0.3500\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.350000\n",
            "new outer  16\n",
            "initial tensor([ 0.0069, -0.0068,  0.0239,  0.0038, -0.0006], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1305.1959 Acc: 0.1562\n",
            "204575040.0 102296712.0\n",
            "val Loss: 1292.3532 Acc: 0.1750\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.9510 Acc: 0.2875\n",
            "val Loss: 1282.1354 Acc: 0.1750\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1277.2733 Acc: 0.5312\n",
            "val Loss: 1273.2646 Acc: 0.4000\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.2860 Acc: 0.7500\n",
            "val Loss: 1265.4134 Acc: 0.3250\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1260.0433 Acc: 0.7500\n",
            "val Loss: 1257.0847 Acc: 0.3750\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.400000\n",
            "new outer  17\n",
            "initial tensor([ 0.0074, -0.0065,  0.0245,  0.0043, -0.0002], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1305.0719 Acc: 0.1125\n",
            "204579776.0 102299080.0\n",
            "val Loss: 1292.7330 Acc: 0.1500\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.8136 Acc: 0.2063\n",
            "val Loss: 1281.9395 Acc: 0.2250\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1276.8666 Acc: 0.5312\n",
            "val Loss: 1273.2768 Acc: 0.2750\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.0083 Acc: 0.7250\n",
            "val Loss: 1265.0568 Acc: 0.3000\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1259.6698 Acc: 0.7750\n",
            "val Loss: 1257.5197 Acc: 0.2250\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.300000\n",
            "new outer  18\n",
            "initial tensor([ 0.0079, -0.0064,  0.0251,  0.0045,  0.0004], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1305.3798 Acc: 0.1250\n",
            "204583712.0 102301048.0\n",
            "val Loss: 1293.1498 Acc: 0.0500\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.7805 Acc: 0.3563\n",
            "val Loss: 1282.0964 Acc: 0.1250\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1277.0822 Acc: 0.5375\n",
            "val Loss: 1273.6008 Acc: 0.2250\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.4420 Acc: 0.6313\n",
            "val Loss: 1265.2574 Acc: 0.3250\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1259.7812 Acc: 0.7750\n",
            "val Loss: 1257.3931 Acc: 0.2250\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.325000\n",
            "new outer  19\n",
            "initial tensor([ 0.0082, -0.0058,  0.0255,  0.0046,  0.0007], device='cuda:0')\n",
            "32\n",
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 1304.7580 Acc: 0.1438\n",
            "204574880.0 102296632.0\n",
            "val Loss: 1292.9636 Acc: 0.3000\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 1286.3460 Acc: 0.3688\n",
            "val Loss: 1281.1103 Acc: 0.3500\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 1276.5761 Acc: 0.5625\n",
            "val Loss: 1273.8386 Acc: 0.1750\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 1268.2052 Acc: 0.6688\n",
            "val Loss: 1264.8836 Acc: 0.3750\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 1259.7050 Acc: 0.7500\n",
            "val Loss: 1257.7185 Acc: 0.2000\n",
            "\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.375000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A0WkKWn3-X8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# run"
      ]
    },
    {
      "metadata": {
        "id": "gcCzS1xv6nZY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_normal_training_with_pruning(this_trainset):\n",
        "  _,mytrainset = torch.utils.data.random_split(this_trainset,(49200,800))\n",
        "\n",
        "  mytrain_data, myval_data = torch.utils.data.random_split(mytrainset,(int(0.8*len(mytrainset)),int(0.2*len(mytrainset))))\n",
        "  print(mytrain_data.__len__(),myval_data.__len__() )\n",
        "\n",
        "  mytrainloader = torch.utils.data.DataLoader(mytrain_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  myvalloader = torch.utils.data.DataLoader(myval_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  mydataloaders = {'train': mytrainloader, 'val': myvalloader}\n",
        "  image_datasets= {'train': mytrain_data,'val': myval_data}\n",
        "  dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "  model_ft = models.vgg16(pretrained=True)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  freeze_layers(model_ft.features, exclude=[])\n",
        "  mask_network(model_ft.classifier,[0],threshold=0.0001)\n",
        "  set_threshold(model_ft)\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()  \n",
        "     \n",
        "  # Observe that all parameters are being optimized\n",
        "  optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  # Decay LR by a factor of 0.1 every 7 epochs\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "  \n",
        "  \n",
        "  model_ft = train_model_prune(model_ft, mydataloaders,dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=2)\n",
        "  \n",
        "# run_normal_training_with_pruning(trainset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GShMvVfL0-mQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_meta_prune(model,trainset, outer_steps, num_samples=800, device='cuda'):\n",
        "  mask_dict = {'0':torch.ones(model.classifier[0].weight.size()).to(device)}\n",
        "  shuffled_train = torch.utils.data.RandomSampler(trainset)\n",
        "  train_sample_list = list(torch.utils.data.BatchSampler(shuffled_train,num_samples,False))\n",
        "  shuffled_train = [x for x in shuffled_train]\n",
        "  for i in range(outer_steps):\n",
        "#     train_sample = [trainset[j] for j in train_sample_list[i]] \n",
        "    \n",
        "#     print(len(train_sample))\n",
        "    _,train_sample = torch.utils.data.random_split(trainset,(49200,800))\n",
        "    train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "    valloader = torch.utils.data.DataLoader(val_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "    \n",
        "    subdataloaders = {'train': trainloader, 'val': valloader}\n",
        "    image_datasets= {'train': train_data,'val': val_data}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    \n",
        "    model_ft = models.vgg16(pretrained=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "    # freeze_layers(model_ft.features, exclude=['28.weight'])\n",
        "    freeze_layers(model_ft.features)   \n",
        "    mask_network(model_ft.classifier,[0],threshold=0.0001,masks=mask_dict)\n",
        "    model_ft = train_model_prune(model_ft, subdataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=10, prop=0.1)\n",
        "    mask_dict = {'0':model_ft.classifier[0].mask}\n",
        "#     set_threshold(model_ft)\n",
        "\n",
        "#     cost = meta_objective({'train':trainloader, 'val':valoader}, model, optimizer, inner_epochs)\n",
        "\n",
        "\n",
        "# model_ft = models.vgg16(pretrained=True)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model_ft = model_ft.to(device)\n",
        "\n",
        "# train_meta_prune(model_ft,trainset,15)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}