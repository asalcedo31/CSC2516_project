{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of clean_pruning",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asalcedo31/CSC2516_project/blob/master/clean_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zZ1NXrBF79lD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# set up\n"
      ]
    },
    {
      "metadata": {
        "id": "WcnGoj4y0dVu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.modules import Module\n",
        "import torchvision.models.vgg as tv_vgg\n",
        "import time\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fmjGlo3V0jCd",
        "colab_type": "code",
        "outputId": "ebf8ac38-62de-4ecc-9a89-3797070266c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=5,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "# _,trainset = torch.utils.data.random_split(trainset,(49200,800))\n",
        "# _,trainset = torch.utils.data.random_split(trainset,(49995,5))\n",
        "# print(trainset.__len__())\n",
        "\n",
        "# train_data, val_data = torch.utils.data.random_split(trainset,(int(0.8*len(trainset)),int(0.2*len(trainset))))\n",
        "# print(train_data.__len__(),val_data.__len__() )\n",
        "\n",
        "# trainloader = torch.utils.data.DataLoader(train_data, batch_size=5,\n",
        "#                                           shuffle=True, num_workers=0)\n",
        "# valloader = torch.utils.data.DataLoader(val_data, batch_size=5,\n",
        "#                                           shuffle=True, num_workers=0)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "15v78wBX0l32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# image_datasets= {'train': train_data,'val': val_data}\n",
        "# dataloaders = {'train': trainloader, 'val': valloader}\n",
        "\n",
        "# dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "# class_names = image_datasets['train'].classes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u6pkokkh0mx_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def freeze_layers(model_ft, exclude=[]):\n",
        "#   children = list(model_ft.named_children())\n",
        "  for name,param in model_ft.named_parameters():   \n",
        "    if(name not in  exclude):\n",
        "      param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PENRAKM0pLT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def countNonZeroWeights(model):\n",
        "    nonzeros = 0\n",
        "    weights = 0\n",
        "    for name,param in model.named_parameters():\n",
        "        if param is not None:\n",
        "            nonzeros += torch.sum((param != 0).int()).data[0]\n",
        "            weights += torch.sum(param).data[0]\n",
        "    \n",
        "    return nonzeros, weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dyXlRGiw0raM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def set_threshold(model,prop=0.05):\n",
        "  for child in model.named_children():    \n",
        "    for child in child[1].named_children():\n",
        "#       print(child)\n",
        "      if type(child[1]) == MaskedLinear or type(child[1]) == MaskedConv: \n",
        "        child[1].set_threshold(prop=prop)\n",
        "        print(\"layer {}  new threshold {:.4f}\".format(child[0], child[1].threshold))        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ear7nRQY0wwC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model_prune(model, dloaders, dataset_sizes, criterion, optimizer, scheduler,prop=0.05, num_epochs=25, device='cuda',pruning='threshold'):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    print(len(dloaders['train']))\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "                data_idx = 0\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                data_idx = 1\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            i=0\n",
        "            \n",
        "#             print(dloaders[phase].__iter__().next())\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dloaders[phase]:               \n",
        "#                 print(\"batch {} phase {}\".format(i, phase))\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    if pruning == 'L0':\n",
        "                      loss = criterion(outputs, labels,model)\n",
        "                    else:\n",
        "                      loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        model.clamp_parameters()\n",
        "                        exp_flops, exp_l0 = model.get_exp_flops_l0()\n",
        "                i+=1\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                           \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            if epoch % 5 == 0 and phase == 'train': \n",
        "              if pruning == 'threshold':\n",
        "                set_threshold(model,prop=prop)\n",
        "              elif pruning == 'L0':\n",
        "                print(exp_flops.item(), exp_l0.item())\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return epoch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FS6V4Olo00JW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Masked:\n",
        "  def make_mask(self, threshold,mask=None):\n",
        "    if mask is None:\n",
        "      print(\"new mask\",device)\n",
        "      self.mask = torch.ones(self.weight.size(), requires_grad=False).to(device)\n",
        "    else:\n",
        "      self.mask = mask      \n",
        "    self.zeros = torch.zeros(self.weight.size(), requires_grad=False).to(device)\n",
        "    self.threshold = threshold\n",
        "  def set_threshold(self,prop=0.05):\n",
        "    unique_weights = torch.unique(self.weight*self.mask)\n",
        "    mask_size = self.mask.reshape(-1).size()[0]\n",
        "#     mask_size = mask_size[0]*mask_size[1]\n",
        "    mask_nonzero = torch.sum(self.mask.view([mask_size]))\n",
        "    mask_total = mask_size\n",
        "    print('nonzero proportion: {:.4f}'.format(mask_nonzero/mask_total))\n",
        "    self.threshold = torch.max(torch.topk(torch.abs(unique_weights),int(prop*unique_weights.size()[0]),largest=False)[0])    \n",
        "  def make_threshold_mask(self):\n",
        "    self.mask = torch.where(torch.abs(self.weight) >= self.threshold,self.mask,self.zeros).to(device)\n",
        "#     self.mask.requires_grad_(requires_grad=False)\n",
        "  def mask_weight(self):\n",
        "    self.weight = torch.nn.Parameter(self.weight*self.mask).to(device) \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Zw6A3138FNi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# L0 pruning\n"
      ]
    },
    {
      "metadata": {
        "id": "QtGNlj9j04hZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MaskedLinear(torch.nn.Linear,Masked):\n",
        "  def __init__(self, in_features, out_features, bias=True, threshold=0.001,mask=None):\n",
        "    super(MaskedLinear, self).__init__(in_features,out_features)\n",
        "    self.make_mask(threshold,mask)\n",
        "  def forward(self, input):\n",
        "    self.make_threshold_mask()\n",
        "    self.mask_weight()\n",
        "#     print(self.mask[125:135,125:135])\n",
        "#     print(self.weight[125:135,125:135])\n",
        "    return F.linear(input, self.weight, self.bias)\n",
        "\n",
        "class MaskedConv(torch.nn.Conv2d,Masked):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, groups, bias=True,threshold=0.0001):\n",
        "    super(MaskedConv,self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "    self.make_mask(threshold)    \n",
        "  def forward(self, input):\n",
        "    self.mask_weight()\n",
        "    return F.conv2d(input, self.weight, self.bias, self.stride,\n",
        "                    self.padding, self.dilation, self.groups)\n",
        "  \n",
        "limit_a, limit_b, epsilon = -.1, 1.1, 1e-6\n",
        "device='cuda'\n",
        "\n",
        "class LinearL0(Module):\n",
        "  \"\"\"Implementation of L0 regularization for the input units of a fully connected layer\"\"\"\n",
        "  def __init__(self, in_features, out_features, bias=True, weight_decay=1., droprate_init=0.5, temperature=2./3.,\n",
        "                 lamba=1., local_rep=False, qz_loga=None, **kwargs):\n",
        "        \"\"\"\n",
        "        :param in_features: Input dimensionality\n",
        "        :param out_features: Output dimensionality\n",
        "        :param bias: Whether we use a bias\n",
        "        :param weight_decay: Strength of the L2 penalty\n",
        "        :param droprate_init: Dropout rate that the L0 gates will be initialize d to\n",
        "        :param temperature: Temperature of the concrete distribution\n",
        "        :param lamba: Strength of the L0 penalty\n",
        "        :param local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
        "        \"\"\"\n",
        "        super(LinearL0, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.prior_prec = weight_decay\n",
        "        self.weights = torch.nn.Parameter(torch.Tensor(in_features, out_features).to(device))\n",
        "#         self.qz_loga = torch.Tensor(in_features).to(device)\n",
        "        self.qz_loga = torch.nn.Parameter(torch.Tensor(in_features).to(device))\n",
        "        self.temperature = temperature\n",
        "        self.droprate_init = droprate_init if droprate_init != 0. else 0.5\n",
        "        self.lamba = lamba\n",
        "        self.use_bias = False\n",
        "        self.local_rep = local_rep\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.Tensor(out_features).to(device))\n",
        "            self.use_bias = True\n",
        "        self.floatTensor = torch.FloatTensor if not torch.cuda.is_available() else torch.cuda.FloatTensor\n",
        "        self.reset_parameters()\n",
        "        if qz_loga is not None:\n",
        "          self.qz_loga = qz_loga\n",
        "\n",
        "  def reset_parameters(self):\n",
        "      torch.nn.init.kaiming_normal(self.weights, mode='fan_out')\n",
        "      self.qz_loga.data.normal_(math.log(1 - self.droprate_init) - math.log(self.droprate_init), 1e-2)\n",
        "\n",
        "      if self.use_bias:\n",
        "          self.bias.data.fill_(0)\n",
        "\n",
        "  def constrain_parameters(self, **kwargs):\n",
        "      self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
        "\n",
        "  def cdf_qz(self, x):\n",
        "      \"\"\"Implements the CDF of the 'stretched' concrete distribution\"\"\"\n",
        "      xn = (x - limit_a) / (limit_b - limit_a)\n",
        "      logits = math.log(xn) - math.log(1 - xn)\n",
        "      return F.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=epsilon, max=1 - epsilon).to(device)\n",
        "\n",
        "  def quantile_concrete(self, x):\n",
        "      \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "      y = F.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature).to(device)\n",
        "      return y * (limit_b - limit_a) + limit_a\n",
        "\n",
        "  def _reg_w(self):\n",
        "      \"\"\"Expected L0 norm under the stochastic gates, takes into account and re-weights also a potential L2 penalty\"\"\"\n",
        "      logpw_col = torch.sum(- (.5 * self.prior_prec * self.weights.pow(2)) - self.lamba, 1).to(device)\n",
        "      logpw = torch.sum((1 - self.cdf_qz(0)) * logpw_col).to(device)\n",
        "      logpb = 0 if not self.use_bias else - torch.sum(.5 * self.prior_prec * self.bias.pow(2)).to(device)\n",
        "      return logpw + logpb\n",
        "\n",
        "  def regularization(self):\n",
        "      return self._reg_w()\n",
        "\n",
        "  def count_expected_flops_and_l0(self):\n",
        "      \"\"\"Measures the expected floating point operations (FLOPs) and the expected L0 norm\"\"\"\n",
        "      # dim_in multiplications and dim_in - 1 additions for each output neuron for the weights\n",
        "      # + the bias addition for each neuron\n",
        "      # total_flops = (2 * in_features - 1) * out_features + out_features\n",
        "      ppos = torch.sum(1 - self.cdf_qz(0))\n",
        "      expected_flops = (2 * ppos - 1) * self.out_features\n",
        "      expected_l0 = ppos * self.out_features\n",
        "      if self.use_bias:\n",
        "          expected_flops += self.out_features\n",
        "          expected_l0 += self.out_features\n",
        "#       return expected_flops.data[0], expected_l0.data[0]\n",
        "      return expected_flops, expected_l0\n",
        "\n",
        "  def get_eps(self, size):\n",
        "      \"\"\"Uniform random numbers for the concrete distribution\"\"\"\n",
        "      eps = self.floatTensor(size).uniform_(epsilon, 1-epsilon).to(device)\n",
        "      eps = Variable(eps)\n",
        "      return eps\n",
        "\n",
        "  def sample_z(self, batch_size, sample=True):\n",
        "      \"\"\"Sample the hard-concrete gates for training and use a deterministic value for testing\"\"\"\n",
        "      if sample:\n",
        "          eps = self.get_eps(self.floatTensor(batch_size, self.in_features))\n",
        "          z = self.quantile_concrete(eps)\n",
        "          return F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "      else:  # mode\n",
        "          pi = F.sigmoid(self.qz_loga).view(1, self.in_features).expand(batch_size, self.in_features).to(device)\n",
        "          return F.hardtanh(pi * (limit_b - limit_a) + limit_a, min_val=0, max_val=1).to(device)\n",
        "\n",
        "  def sample_weights(self):\n",
        "      z = self.quantile_concrete(self.get_eps(self.floatTensor(self.in_features)))\n",
        "      mask = F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "      return mask.view(self.in_features, 1) * self.weights\n",
        "\n",
        "  def forward(self, input):\n",
        "      if self.local_rep or not self.training:\n",
        "          z = self.sample_z(input.size(0), sample=self.training)\n",
        "          xin = input.mul(z)\n",
        "          output = xin.mm(self.weights)\n",
        "      else:\n",
        "          weights = self.sample_weights()\n",
        "          output = input.mm(weights)\n",
        "      if self.use_bias:\n",
        "          output.add_(self.bias)\n",
        "      return output\n",
        "\n",
        "  def __repr__(self):\n",
        "      s = ('{name}({in_features} -> {out_features}, droprate_init={droprate_init}, '\n",
        "           'lamba={lamba}, temperature={temperature}, weight_decay={prior_prec}, '\n",
        "           'local_rep={local_rep}')\n",
        "      if not self.use_bias:\n",
        "          s += ', bias=False'\n",
        "      s += ')'\n",
        "      return s.format(name=self.__class__.__name__, **self.__dict__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJeDMbDw06i2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mask_network(network,layers_to_mask, threshold=0.002, linear_masking=None,random_init=False, bias=True,masks=None):\n",
        "  \"\"\"\"\n",
        "  replaces linear layers with masked linear layers\n",
        "  network is the initial sequential container\n",
        "  layers is a list of layers to mask\n",
        "  random init is a logical indicating whether to preserve the initial weights or to modify them\n",
        "  \"\"\"\n",
        "  network.masked_layers=[]\n",
        "  for name,layer in network.named_children():   \n",
        "    if int(name) in layers_to_mask:\n",
        "      layer_mask = None\n",
        "      if masks is not None:\n",
        "        if name in masks:\n",
        "          layer_mask = masks.get(name)      \n",
        "      if type(layer)== torch.nn.Linear and linear_masking is None:\n",
        "        masked_layer = MaskedLinear(layer.in_features, layer.out_features, bias=bias,threshold=threshold,masks=layer_mask)\n",
        "      elif type(layer)== torch.nn.Linear and linear_masking =='L0':\n",
        "        masked_layer = LinearL0(layer.in_features, layer.out_features, bias=bias, lamba=0.1/640,qz_loga=layer_mask)\n",
        "#         masked_layer = LinearL0(layer.in_features, layer.out_features, bias=bias)\n",
        "        network.masked_layers.append(masked_layer)\n",
        "      elif type(layer)== torch.nn.Conv2d:\n",
        "        masked_layer = MaskedConv(layer.in_channels, layer.out_channels, layer.kernel_size, layer.stride, layer.padding, layer.dilation,layer.groups, bias=bias, threshold=threshold)\n",
        "      if random_init != True and linear_masking != 'L0':\n",
        "        masked_layer.weight = copy.deepcopy(layer.weight)\n",
        "        masked_layer.bias = copy.deepcopy(layer.bias)\n",
        "      elif random_init != True and linear_masking == 'L0':\n",
        "        weight_copy = torch.transpose(copy.deepcopy(layer.weight),0,1)\n",
        "        masked_layer.weights = torch.nn.Parameter(weight_copy)\n",
        "        \n",
        "      network[int(name)] = masked_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VdH3hF8PMqRo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VGG_L0(tv_vgg.VGG):\n",
        "  def regularization(self):\n",
        "    regularization = 0.\n",
        "    for layer in self.layers:\n",
        "        regularization += - (1. / self.N) * layer.regularization()\n",
        "    if torch.cuda.is_available():\n",
        "        regularization = regularization.cuda()\n",
        "    return regularization\n",
        "  \n",
        "  def regularize(self, N):\n",
        "    regularization = 0.\n",
        "    for layer in self.masked_layers:\n",
        "          regularization += - (1. / N) * layer.regularization()          \n",
        "    if torch.cuda.is_available():\n",
        "        regularization = regularization.cuda()\n",
        "    return regularization\n",
        "    \n",
        "  def clamp_parameters(self):\n",
        "    for layer in self.masked_layers:\n",
        "      layer.constrain_parameters()\n",
        "  \n",
        "  def get_exp_flops_l0(self):\n",
        "    total_flops = 0\n",
        "    total_l0 = 0\n",
        "    for layer in self.masked_layers:\n",
        "      exp_flops, exp_l0 = layer.count_expected_flops_and_l0()\n",
        "      total_flops += exp_flops\n",
        "      total_l0 += exp_l0\n",
        "    return total_flops, total_l0  \n",
        "          \n",
        "\n",
        "def vgg16_L0(pretrained=False, **kwargs):\n",
        "  \"\"\"VGG 16-layer model (configuration \"D\")\n",
        "  Args:\n",
        "      pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "  \"\"\"\n",
        "  if pretrained:\n",
        "      kwargs['init_weights'] = False\n",
        "  model = VGG_L0(tv_vgg.make_layers(tv_vgg.cfg['D']), **kwargs)\n",
        "  if pretrained:\n",
        "      model.load_state_dict(model_zoo.load_url(tv_vgg.model_urls['vgg16']))\n",
        "  return model\n",
        "\n",
        "\n",
        "def run_normal_training_with_L0_pruning(this_trainset):\n",
        "  print(this_trainset.__len__())  \n",
        "  _,mytrainset = torch.utils.data.random_split(this_trainset,(49200,800))\n",
        "  # _,trainset = torch.utils.data.random_split(trainset,(49995,5))\n",
        "  print(mytrainset.__len__())\n",
        "\n",
        "  mytrain_data, myval_data = torch.utils.data.random_split(mytrainset,(int(0.8*len(mytrainset)),int(0.2*len(mytrainset))))\n",
        "  print(mytrain_data.__len__(),myval_data.__len__() )\n",
        "\n",
        "  mytrainloader = torch.utils.data.DataLoader(mytrain_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  myvalloader = torch.utils.data.DataLoader(myval_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  mydataloaders = {'train': mytrainloader, 'val': myvalloader}\n",
        "  image_datasets= {'train': mytrain_data,'val': myval_data}\n",
        "  dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}  \n",
        "\n",
        "  model_ft = vgg16_L0(pretrained=True)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  freeze_layers(model_ft.features, exclude=[])\n",
        "  mask_network(model_ft.classifier,[0,3,6],linear_masking=\"L0\", random_init=False)\n",
        "  model_ft.masked_layers = model_ft.classifier.masked_layers\n",
        "  \n",
        "  print(model_ft.classifier[0].weights.size())  \n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  def loss_function(outputs,targets, model):\n",
        "    loss = criterion(outputs,targets)\n",
        "    loss += model.regularize(640)\n",
        "    return loss\n",
        "\n",
        "\n",
        "  # Observe that all parameters are being optimized\n",
        "  optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "#   [print(p) for p in model_ft.parameters()]\n",
        "#   return\n",
        "\n",
        "  # Decay LR by a factor of 0.1 every 7 epochs\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "  model_ft = train_model_prune(model_ft, mydataloaders,dataset_sizes, loss_function, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=20, pruning=\"L0\")\n",
        "  \n",
        "# run_normal_training_with_L0_pruning(trainset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-BTMME3oonFj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class L0_Meta_Objective():\n",
        "  def __init__(self,train_data,val_data):\n",
        "    self.trainloader = torch.utils.data.DataLoader(train_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "    \n",
        "    self.valloader = torch.utils.data.DataLoader(val_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "    \n",
        "    self.dataloaders = {'train':self.trainloader, 'val': self.valloader}\n",
        "    self.image_datasets= {'train': train_data,'val': val_data}\n",
        "    self.dataset_sizes = {x: len(self.image_datasets[x]) for x in ['train', 'val']}\n",
        "    self.model_ft = vgg16_L0(pretrained=True)\n",
        "    self.model_ft.get_params()\n",
        "#     print('params',self.model_ft.vars)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.model_ft = self.model_ft.to(device)    \n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.optimizer_ft = optim.Adam(self.model_ft.parameters(), lr=0.001)    \n",
        "    freeze_layers(self.model_ft.features, exclude=[])    \n",
        "    \n",
        "  def add_L0_layers(self,layers_to_prune,initial_masks=None):    \n",
        "    mask_network(self.model_ft.classifier,layers_to_prune,linear_masking=\"L0\",masks=initial_masks)\n",
        "    self.model_ft.masked_layers = self.model_ft.classifier.masked_layers  \n",
        "  \n",
        "  def inner_loss_function(self,outputs,targets,model):\n",
        "      loss = self.criterion(outputs,targets)\n",
        "      loss += self.model_ft.regularize(640)\n",
        "      return loss\n",
        "        \n",
        "  def inner_train_loop(self, inner_epochs):    \n",
        "    # Observe that all parameters are being optimized\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(self.optimizer_ft, step_size=7, gamma=0.1)\n",
        "    final_mod = train_model_prune(self.model_ft, self.dataloaders,self.dataset_sizes, self.inner_loss_function, self.optimizer_ft, exp_lr_scheduler,\n",
        "                                    num_epochs=inner_epochs, pruning=\"L0\")\n",
        "    return final_mod\n",
        "    \n",
        "def get_initial_masks(model,layers_to_mask,droprate_init=0.5):\n",
        "    initial_masks = {}    \n",
        "    for name,layer in model.named_children():\n",
        "      if int(name) in layers_to_mask:\n",
        "        qz_loga = torch.nn.Parameter(torch.Tensor(layer.in_features).to(device))\n",
        "        qz_loga.data.normal_(math.log(1 - droprate_init) - math.log(droprate_init), 1e-2)\n",
        "        initial_masks[name] = qz_loga                             \n",
        "    return initial_masks\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YaBU663jnTiW",
        "colab_type": "code",
        "outputId": "e2e8d79f-cdd8-480b-83d2-9ef4b0972e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2784
        }
      },
      "cell_type": "code",
      "source": [
        "def gd_step(cost, params, lrate):\n",
        "    \"\"\"Perform one gradient descent step on the given cost function with learning\n",
        "    rate lrate. Returns a new set of parameters, and (IMPORTANT) does not modify\n",
        "    the input parameters.\"\"\"\n",
        "\n",
        "    cost_grad_fun = torch.autograd.grad(cost)\n",
        "    grads = cost_grad_fun(params)\n",
        "    opt_params = {}\n",
        "    for p in params.keys():\n",
        "        opt_params[p] = params[p] - grads[p]*lrate\n",
        "    return opt_params\n",
        "\n",
        "def set_params(model,inparams):\n",
        "  for name,layer in model.named_children():\n",
        "    for name_p, p in layer.named_parameters():\n",
        "      key = str(name)+'.'+name_p\n",
        "      if key in inparams.keys():\n",
        "#         print(layer._parameters)\n",
        "        layer._parameters[name_p] = inparams[key]\n",
        "#         print(key)\n",
        "#         print(inparams[key])\n",
        "  return\n",
        " \n",
        "\n",
        "#MAML attempt\n",
        "def train_meta_prune_L0(trainset, valset, layers_to_prune,outer_steps=5, inner_steps=5, num_samples=800, inner_lr=0.001,outer_lr=0.001, device='cuda'):  \n",
        "  shuffled_train = torch.utils.data.RandomSampler(trainset)\n",
        "  train_sample_list = list(torch.utils.data.BatchSampler(shuffled_train,num_samples,False))\n",
        "  shuffled_train = [x for x in shuffled_train]\n",
        "  #sample model for dimensions\n",
        "  model_ft = vgg16_L0(pretrained=True)\n",
        "  initial_masks = get_initial_masks(model_ft.classifier, [0])\n",
        "#   outer_optimizer = optim.RMSprop(initial_masks.values(), lr=0.001, momentum=0.9)\n",
        "#   print(initial_masks)\n",
        "  \n",
        "  for i in range(outer_steps):\n",
        "#     for phase in ['train','val']\n",
        "    train_sample = [trainset[j] for j in train_sample_list[i]] \n",
        "    train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "    \n",
        "#     outer_optimizer.zero_grad()\n",
        "    #meta obejctive object contains model and loaders\n",
        "    MO = L0_Meta_Objective(train_data,val_data)\n",
        "        \n",
        "    #make separate inital masks for each layer    \n",
        "    MO.add_L0_layers(layers_to_prune,initial_masks)\n",
        "    print(\"new outer\")\n",
        "    losses_ta\n",
        "    \n",
        "    for i in range(5):\n",
        "      print(\"i \",i)\n",
        "      inputs,labels = MO.trainloader.__iter__().next()\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      preds = MO.model_ft(inputs)\n",
        "      \n",
        "      loss =  F.cross_entropy(preds,labels)+MO.model_ft.regularize(640)\n",
        "      vars = dict(MO.model_ft.classifier.named_parameters())\n",
        "      grad = torch.autograd.grad(loss,vars.values())\n",
        "#       grad = torch.autograd.grad(loss,[MO.model_ft.classifier[0].weights,MO.model_ft.classifier[3].weight, MO.model_ft.classifier[6].weight, MO.model_ft.classifier[0].qz_loga] )\n",
        "      params = {}\n",
        "      \n",
        "      for p in zip(grad,vars.values(),vars.keys()):\n",
        "#         print(p[1]-p[0])\n",
        "        params[p[2]] = p[1] - inner_lr* p[0]\n",
        "      \n",
        "#       print(params['3.weight'])  \n",
        "#       params = dict(p[0]= p[1] - inner_lr* p[0], zip(grad,vars.values(),vars.keys()))\n",
        "      set_params( MO.model_ft.classifier,params)\n",
        "#       loss += self.model_ft.regularize(640)\n",
        "      print('final inner loss {:.4f}'.format(loss))\n",
        "    \n",
        "    inputs,labels = MO.dataloaders['train'].__iter__().next()\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    final_preds = MO.model_ft(inputs)\n",
        "    final_loss =  F.cross_entropy(final_preds,labels)+MO.model_ft.regularize(640)\n",
        "    print('final outer loss {:.4f}'.format(final_loss))\n",
        "    outer_grad = torch.autograd.grad(final_loss,initial_masks.values())\n",
        "    for p in zip(outer_grad,initial_masks.values(),initial_masks.keys()):\n",
        "#           print(p[1]-p[0])\n",
        "        initial_masks[p[2]] = torch.nn.Parameter(p[1] - outer_lr* p[0])\n",
        "\n",
        "# train_meta_prune_L0(trainset,testset,[0], outer_steps=10,inner_lr=0.1, outer_lr=0.01)\n",
        "\n",
        "\n",
        "def meta_prune_reptile(trainset, valset, layers_to_prune,outer_steps=10, inner_steps=5, num_samples=200, inner_lr=0.001,outer_lr=0.001, device='cuda'):  \n",
        "  shuffled_train = torch.utils.data.RandomSampler(trainset)\n",
        "  train_sample_list = list(torch.utils.data.BatchSampler(shuffled_train,num_samples,False))\n",
        "  \n",
        "  train_sample = train_sample_list[0]\n",
        "  train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "  \n",
        "  MO = L0_Meta_Objective(train_data,val_data)\n",
        "  \n",
        "  #make separate inital masks for each layer    \n",
        "  MO.add_L0_layers(layers_to_prune)\n",
        "  initial_p = OrderedDict(MO.model_ft.classifier.named_parameters())\n",
        "  \n",
        "  for i in range(1,outer_steps):\n",
        "    print(\"new outer \", i)\n",
        "    train_sample = [trainset[j] for j in train_sample_list[i]] \n",
        "    train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "    MO = L0_Meta_Objective(train_data,val_data)\n",
        "    MO.add_L0_layers(layers_to_prune)\n",
        "    \n",
        "#     print(dataloaders['train'].__iter__().next())\n",
        "#     set_params(MO.model_ft.classifier, initial_p)\n",
        "    new_mod = MO.inner_train_loop(3)\n",
        "    new_p = OrderedDict(MO.model_ft.classifier.named_parameters())\n",
        "    \n",
        "    for name, p in initial_p.items():\n",
        "      initial_p[name] = initial_p[name] + (initial_p[name] - new_p[name])/num_samples*outer_lr\n",
        "    \n",
        "      \n",
        "  \n",
        "  print(initial_p)\n",
        "\n",
        "\n",
        "#   for i in range(outer_steps):\n",
        "meta_prune_reptile(trainset,testset,[0], outer_steps=10,inner_lr=0.1, outer_lr=0.01)    "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new outer  1\n",
            "32\n",
            "Epoch 0/2\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 26.7474 Acc: 0.1750\n",
            "170955264.0 85481728.0\n",
            "val Loss: 25.2864 Acc: 0.0750\n",
            "\n",
            "Epoch 1/2\n",
            "----------\n",
            "train Loss: 24.5266 Acc: 0.4375\n",
            "val Loss: 24.6544 Acc: 0.3500\n",
            "\n",
            "Epoch 2/2\n",
            "----------\n",
            "train Loss: 23.9025 Acc: 0.5250\n",
            "val Loss: 24.9495 Acc: 0.2500\n",
            "\n",
            "Training complete in 0m 13s\n",
            "Best val Acc: 0.350000\n",
            "new outer  2\n",
            "32\n",
            "Epoch 0/2\n",
            "----------\n",
            "train Loss: 26.0159 Acc: 0.2000\n",
            "170959200.0 85483696.0\n",
            "val Loss: 25.2369 Acc: 0.2750\n",
            "\n",
            "Epoch 1/2\n",
            "----------\n",
            "train Loss: 24.5290 Acc: 0.4250\n",
            "val Loss: 24.8651 Acc: 0.3250\n",
            "\n",
            "Epoch 2/2\n",
            "----------\n",
            "train Loss: 24.1736 Acc: 0.4875\n",
            "val Loss: 24.7694 Acc: 0.3250\n",
            "\n",
            "Training complete in 0m 13s\n",
            "Best val Acc: 0.325000\n",
            "new outer  3\n",
            "32\n",
            "Epoch 0/2\n",
            "----------\n",
            "train Loss: 26.1236 Acc: 0.1438\n",
            "170957440.0 85482816.0\n",
            "val Loss: 25.1407 Acc: 0.2500\n",
            "\n",
            "Epoch 1/2\n",
            "----------\n",
            "train Loss: 24.5565 Acc: 0.3688\n",
            "val Loss: 24.6604 Acc: 0.3000\n",
            "\n",
            "Epoch 2/2\n",
            "----------\n",
            "train Loss: 24.2543 Acc: 0.4313\n",
            "val Loss: 24.6017 Acc: 0.2750\n",
            "\n",
            "Training complete in 0m 13s\n",
            "Best val Acc: 0.300000\n",
            "new outer  4\n",
            "32\n",
            "Epoch 0/2\n",
            "----------\n",
            "train Loss: 26.1700 Acc: 0.1875\n",
            "170957440.0 85482816.0\n",
            "val Loss: 24.8627 Acc: 0.2500\n",
            "\n",
            "Epoch 1/2\n",
            "----------\n",
            "train Loss: 24.4348 Acc: 0.4375\n",
            "val Loss: 24.6883 Acc: 0.3500\n",
            "\n",
            "Epoch 2/2\n",
            "----------\n",
            "train Loss: 24.1705 Acc: 0.5062\n",
            "val Loss: 24.4627 Acc: 0.4500\n",
            "\n",
            "Training complete in 0m 13s\n",
            "Best val Acc: 0.450000\n",
            "new outer  5\n",
            "32\n",
            "Epoch 0/2\n",
            "----------\n",
            "train Loss: 26.3709 Acc: 0.1375\n",
            "170959680.0 85483936.0\n",
            "val Loss: 24.8228 Acc: 0.3000\n",
            "\n",
            "Epoch 1/2\n",
            "----------\n",
            "train Loss: 24.5946 Acc: 0.4313\n",
            "val Loss: 24.8222 Acc: 0.3000\n",
            "\n",
            "Epoch 2/2\n",
            "----------\n",
            "train Loss: 24.3065 Acc: 0.4625\n",
            "val Loss: 24.3267 Acc: 0.3500\n",
            "\n",
            "Training complete in 0m 13s\n",
            "Best val Acc: 0.350000\n",
            "new outer  6\n",
            "32\n",
            "Epoch 0/2\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-64d329044113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;31m#   for i in range(outer_steps):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mmeta_prune_reptile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouter_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minner_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouter_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-64d329044113>\u001b[0m in \u001b[0;36mmeta_prune_reptile\u001b[0;34m(trainset, valset, layers_to_prune, outer_steps, inner_steps, num_samples, inner_lr, outer_lr, device)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m#     print(dataloaders['train'].__iter__().next())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m#     set_params(MO.model_ft.classifier, initial_p)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mnew_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mnew_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-455be8bebf36>\u001b[0m in \u001b[0;36minner_train_loop\u001b[0;34m(self, inner_epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     final_mod = train_model_prune(self.model_ft, self.dataloaders,self.dataset_sizes, self.inner_loss_function, self.optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m---> 36\u001b[0;31m                                     num_epochs=inner_epochs, pruning=\"L0\")\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-0908cdae2284>\u001b[0m in \u001b[0;36mtrain_model_prune\u001b[0;34m(model, dloaders, dataset_sizes, criterion, optimizer, scheduler, prop, num_epochs, device, pruning)\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0;31m# backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 14.73 GiB total capacity; 13.70 GiB already allocated; 73.94 MiB free; 222.49 MiB cached)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "A0WkKWn3-X8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# run"
      ]
    },
    {
      "metadata": {
        "id": "gcCzS1xv6nZY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_normal_training_with_pruning(this_trainset):\n",
        "  _,mytrainset = torch.utils.data.random_split(this_trainset,(49200,800))\n",
        "\n",
        "  mytrain_data, myval_data = torch.utils.data.random_split(mytrainset,(int(0.8*len(mytrainset)),int(0.2*len(mytrainset))))\n",
        "  print(mytrain_data.__len__(),myval_data.__len__() )\n",
        "\n",
        "  mytrainloader = torch.utils.data.DataLoader(mytrain_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  myvalloader = torch.utils.data.DataLoader(myval_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "  mydataloaders = {'train': mytrainloader, 'val': myvalloader}\n",
        "  image_datasets= {'train': mytrain_data,'val': myval_data}\n",
        "  dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "  model_ft = models.vgg16(pretrained=True)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  freeze_layers(model_ft.features, exclude=[])\n",
        "  mask_network(model_ft.classifier,[0],threshold=0.0001)\n",
        "  set_threshold(model_ft)\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()  \n",
        "     \n",
        "  # Observe that all parameters are being optimized\n",
        "  optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  # Decay LR by a factor of 0.1 every 7 epochs\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "  \n",
        "  \n",
        "  model_ft = train_model_prune(model_ft, mydataloaders,dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=2)\n",
        "  \n",
        "# run_normal_training_with_pruning(trainset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GShMvVfL0-mQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_meta_prune(model,trainset, outer_steps, num_samples=800, device='cuda'):\n",
        "  mask_dict = {'0':torch.ones(model.classifier[0].weight.size()).to(device)}\n",
        "  shuffled_train = torch.utils.data.RandomSampler(trainset)\n",
        "  train_sample_list = list(torch.utils.data.BatchSampler(shuffled_train,num_samples,False))\n",
        "  shuffled_train = [x for x in shuffled_train]\n",
        "  for i in range(outer_steps):\n",
        "#     train_sample = [trainset[j] for j in train_sample_list[i]] \n",
        "    \n",
        "#     print(len(train_sample))\n",
        "    _,train_sample = torch.utils.data.random_split(trainset,(49200,800))\n",
        "    train_data, val_data = torch.utils.data.random_split(train_sample,(int(0.8*num_samples),int(0.2*num_samples)))\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "    valloader = torch.utils.data.DataLoader(val_data, batch_size=5,\n",
        "                                            shuffle=True, num_workers=0)\n",
        "    \n",
        "    subdataloaders = {'train': trainloader, 'val': valloader}\n",
        "    image_datasets= {'train': train_data,'val': val_data}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    \n",
        "    model_ft = models.vgg16(pretrained=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "    # freeze_layers(model_ft.features, exclude=['28.weight'])\n",
        "    freeze_layers(model_ft.features)   \n",
        "    mask_network(model_ft.classifier,[0],threshold=0.0001,masks=mask_dict)\n",
        "    model_ft = train_model_prune(model_ft, subdataloaders, dataset_sizes, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=10, prop=0.1)\n",
        "    mask_dict = {'0':model_ft.classifier[0].mask}\n",
        "#     set_threshold(model_ft)\n",
        "\n",
        "#     cost = meta_objective({'train':trainloader, 'val':valoader}, model, optimizer, inner_epochs)\n",
        "\n",
        "\n",
        "# model_ft = models.vgg16(pretrained=True)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model_ft = model_ft.to(device)\n",
        "\n",
        "# train_meta_prune(model_ft,trainset,15)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}