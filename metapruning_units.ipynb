{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metapruning_units.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "tYqrMVdpA1NX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision as tv\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.modules import Module\n",
        "import torchvision.models.vgg as tv_vgg\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.modules.utils import _pair as pair\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BhzbK8ntAmnN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Computation Routines"
      ]
    },
    {
      "metadata": {
        "id": "NuhlcxSIA94G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ]
    },
    {
      "metadata": {
        "id": "YC8SpzkJA8gK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DatasetManager:\n",
        "    \n",
        "    def __init__(self, dataset='cifar10', percent_data=10.0, percent_val=20.0, data_path='./data'):\n",
        "        \n",
        "        # 'dataset' can be 'cifar10', 'cifar100', 'mnist', 'fashionmnist', 'kmnist', 'emnist', 'stl10', 'svhn'.\n",
        "        # 'percent_data' is the percentage of the full training set to be used.\n",
        "        # 'percent_val' is the percentage of the *loaded* training set to be used as validation data.\n",
        "        \n",
        "        data_path = './data/{}'.format(dataset)\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.data_path = data_path\n",
        "        self.percent_data = percent_data\n",
        "        self.percent_val = percent_val\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "            \n",
        "        elif self.dataset == 'cifar10' or\\\n",
        "             self.dataset == 'cifar100' or\\\n",
        "             self.dataset == 'stl10' or\\\n",
        "             self.dataset == 'svhn':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "        \n",
        "        elif self.dataset == 'mnist' or\\\n",
        "             self.dataset == 'fashionmnist' or\\\n",
        "             self.dataset == 'kmnist' or\\\n",
        "             self.dataset == 'emnist':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "        return\n",
        "    \n",
        "    \n",
        "    def ImportDataset(self, batch_size=5):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "        \n",
        "            self.trainset = tv.datasets.ImageFolder(root=self.data_path,\n",
        "                             transform=self.transform)\n",
        "        \n",
        "        # todo\n",
        "        \n",
        "        elif self.dataset == 'cifar10':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR10(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR10(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "        \n",
        "        elif self.dataset == 'cifar100':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR100(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR100(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "             \n",
        "        elif self.dataset == 'mnist':\n",
        "\n",
        "            self.trainset = tv.datasets.MNIST(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.MNIST(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'fashionmnist':\n",
        "\n",
        "            self.trainset = tv.datasets.FashionMNIST(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.FashionMNIST(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'kmnist':\n",
        "\n",
        "            self.trainset = tv.datasets.KMNIST(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.KMNIST(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'emnist':\n",
        "\n",
        "            self.trainset = tv.datasets.EMNIST(root=self.data_path, split='balanced', train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.EMNIST(root=self.data_path, split='balanced', train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'stl10':\n",
        "\n",
        "            self.trainset = tv.datasets.STL10(root=self.data_path, split='train',\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.STL10(root=self.data_path, split='test',\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'svhn':\n",
        "\n",
        "            self.trainset = tv.datasets.SVHN(root=self.data_path, split='train',\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.SVHN(root=self.data_path, split='test',\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        self.SplitData();\n",
        "        self.GenerateLoaders();\n",
        "                \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def SplitData(self):\n",
        "        \n",
        "        len_full = self.trainset.__len__()\n",
        "        len_train = int(np.round(len_full*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.trainset = torch.utils.data.random_split(self.trainset, (len_full-len_train, len_train))\n",
        "        \n",
        "        len_val = int(np.round(len_train*self.percent_val/100.0))\n",
        "        len_train = len_train - len_val\n",
        "        \n",
        "        self.valset, self.trainset = torch.utils.data.random_split(self.trainset, (len_val, len_train))\n",
        "         \n",
        "        len_full_test = self.testset.__len__()\n",
        "        len_test = int(np.round(len_full_test*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.testset = torch.utils.data.random_split(self.testset, (len_full_test-len_test, len_test))\n",
        "\n",
        "        print('\\nFull training set size: {}'.format(len_full))\n",
        "        print('Full test set size: {}'.format(len_full_test))\n",
        "        print('\\nActive training set size: {}'.format(len_train))\n",
        "        print('Active validation set size: {}'.format(len_val))\n",
        "        print('Active test set size: {}\\n'.format(len_test))\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def GenerateLoaders(self):\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.val_loader = torch.utils.data.DataLoader(self.valset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)          \n",
        "            \n",
        "        return\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUJWD_oDBKHD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training functions"
      ]
    },
    {
      "metadata": {
        "id": "34G6XlaFBMUW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def RecordLosses(phase, epoch_loss, epoch_acc, prune_settings):\n",
        "    \n",
        "    # Record losses for later use, plotting etc\n",
        "    if phase == 'train':\n",
        "        prune_settings.epoch_loss.append(epoch_loss)\n",
        "        prune_settings.epoch_acc.append(epoch_acc)\n",
        "    elif phase == 'val':\n",
        "        prune_settings.val_loss.append(epoch_loss)\n",
        "        prune_settings.val_acc.append(epoch_acc)\n",
        "\n",
        "    return prune_settings\n",
        "\n",
        "\n",
        "def PlotResults(prune_settings, savename=\"fig\"):\n",
        "    \n",
        "    # ====== Plot ======\n",
        "\n",
        "    # ------ Loss ------\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(1, len(prune_settings.epoch_loss)+1), \n",
        "             prune_settings.epoch_loss, \n",
        "             color='red', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Epoch loss')\n",
        "    plt.plot(np.arange(1, len(prune_settings.val_loss)+1), \n",
        "             prune_settings.val_loss, \n",
        "             color='blue', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Validation loss')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    savename_full = \"{}_loss.pdf\".format(savename)\n",
        "    plt.savefig(savename_full)\n",
        "    files.download(savename_full)\n",
        "\n",
        "    # ------ Accuracy ------\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(1, len(prune_settings.epoch_acc)+1), \n",
        "             np.asarray(prune_settings.epoch_acc)*100.0, \n",
        "             color='red', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Epoch accuracy')\n",
        "    plt.plot(np.arange(1, len(prune_settings.val_acc)+1), \n",
        "             np.asarray(prune_settings.val_acc)*100.0, \n",
        "             color='blue', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label = 'Validation accuracy')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    savename_full = \"{}_acc.pdf\".format(savename)\n",
        "    plt.savefig(savename_full)\n",
        "    files.download(savename_full)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def SaveResults(prune_settings, model):\n",
        "    \n",
        "    \n",
        "    \n",
        "    return\n",
        "\n",
        "\n",
        "def train_model(model, dat, criterion, optimizer, scheduler, prune_settings=0, num_epochs=25):\n",
        "    \n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                dataloader = dat.train_loader\n",
        "                dataset_size = dat.trainset.__len__()\n",
        "                \n",
        "                model.train()  # Set model to training mode\n",
        "                \n",
        "            else:\n",
        "                \n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                dataloader = dat.val_loader\n",
        "                dataset_size = dat.valset.__len__()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloader:\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                if prune_settings != 0:\n",
        "                    TrackConv2DNorms(model, prune_settings, inputs)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if training\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                \n",
        "            # Record losses for later use, plotting etc\n",
        "            prune_settings = RecordLosses(phase, epoch_loss, epoch_acc, prune_settings)\n",
        "\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # Record losses for later use, plotting etc\n",
        "    prune_settings.outer_iter_time.append(time_elapsed)\n",
        "    \n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z7C3UubBBXFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Filter Pruning functions"
      ]
    },
    {
      "metadata": {
        "id": "MaSebsFceMRD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning settings"
      ]
    },
    {
      "metadata": {
        "id": "jAZVF5O-BZNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Constants that define possible pruning metrics\n",
        "WEIGHT_NORM = 1\n",
        "ACT_NORM = 2\n",
        "\n",
        "# Class that contains various settings pertaining to how filters are pruned\n",
        "class UnitPruningSettings:\n",
        "    \n",
        "    def __init__(self, idx_layer=0, idx_filter=0, N_prune=1, P_prune=10, p=2, pruning_metric=WEIGHT_NORM):\n",
        "        \n",
        "        # EITHER N_prune OR P_prune will be used to decide how many filters to prune.\n",
        "        # If one is non-positive, the other is used.\n",
        "        # If neither is non-positive, priority is given to P_prune.\n",
        "        # If both are non-positive, no pruning will happen.\n",
        "\n",
        "        self.N_prune = N_prune # Number of filters allowed to be pruned in one pass\n",
        "        self.P_prune = P_prune; # Percent of filters of the current layer to prune\n",
        "        \n",
        "        self.idx_filter = idx_filter # Indices of the N_prune filters\n",
        "        self.idx_layer = idx_layer # Current layer under consideration\n",
        "        self.p = p # p-norm to use when computing which filters to remove\n",
        "        self.pruning_metric = pruning_metric\n",
        "        \n",
        "        self.norms_botk = []\n",
        "        self.idx_norms_botk = []\n",
        "        \n",
        "        # Various statistics will be stored and computed to keep track of how the network changes\n",
        "        \n",
        "        # Number of filters per layer in the original network\n",
        "        self.filters_per_layer_orig = []\n",
        "        \n",
        "        # Number of filters per layer after pruning - this gets updated every time the network is pruned\n",
        "        self.filters_per_layer_after = []\n",
        "        \n",
        "        # Time taken to prune in sec (running total, updated every time pruning happens)\n",
        "        self.prune_time = 0.0\n",
        "        self.outer_iter_time = []\n",
        "        \n",
        "        # Keep track of running epoch loss and validation loss, and corresponding accuracy\n",
        "        self.epoch_loss = []\n",
        "        self.val_loss = []\n",
        "        self.epoch_acc = []\n",
        "        self.val_acc = []\n",
        "        \n",
        "        return\n",
        "    \n",
        "    # Function to print the current pruning state of the model. Verbose can be 0, 1, or 2.\n",
        "    def PrintPruningStatistics(self, verbose=1):\n",
        "    \n",
        "        if verbose == 0:\n",
        "            return\n",
        "        \n",
        "        print(\"Total number of filters before pruning: {}\".format(sum(self.filters_per_layer_orig)))\n",
        "        print(\"Total number of filters after pruning: {}\".format(sum(self.filters_per_layer_after)))\n",
        "    \n",
        "        return\n",
        "    \n",
        "    # Function to set up and initialize based on a given model\n",
        "    def Setup(self, model):\n",
        "        \n",
        "        # Count the number of conv layers\n",
        "        self.N_layers = 0\n",
        "        \n",
        "        for layer, (name, module) in enumerate(model.features._modules.items()):\n",
        "            self.N_layers += 1\n",
        "                    \n",
        "        # Initialize storage containers\n",
        "        self.norms_botk = [None]*self.N_layers\n",
        "        self.idx_norms_botk = [None]*self.N_layers\n",
        "        \n",
        "    \n",
        "    # Function to reset norm containers\n",
        "    def ResetNormContainers(self):\n",
        "    \n",
        "        self.norms_botk = [None]*self.N_layers\n",
        "        self.idx_norms_botk = [None]*self.N_layers\n",
        "    \n",
        "        return\n",
        "\n",
        "    # Function to reset filter containers\n",
        "    def ResetFilterContainers(self):\n",
        "    \n",
        "        self.filters_per_layer_orig = []\n",
        "        self.filters_per_layer_after = []\n",
        "    \n",
        "        return\n",
        "\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qc41Ng14ezag",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning decisions"
      ]
    },
    {
      "metadata": {
        "id": "2F0n6VUTe2ND",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to compute the p-norm of weights in all filters of a given layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DWeightNorms(model, idx_layer, p):\n",
        "    \n",
        "    # Extract the layer of the model currently being considered\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    weights = conv.weight.data\n",
        "\n",
        "    # Compute norms of each filter\n",
        "    norms = weights.norm(p, dim=2).norm(p, dim=2).norm(p, dim=1)\n",
        "#     norms = norms/torch.max(torch.abs(norms))\n",
        "    \n",
        "    return norms\n",
        "\n",
        "\n",
        "# Function to compute the p-norm of activations in all filters per layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DActNorms(activation, prune_settings):\n",
        "    \n",
        "    p = prune_settings.p\n",
        "    \n",
        "    # Compute norms of each activation\n",
        "    norms = torch.norm(activation, p, dim=0).norm(p, dim=1).norm(p, dim=1)\n",
        "#     norms = norms/torch.max(torch.abs(norms))\n",
        "        \n",
        "    return norms\n",
        "\n",
        "\n",
        "# Function to track the p-norm of activations of all filters of during training.\n",
        "def TrackConv2DNorms(model, prune_settings, inputs):\n",
        "    \n",
        "    p = prune_settings.p\n",
        "    P_prune = prune_settings.P_prune\n",
        "\n",
        "    x = Variable(inputs)\n",
        "\n",
        "    ii = -1\n",
        "    for layer, (name, module) in enumerate(model.features._modules.items()):\n",
        "        ii += 1\n",
        "        x = module(x)\n",
        "        \n",
        "        if isinstance(module, torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            if prune_settings.pruning_metric == WEIGHT_NORM:\n",
        "                norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "            elif prune_settings.pruning_metric == ACT_NORM:\n",
        "                norms = ComputeConv2DActNorms(x, prune_settings)\n",
        "\n",
        "            # Use the given prune percentage to figure out how many filters to prune\n",
        "            if (P_prune >= 0):\n",
        "                N_prune = int(len(norms.float())*P_prune/100.0)\n",
        "                prune_settings.N_prune = N_prune\n",
        "\n",
        "#             n_botk, ind_botk = torch.topk(norms, N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            norms = norms.cpu().detach().numpy()\n",
        "    \n",
        "            # Store the norms for each filter\n",
        "#             if prune_settings.norms_botk[ii] is None:\n",
        "#                 prune_settings.norms_botk[ii] = norms\n",
        "#             else:\n",
        "#                 prune_settings.norms_botk[ii] += norms\n",
        "                \n",
        "            # Store normalized norms for each filter\n",
        "            if prune_settings.norms_botk[ii] is None:\n",
        "                prune_settings.norms_botk[ii] = norms/max(norms)\n",
        "            else:\n",
        "                prune_settings.norms_botk[ii] += norms/max(norms)\n",
        "                \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U59vXkzfehDX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning workers"
      ]
    },
    {
      "metadata": {
        "id": "RY5GZWxXej1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The following functions were adapted from https://github.com/jacobgil/pytorch-pruning/blob/master/prune.py\n",
        "\n",
        "def replace_layers(model, i, idx, layers):\n",
        "\tif i in idx:\n",
        "\t\treturn layers[idx.index(i)]\n",
        "\treturn model[i]\n",
        "\n",
        "\n",
        "# Function to prune a given convolution layer in the model provided.\n",
        "# Input \"idx_layers\" is the global index of the convolution layer to be pruned.\n",
        "# Input \"prune_settings\" is a data structure containing information on how pruning is performed.\n",
        "def PruneConvLayers(model, prune_settings):\n",
        "    \n",
        "    # Strategy: in order to prune a particular layer, the output of the previous layer \n",
        "    # and the inputs to the next layer must also be altered accordingly.\n",
        "\t\n",
        "    # Extract pruning settings for convenience\n",
        "    N_prune = prune_settings.N_prune\n",
        "    idx_filter = prune_settings.idx_filter\n",
        "    idx_layer = prune_settings.idx_layer\n",
        "    \n",
        "    if idx_layer >= len(model.features._modules.items()):\n",
        "        return\n",
        "        \n",
        "    # Extract the layer of the model currently being pruned\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    \n",
        "\n",
        "    # In case the list of target filters to delete has out-of-range entries, detect and ignore them\n",
        "    del_filters = []\n",
        "    for kk in range(0, len(idx_filter)):\n",
        "        if idx_filter[kk] >= conv.out_channels:\n",
        "            del_filters.extend(kk)\n",
        "    \n",
        "    if (len(del_filters) > 0):\n",
        "        idx_filter = np.delete(idx_filter, del_filters, 0)\n",
        "        N_prune = len(idx_filter)\n",
        "        prune_settings.N_prune = N_prune\n",
        "        print(\"[WARNING] Encountered an out-of-range target filter; it will be ignored.\")\n",
        "    \n",
        "    # Record pruning statistics\n",
        "    prune_settings.filters_per_layer_orig[idx_layer] = conv.out_channels\n",
        "    prune_settings.filters_per_layer_after[idx_layer] = conv.out_channels - N_prune\n",
        "    \n",
        "        \n",
        "    # To keep track of the succeeding convolution layer\n",
        "    next_conv = None\n",
        "    offset = 1\n",
        "    \n",
        "    # Figure out how many layers after this one are NOT conv layers, in order to skip pruning them\n",
        "    while idx_layer + offset < len(model.features._modules.items()):\n",
        "        \n",
        "        res =  list(model.features._modules.items())[idx_layer + offset]\n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            next_name, next_conv = res\n",
        "            break\n",
        "        offset = offset + 1\n",
        "    \n",
        "    # Create a new, replacement conv layer to remove a given number of filters.\n",
        "    # The rest of its settings should remain the same as the original conv layer.\n",
        "    new_conv = torch.nn.Conv2d(in_channels = conv.in_channels,\n",
        "                               out_channels = conv.out_channels - N_prune,\n",
        "\t\t\t                   kernel_size = conv.kernel_size,\n",
        "                               stride = conv.stride,\n",
        "                               padding = conv.padding,\n",
        "                               dilation = conv.dilation,\n",
        "                               groups = conv.groups,\n",
        "                               bias = True)\n",
        "    \n",
        "    new_conv.bias = conv.bias\n",
        "    \n",
        "    # Copy over the weights to the new conv layer, except the ones corresponding to the filter to be removed\n",
        "    old_weights = conv.weight.data.cpu().numpy()\n",
        "    new_weights = new_conv.weight.data.cpu().numpy()\n",
        "    \n",
        "    # Copy over the set of filters, excluding the ones to be removed\n",
        "    new_weights_temp = np.copy(old_weights)\n",
        "    new_weights_temp = np.delete(new_weights_temp, idx_filter, 0)\n",
        "    new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "    # Update weight data of the new conv layer\n",
        "    new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "    \n",
        "    # Now do the same thing for biases\n",
        "    old_biases = conv.bias.data.cpu().numpy()\n",
        "    new_biases = np.zeros(shape=(old_biases.shape[0] - N_prune), dtype=np.float32)\n",
        "    \n",
        "    new_biases_temp = np.copy(old_biases)\n",
        "    new_biases_temp = np.delete(new_biases_temp, idx_filter, 0)\n",
        "    new_biases[:] = new_biases_temp[:]\n",
        "        \n",
        "    new_conv.bias.data = torch.from_numpy(new_biases).cuda()\n",
        "    \n",
        "    # If there is a succeeding conv layer, adjust its input units and weights accordingly\n",
        "    if next_conv != None:\n",
        "        \n",
        "        next_new_conv = torch.nn.Conv2d(in_channels = next_conv.in_channels - N_prune,\n",
        "                                        out_channels =  next_conv.out_channels,\n",
        "                                        kernel_size = next_conv.kernel_size,\n",
        "                                        stride = next_conv.stride,\n",
        "                                        padding = next_conv.padding,\n",
        "                                        dilation = next_conv.dilation,\n",
        "                                        groups = next_conv.groups,\n",
        "                                        bias = True)\n",
        "        \n",
        "        next_new_conv.bias = next_conv.bias\n",
        "\n",
        "        old_weights = next_conv.weight.data.cpu().numpy()\n",
        "        new_weights = next_new_conv.weight.data.cpu().numpy()\n",
        "        \n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_filter, 1)\n",
        "        new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "        next_new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        # Now do the same thing for biases\n",
        "        next_new_conv.bias.data = next_conv.bias.data\n",
        "\n",
        "        # Update the actual model by replacing the existing filters with the new ones\n",
        "        features = torch.nn.Sequential(\n",
        "                *(replace_layers(model.features, i, [idx_layer, idx_layer + offset], \\\n",
        "                    [new_conv, next_new_conv]) for i, _ in enumerate(model.features)))\n",
        "        del model.features\n",
        "        del conv\n",
        "\n",
        "        model.features = features\n",
        "    \n",
        "    else:\n",
        "\n",
        "        # This is the last conv layer. This affects the first linear layer of the classifier.\n",
        "        model.features = torch.nn.Sequential(*(replace_layers(model.features, i, [idx_layer], [new_conv]) for i, _ in enumerate(model.features)))\n",
        "        idx_layer = 0\n",
        "        old_linear_layer = None\n",
        "\n",
        "        for _, module in model.classifier._modules.items():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                old_linear_layer = module\n",
        "                break\n",
        "            idx_layer = idx_layer + 1\n",
        "\n",
        "        if old_linear_layer == None:\n",
        "            raise BaseException(\"No linear layer found in classifier.\")\n",
        "            \n",
        "        params_per_input_channel = int(old_linear_layer.in_features/conv.out_channels)\n",
        "\n",
        "        new_linear_layer = torch.nn.Linear(old_linear_layer.in_features - N_prune*params_per_input_channel, \n",
        "                                           old_linear_layer.out_features)\n",
        "\n",
        "        old_weights = old_linear_layer.weight.data.cpu().numpy()\n",
        "        new_weights = new_linear_layer.weight.data.cpu().numpy()\t \t\n",
        "\n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        idx_expanded = np.zeros(shape=(N_prune*params_per_input_channel))\n",
        "        \n",
        "        for kk in range(0, len(idx_filter)):\n",
        "            idx_expanded[kk*params_per_input_channel:kk*params_per_input_channel+params_per_input_channel] = np.arange(idx_filter[kk]*params_per_input_channel, idx_filter[kk]*params_per_input_channel + params_per_input_channel)\n",
        "\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_expanded.astype(int), 1)\n",
        "        new_weights[:, :] = new_weights_temp[:, :]\n",
        "        \n",
        "        new_linear_layer.bias.data = old_linear_layer.bias.data\n",
        "        new_linear_layer.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        classifier = torch.nn.Sequential(*(replace_layers(model.classifier, i, [idx_layer], [new_linear_layer]) for i, _ in enumerate(model.classifier)))\n",
        "\n",
        "        del model.classifier\n",
        "        del next_conv\n",
        "        del conv\n",
        "        model.classifier = classifier\n",
        "        \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7h5EFGhe-PJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning driver"
      ]
    },
    {
      "metadata": {
        "id": "Xp8V5o_UfADH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to iterate through all conv2D layers of the network and determine \n",
        "# filters to be pruned, and then carry out the pruning.\n",
        "def PruneAllConv2DLayers(model, prune_settings):\n",
        "    \n",
        "    # Extract pruning settings for convenience\n",
        "    # Note that \"N_prune\" *consecutive* filters will get pruned\n",
        "    N_prune = prune_settings.N_prune\n",
        "    P_prune = prune_settings.P_prune\n",
        "    p = prune_settings.p\n",
        "    pruning_metric = prune_settings.pruning_metric\n",
        "    \n",
        "    # Count number of prunable layers for preallocation\n",
        "    N_layers = len(model.features._modules.items())       \n",
        "    prune_settings.filters_per_layer_orig = np.zeros(shape=(1, N_layers)).ravel()\n",
        "    prune_settings.filters_per_layer_after = np.zeros(shape=(1, N_layers)).ravel()\n",
        "\n",
        "    \n",
        "    # Find the N_prune filters to remove\n",
        "    ii = 0\n",
        "    while ii < len(model.features._modules.items()):\n",
        "        \n",
        "        res = list(model.features._modules.items())[ii]\n",
        "        \n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            _, conv = list(model.features._modules.items())[ii]\n",
        "            \n",
        "            # Record pruning statistics\n",
        "            prune_settings.filters_per_layer_orig[ii] = conv.out_channels\n",
        "            prune_settings.filters_per_layer_after[ii] = conv.out_channels\n",
        "            \n",
        "            # Compute values and indices of the N_prune smallest norms\n",
        "#             if pruning_metric == WEIGHT_NORM:\n",
        "#                 norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "#             elif pruning_metric == ACT_NORM:\n",
        "# #                 norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "#                 norms = ComputeConv2DActNorms(res[1], prune_settings)\n",
        "                \n",
        "        \n",
        "            if (P_prune >= 0):\n",
        "                N_prune = int(conv.out_channels*P_prune/100.0)\n",
        "                prune_settings.N_prune = N_prune\n",
        "            \n",
        "            if prune_settings.norms_botk[ii] is not None:\n",
        "                \n",
        "#                 n_botk, ind_botk = torch.topk(torch.from_numpy(prune_settings.norms_botk[ii]), N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            \n",
        "                norms = np.asarray(prune_settings.norms_botk[ii]).ravel()\n",
        "                ind_botk = np.argpartition(norms, N_prune)    \n",
        "                n_botk = norms[ind_botk[:N_prune]]\n",
        "                ind_botk = ind_botk[:N_prune]\n",
        "        \n",
        "                prune_settings.idx_layer = ii\n",
        "                prune_settings.idx_filter = ind_botk\n",
        "\n",
        "                model = PruneConvLayers(model, prune_settings)\n",
        "                \n",
        "        ii = ii + 1\n",
        "            \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cCEASFCwB3uP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Pruning"
      ]
    },
    {
      "metadata": {
        "id": "LASRjk-I2zXD",
        "colab_type": "code",
        "outputId": "901563a6-fffe-45d0-dfdd-62e22aa68be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Test pruning\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.train()\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10, 12, 15, 16, 21), \n",
        "                                     N_prune=5, p=2, pruning_metric=WEIGHT_NORM)\n",
        "# prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10), \n",
        "#                                      N_prune=1, p=2, pruning_metric=WEIGHT_NORM)\n",
        "\n",
        "N_layers = len(model.features._modules.items())       \n",
        "prune_settings.filters_per_layer_orig = np.zeros(shape=(1, N_layers)).ravel()\n",
        "prune_settings.filters_per_layer_after = np.zeros(shape=(1, N_layers)).ravel()\n",
        "\n",
        "t0 = time.time()\n",
        "model = PruneConvLayers(model, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning took 8.824309349060059 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BgOIBqly2ntw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# L0 Masking Functions"
      ]
    },
    {
      "metadata": {
        "id": "zEl8diRM2sa0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Masked:\n",
        "    def make_mask(self, threshold, mask=None):\n",
        "        if mask is None:\n",
        "            print(\"new mask\", device)\n",
        "            self.mask = torch.ones(self.weight.size(), requires_grad=False).to(device)\n",
        "        else:\n",
        "            self.mask = mask      \n",
        "        self.zeros = torch.zeros(self.weight.size(), requires_grad=False).to(device)\n",
        "        self.threshold = threshold\n",
        "    \n",
        "    def set_threshold(self, prop=0.05):\n",
        "        unique_weights = torch.unique(self.weight*self.mask)\n",
        "        mask_size = self.mask.reshape(-1).size()[0]\n",
        "#     mask_size = mask_size[0]*mask_size[1]\n",
        "        mask_nonzero = torch.sum(self.mask.view([mask_size]))\n",
        "        mask_total = mask_size\n",
        "        print('nonzero proportion: {:.4f}'.format(mask_nonzero/mask_total))\n",
        "        self.threshold = torch.max(torch.topk(torch.abs(unique_weights),int(prop*unique_weights.size()[0]),largest=False)[0])    \n",
        "\n",
        "    def make_threshold_mask(self):\n",
        "        self.mask = torch.where(torch.abs(self.weight) >= self.threshold,self.mask,self.zeros).to(device)\n",
        "#     self.mask.requires_grad_(requires_grad=False)\n",
        "    def mask_weight(self):\n",
        "        self.weight = torch.nn.Parameter(self.weight*self.mask).to(device)\n",
        "    \n",
        "class MaskedLinear(torch.nn.Linear, Masked):\n",
        "    def __init__(self, in_features, out_features, bias=True, threshold=0.001, mask=None):\n",
        "        super(MaskedLinear, self).__init__(in_features,out_features)\n",
        "        self.make_mask(threshold,mask)\n",
        "    def forward(self, input):\n",
        "        self.make_threshold_mask()\n",
        "        self.mask_weight()\n",
        "        #     print(self.mask[125:135,125:135])\n",
        "        #     print(self.weight[125:135,125:135])\n",
        "        return F.linear(input, self.weight, self.bias)\n",
        "\n",
        "class MaskedConv(torch.nn.Conv2d, Masked):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, groups, bias=True, threshold=0.0001):\n",
        "        super(MaskedConv,self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.make_mask(threshold)    \n",
        "    def forward(self, input):\n",
        "        self.mask_weight()\n",
        "        return F.conv2d(input, self.weight, self.bias, self.stride,\n",
        "                    self.padding, self.dilation, self.groups)\n",
        "\n",
        "limit_a, limit_b, epsilon = -.1, 1.1, 1e-6\n",
        "device='cuda'\n",
        "\n",
        "class LinearL0(Module):\n",
        "    \"\"\"Implementation of L0 regularization for the input units of a fully connected layer\"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True, weight_decay=1., droprate_init=0.5, temperature=2./3.,\n",
        "                 lamba=1., local_rep=False, **kwargs):\n",
        "        \"\"\"\n",
        "        :param in_features: Input dimensionality\n",
        "        :param out_features: Output dimensionality\n",
        "        :param bias: Whether we use a bias\n",
        "        :param weight_decay: Strength of the L2 penalty\n",
        "        :param droprate_init: Dropout rate that the L0 gates will be initialized to\n",
        "        :param temperature: Temperature of the concrete distribution\n",
        "        :param lamba: Strength of the L0 penalty\n",
        "        :param local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
        "        \"\"\"\n",
        "        super(LinearL0, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.prior_prec = weight_decay\n",
        "        self.weights = torch.nn.Parameter(torch.Tensor(in_features, out_features).to(device))\n",
        "        #         self.qz_loga = torch.Tensor(in_features).to(device)\n",
        "        self.qz_loga = torch.nn.Parameter(torch.Tensor(in_features).to(device))\n",
        "        self.temperature = temperature\n",
        "        self.droprate_init = droprate_init if droprate_init != 0. else 0.5\n",
        "        self.lamba = lamba\n",
        "        self.use_bias = False\n",
        "        self.local_rep = local_rep\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
        "            self.use_bias = True\n",
        "        self.floatTensor = torch.FloatTensor if not torch.cuda.is_available() else torch.cuda.FloatTensor\n",
        "        self.reset_parameters()\n",
        "        print(self)\n",
        "        \n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.kaiming_normal(self.weights, mode='fan_out')\n",
        "\n",
        "        self.qz_loga.data.normal_(math.log(1 - self.droprate_init) - math.log(self.droprate_init), 1e-2)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias.data.fill_(0)\n",
        "\n",
        "    def constrain_parameters(self, **kwargs):\n",
        "        self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
        "\n",
        "    def cdf_qz(self, x):\n",
        "        \"\"\"Implements the CDF of the 'stretched' concrete distribution\"\"\"\n",
        "        xn = (x - limit_a) / (limit_b - limit_a)\n",
        "        logits = math.log(xn) - math.log(1 - xn)\n",
        "        return F.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=epsilon, max=1 - epsilon).to(device)\n",
        "\n",
        "    def quantile_concrete(self, x):\n",
        "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "        y = F.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature).to(device)\n",
        "        return y * (limit_b - limit_a) + limit_a\n",
        "\n",
        "    def _reg_w(self):\n",
        "        \"\"\"Expected L0 norm under the stochastic gates, takes into account and re-weights also a potential L2 penalty\"\"\"\n",
        "        logpw_col = torch.sum(- (.5 * self.prior_prec * self.weights.pow(2)) - self.lamba, 1).to(device)\n",
        "        logpw = torch.sum((1 - self.cdf_qz(0)) * logpw_col).to(device)\n",
        "        logpb = 0 if not self.use_bias else - torch.sum(.5 * self.prior_prec * self.bias.pow(2)).to(device)\n",
        "        return logpw + logpb\n",
        "        \n",
        "        \n",
        "    def regularization(self):\n",
        "        return self._reg_w()\n",
        "\n",
        "    def count_expected_flops_and_l0(self):\n",
        "        \"\"\"Measures the expected floating point operations (FLOPs) and the expected L0 norm\"\"\"\n",
        "        # dim_in multiplications and dim_in - 1 additions for each output neuron for the weights\n",
        "        # + the bias addition for each neuron\n",
        "        # total_flops = (2 * in_features - 1) * out_features + out_features\n",
        "        ppos = torch.sum(1 - self.cdf_qz(0))\n",
        "        expected_flops = (2 * ppos - 1) * self.out_features\n",
        "        expected_l0 = ppos * self.out_features\n",
        "        if self.use_bias:\n",
        "            expected_flops += self.out_features\n",
        "            expected_l0 += self.out_features\n",
        "#       return expected_flops.data[0], expected_l0.data[0]\n",
        "        return expected_flops, expected_l0\n",
        "\n",
        "    def get_eps(self, size):\n",
        "        \"\"\"Uniform random numbers for the concrete distribution\"\"\"\n",
        "        eps = self.floatTensor(size).uniform_(epsilon, 1-epsilon).to(device)\n",
        "        eps = Variable(eps)\n",
        "        return eps\n",
        "\n",
        "    def sample_z(self, batch_size, sample=True):\n",
        "        \"\"\"Sample the hard-concrete gates for training and use a deterministic value for testing\"\"\"\n",
        "        if sample:\n",
        "            eps = self.get_eps(self.floatTensor(batch_size, self.in_features).to(device))\n",
        "            z = self.quantile_concrete(eps)\n",
        "            return F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "        else:  # mode\n",
        "            pi = F.sigmoid(self.qz_loga).view(1, self.in_features).expand(batch_size, self.in_features).to(device)\n",
        "            return F.hardtanh(pi * (limit_b - limit_a) + limit_a, min_val=0, max_val=1).to(device)\n",
        "        \n",
        "    def sample_weights(self):\n",
        "        z = self.quantile_concrete(self.get_eps(self.floatTensor(self.in_features).to(device)))\n",
        "        mask = F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "        return mask.view(self.in_features, 1) * self.weights\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.local_rep or not self.training:\n",
        "            z = self.sample_z(input.size(0), sample=self.training)\n",
        "            xin = input.mul(z)\n",
        "            output = xin.mm(self.weights)\n",
        "        else:\n",
        "            weights = self.sample_weights()\n",
        "            output = input.mm(weights)\n",
        "        if self.use_bias:\n",
        "            output.add_(self.bias)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = ('{name}({in_features} -> {out_features}, droprate_init={droprate_init}, '\n",
        "            'lamba={lamba}, temperature={temperature}, weight_decay={prior_prec}, '\n",
        "            'local_rep={local_rep}')\n",
        "        if not self.use_bias:\n",
        "            s += ', bias=False'\n",
        "        s += ')'\n",
        "        return s.format(name=self.__class__.__name__, **self.__dict__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ikIYcgE-FbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class L0Conv2d(Module):\n",
        "    \"\"\"Implementation of L0 regularization for the feature maps of a convolutional layer\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,\n",
        "                 droprate_init=0.5, temperature=2./3., weight_decay=1., lamba=1., local_rep=False, **kwargs):\n",
        "        \"\"\"\n",
        "        :param in_channels: Number of input channels\n",
        "        :param out_channels: Number of output channels\n",
        "        :param kernel_size: Size of the kernel\n",
        "        :param stride: Stride for the convolution\n",
        "        :param padding: Padding for the convolution\n",
        "        :param dilation: Dilation factor for the convolution\n",
        "        :param groups: How many groups we will assume in the convolution\n",
        "        :param bias: Whether we will use a bias\n",
        "        :param droprate_init: Dropout rate that the L0 gates will be initialized to\n",
        "        :param temperature: Temperature of the concrete distribution\n",
        "        :param weight_decay: Strength of the L2 penalty\n",
        "        :param lamba: Strength of the L0 penalty\n",
        "        :param local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
        "        \"\"\"\n",
        "        super(L0Conv2d, self).__init__()\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = pair(kernel_size)\n",
        "        self.stride = pair(stride)\n",
        "        self.padding = pair(padding)\n",
        "        self.dilation = pair(dilation)\n",
        "        self.output_padding = pair(0)\n",
        "        self.groups = groups\n",
        "        self.prior_prec = weight_decay\n",
        "        self.lamba = lamba\n",
        "        self.droprate_init = droprate_init if droprate_init != 0. else 0.5\n",
        "        self.temperature = temperature\n",
        "        self.floatTensor = torch.FloatTensor if not torch.cuda.is_available() else torch.cuda.FloatTensor\n",
        "        self.use_bias = False\n",
        "        self.weights = Parameter(torch.Tensor(out_channels, in_channels // groups, *self.kernel_size).to(device))\n",
        "        self.qz_loga = Parameter(torch.Tensor(out_channels).to(device))\n",
        "        self.dim_z = out_channels\n",
        "        self.input_shape = None\n",
        "        self.local_rep = local_rep\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels).to(device))\n",
        "            self.use_bias = True\n",
        "\n",
        "        self.reset_parameters()\n",
        "        print(self)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_normal(self.weights, mode='fan_in')\n",
        "\n",
        "        self.qz_loga.data.normal_(math.log(1 - self.droprate_init) - math.log(self.droprate_init), 1e-2)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias.data.fill_(0)\n",
        "\n",
        "    def constrain_parameters(self, **kwargs):\n",
        "        self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
        "\n",
        "    def cdf_qz(self, x):\n",
        "        \"\"\"Implements the CDF of the 'stretched' concrete distribution\"\"\"\n",
        "        xn = (x - limit_a) / (limit_b - limit_a)\n",
        "        logits = math.log(xn) - math.log(1 - xn)\n",
        "        return F.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=epsilon, max=1 - epsilon)\n",
        "\n",
        "    def quantile_concrete(self, x):\n",
        "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "        y = F.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature)\n",
        "        return y * (limit_b - limit_a) + limit_a\n",
        "\n",
        "    def _reg_w(self):\n",
        "        \"\"\"Expected L0 norm under the stochastic gates, takes into account and re-weights also a potential L2 penalty\"\"\"\n",
        "        q0 = self.cdf_qz(0)\n",
        "        logpw_col = torch.sum(- (.5 * self.prior_prec * self.weights.pow(2)) - self.lamba, 3).sum(2).sum(1)\n",
        "        logpw = torch.sum((1 - q0) * logpw_col).to(device)\n",
        "        logpb = 0 if not self.use_bias else - torch.sum((1 - q0) * (.5 * self.prior_prec * self.bias.pow(2) -\n",
        "                                                                    self.lamba))\n",
        "        return logpw + logpb\n",
        "\n",
        "    def regularization(self):\n",
        "        return self._reg_w()\n",
        "\n",
        "    def count_expected_flops_and_l0(self):\n",
        "        \"\"\"Measures the expected floating point operations (FLOPs) and the expected L0 norm\"\"\"\n",
        "        ppos = torch.sum(1 - self.cdf_qz(0))\n",
        "        n = self.kernel_size[0] * self.kernel_size[1] * self.in_channels  # vector_length\n",
        "        flops_per_instance = n + (n - 1)  # (n: multiplications and n-1: additions)\n",
        "\n",
        "        num_instances_per_filter = ((self.input_shape[1] - self.kernel_size[0] + 2 * self.padding[0]) / self.stride[0]) + 1  # for rows\n",
        "        num_instances_per_filter *= ((self.input_shape[2] - self.kernel_size[1] + 2 * self.padding[1]) / self.stride[1]) + 1  # multiplying with cols\n",
        "\n",
        "        flops_per_filter = num_instances_per_filter * flops_per_instance\n",
        "        expected_flops = flops_per_filter * ppos  # multiply with number of filters\n",
        "        expected_l0 = n * ppos\n",
        "\n",
        "        if self.use_bias:\n",
        "            # since the gate is applied to the output we also reduce the bias computation\n",
        "            expected_flops += num_instances_per_filter * ppos\n",
        "            expected_l0 += ppos\n",
        "\n",
        "#         return expected_flops.data[0], expected_l0.data[0]\n",
        "        return expected_flops, expected_l0\n",
        "\n",
        "    def get_eps(self, size):\n",
        "        \"\"\"Uniform random numbers for the concrete distribution\"\"\"\n",
        "        eps = self.floatTensor(size).uniform_(epsilon, 1-epsilon).to(device)\n",
        "        eps = Variable(eps)\n",
        "        return eps\n",
        "\n",
        "    def sample_z(self, batch_size, sample=True):\n",
        "        \"\"\"Sample the hard-concrete gates for training and use a deterministic value for testing\"\"\"\n",
        "        if sample:\n",
        "            eps = self.get_eps(self.floatTensor(batch_size, self.dim_z)).to(device)\n",
        "            z = self.quantile_concrete(eps).view(batch_size, self.dim_z, 1, 1)\n",
        "            return F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "        else:  # mode\n",
        "            pi = F.sigmoid(self.qz_loga).view(1, self.dim_z, 1, 1)\n",
        "            return F.hardtanh(pi * (limit_b - limit_a) + limit_a, min_val=0, max_val=1).to(device)\n",
        "\n",
        "    def sample_weights(self):\n",
        "        z = self.quantile_concrete(self.get_eps(self.floatTensor(self.dim_z).to(device))).view(self.dim_z, 1, 1, 1)\n",
        "        return F.hardtanh(z, min_val=0, max_val=1).to(device) * self.weights\n",
        "\n",
        "    def forward(self, input_):\n",
        "        if self.input_shape is None:\n",
        "            self.input_shape = input_.size()\n",
        "        b = None if not self.use_bias else self.bias\n",
        "        if self.local_rep or not self.training:\n",
        "            output = F.conv2d(input_, self.weights, b, self.stride, self.padding, self.dilation, self.groups)\n",
        "            z = self.sample_z(output.size(0), sample=self.training)\n",
        "            return output.mul(z)\n",
        "        else:\n",
        "            weights = self.sample_weights()\n",
        "            output = F.conv2d(input_, weights, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = ('{name}({in_channels}, {out_channels}, kernel_size={kernel_size}, stride={stride}, '\n",
        "             'droprate_init={droprate_init}, temperature={temperature}, prior_prec={prior_prec}, '\n",
        "             'lamba={lamba}, local_rep={local_rep}')\n",
        "        if self.padding != (0,) * len(self.padding):\n",
        "            s += ', padding={padding}'\n",
        "        if self.dilation != (1,) * len(self.dilation):\n",
        "            s += ', dilation={dilation}'\n",
        "        if self.output_padding != (0,) * len(self.output_padding):\n",
        "            s += ', output_padding={output_padding}'\n",
        "        if self.groups != 1:\n",
        "            s += ', groups={groups}'\n",
        "        if not self.use_bias:\n",
        "            s += ', bias=False'\n",
        "        s += ')'\n",
        "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jyVxLH0h5HEQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mask_network(network, layers_to_mask, threshold=0.002, linear_masking=None, random_init=False, bias=True, masks=None):\n",
        "    \"\"\"\"\n",
        "    replaces linear layers with masked linear layers\n",
        "    replaces conv layers with masked conv layers\n",
        "    network is the initial sequential container\n",
        "    layers is a list of layers to mask\n",
        "    random init is a logical indicating whether to preserve the initial weights or to modify them\n",
        "    \"\"\"\n",
        "\n",
        "    network.masked_layers=[]\n",
        "    ii = -1\n",
        "    for layer, (name, module) in enumerate(network._modules.items()):\n",
        "        \n",
        "            ii += 1\n",
        "            \n",
        "            layer_mask = None\n",
        "            if masks is not None:\n",
        "                if name in masks:\n",
        "                    layer_mask = masks.get(name)      \n",
        "            if isinstance(module, torch.nn.Linear) and linear_masking is None:\n",
        "                masked_layer = MaskedLinear(layer.in_features, layer.out_features, bias=bias,threshold=threshold,mask=layer_mask)\n",
        "            elif isinstance(module, torch.nn.Linear) and linear_masking =='L0':\n",
        "                _, layer = list(network._modules.items())[ii]\n",
        "                masked_layer = LinearL0(layer.in_features, layer.out_features, bias=bias, lamba=0.1/640)\n",
        "                network.masked_layers.append(masked_layer)\n",
        "            elif isinstance(module, torch.nn.Conv2d):\n",
        "                _, layer = list(network._modules.items())[ii]\n",
        "                masked_layer = L0Conv2d(layer.in_channels, layer.out_channels, layer.kernel_size, layer.stride, layer.padding, layer.dilation, layer.groups, bias=bias, \n",
        "                                        droprate_init=0.5, temperature=2./3., weight_decay=1., lamba=0.1/640, local_rep=False)\n",
        "                network.masked_layers.append(masked_layer)\n",
        "            else:\n",
        "                continue\n",
        "            if random_init != True:\n",
        "                masked_layer.weight = copy.deepcopy(layer.weight)\n",
        "                masked_layer.bias = copy.deepcopy(layer.bias)\n",
        "                \n",
        "            network[int(name)] = masked_layer\n",
        "            \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YbnaQGqI6PAy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VGG_L0(tv_vgg.VGG):\n",
        "    def regularization(self):\n",
        "        regularization = 0.\n",
        "        for layer in self.layers:\n",
        "            regularization += - (1. / self.N) * layer.regularization()\n",
        "#         if torch.cuda.is_available():\n",
        "#             regularization = regularization.cuda()\n",
        "        return regularization\n",
        "    \n",
        "    def regularize(self, N):\n",
        "        regularization = 0.\n",
        "        for layer in self.masked_layers:\n",
        "            regularization += - (1. / N) * layer.regularization()          \n",
        "#         if torch.cuda.is_available():\n",
        "#             regularization = regularization.cuda()\n",
        "        return regularization\n",
        "\n",
        "    def clamp_parameters(self):\n",
        "        for layer in self.masked_layers:\n",
        "            layer.constrain_parameters()\n",
        "    \n",
        "    def get_exp_flops_l0(self):\n",
        "        total_flops = 0\n",
        "        total_l0 = 0\n",
        "#         print(self.masked_layers)\n",
        "        for layer in self.masked_layers:\n",
        "            exp_flops, exp_l0 = layer.count_expected_flops_and_l0()\n",
        "            total_flops += exp_flops\n",
        "            total_l0 += exp_l0\n",
        "        return total_flops, total_l0\n",
        "    \n",
        "    \n",
        "def vgg16_L0(pretrained=False, **kwargs):\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\")\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGG_L0(tv_vgg.make_layers(tv_vgg.cfg['D']), **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(tv_vgg.model_urls['vgg16']))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quImFgeUDM1U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def freeze_layers(model_ft, exclude=[]):\n",
        "#   children = list(model_ft.named_children())\n",
        "    for name,param in model_ft.named_parameters():   \n",
        "        if(name not in exclude):\n",
        "            param.requires_grad = False\n",
        "        \n",
        "def CountNonZeroWeights(model):\n",
        "    \n",
        "    total = 0\n",
        "    zeros = 0\n",
        "    nonzeros = 0\n",
        "    \n",
        "    tol = 1.0e-15\n",
        "    \n",
        "    for name, param in model.named_parameters():\n",
        "        if param is not None:\n",
        "            \n",
        "            param = torch.zeros(3,3)\n",
        "#             print(torch.min(torch.abs(param)))\n",
        "            nonzeros += torch.sum((param != 0.0).int())\n",
        "            zeros += torch.sum((param == 0.0).int())\n",
        "\n",
        "    total = zeros + nonzeros\n",
        "    \n",
        "    print(\"Total number of weights: {}\".format(total))\n",
        "    print(\"Number of non-zero weights: {}\".format(nonzeros))\n",
        "    print(\"Number of zero weights: {}\".format(zeros))\n",
        "    \n",
        "    return total, nonzeros, zeros\n",
        "\n",
        "\n",
        "def set_threshold(model, prop=0.05):\n",
        "    for child in model.named_children():    \n",
        "        for child in child[1].named_children():\n",
        "#       print(child)\n",
        "            if type(child[1]) == MaskedLinear or type(child[1]) == MaskedConv: \n",
        "                child[1].set_threshold(prop=prop)\n",
        "                print(\"layer {}  new threshold {:.4f}\".format(child[0], child[1].threshold))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nclNqiyMDzp9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model_prune(model, dloaders, dataset_sizes, criterion, optimizer, scheduler,prop=0.05, num_epochs=25, device='cuda',pruning='threshold'):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    print(len(dloaders['train']))\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "                data_idx = 0\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                data_idx = 1\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            i=0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dloaders[phase]:               \n",
        "#                 print(\"batch {} phase {}\".format(i, phase))\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    if pruning == 'L0':\n",
        "                        loss = criterion(outputs, labels, model)\n",
        "                    else:\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        model.clamp_parameters()\n",
        "                        exp_flops, exp_l0 = model.get_exp_flops_l0()\n",
        "                i+=1\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                           \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            if epoch % 5 == 0 and phase == 'train': \n",
        "                CountNonZeroWeights(model)\n",
        "                \n",
        "                if pruning == 'threshold':\n",
        "                    set_threshold(model,prop=prop)\n",
        "                elif pruning == 'L0':\n",
        "                    print(\"Expected flops: {} | Expected L0 norm: {}\".format(exp_flops.item(), exp_l0.item()))\n",
        "            \n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YC2Z_YkP68mR",
        "colab_type": "code",
        "outputId": "6caa2539-a91b-484a-c126-436e73fd1516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2490
        }
      },
      "cell_type": "code",
      "source": [
        "def run_normal_training_with_L0_pruning(this_trainset):\n",
        "#     print(this_trainset.__len__())  \n",
        "    _,mytrainset = torch.utils.data.random_split(this_trainset, (49200, 800))\n",
        "    # _,trainset = torch.utils.data.random_split(trainset,(49995,5))\n",
        "#     print(mytrainset.__len__())\n",
        "\n",
        "    mytrain_data, myval_data = torch.utils.data.random_split(mytrainset,(int(0.8*len(mytrainset)),int(0.2*len(mytrainset))))\n",
        "#     print(mytrain_data.__len__(),myval_data.__len__() )\n",
        "\n",
        "    mytrainloader = torch.utils.data.DataLoader(mytrain_data, batch_size=5,\n",
        "                                                shuffle=True, num_workers=0)\n",
        "    myvalloader = torch.utils.data.DataLoader(myval_data, batch_size=5,\n",
        "                                              shuffle=True, num_workers=0)\n",
        "    mydataloaders = {'train': mytrainloader, 'val': myvalloader}\n",
        "    image_datasets = {'train': mytrain_data,'val': myval_data}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}  \n",
        "\n",
        "    model_ft = vgg16_L0(pretrained=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    # ------ Mask classifier layers ------\n",
        "    \n",
        "#     freeze_layers(model_ft.features, exclude=[])\n",
        "    mask_network(model_ft.classifier, [0,3,6], linear_masking=\"L0\")\n",
        "    model_ft.masked_layers = model_ft.classifier.masked_layers\n",
        "    \n",
        "    # ------ Mask conv2D layers ------\n",
        "    \n",
        "    mask_network(model_ft.features, np.arange(0, 30), linear_masking=\"L0\")\n",
        "    model_ft.masked_layers.extend(model_ft.features.masked_layers)\n",
        "\n",
        "    print(\"Number of masked layers: {}\".format(len(model_ft.masked_layers)))\n",
        "\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    def loss_function(outputs, targets, model):\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss += model.regularize(640)\n",
        "        return loss\n",
        "    \n",
        "    # Observe that all parameters are being optimized\n",
        "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "#   [print(p) for p in model_ft.parameters()]\n",
        "#   return\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "    model_ft = train_model_prune(model_ft, mydataloaders, dataset_sizes, loss_function, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=20, pruning=\"L0\")\n",
        "\n",
        "    \n",
        "    \n",
        "transform = transforms.Compose(\n",
        "    [transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=5,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "\n",
        "run_normal_training_with_L0_pruning(trainset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:85: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LinearL0(25088 -> 4096, droprate_init=0.5, lamba=0.00015625, temperature=0.6666666666666666, weight_decay=1.0, local_rep=False)\n",
            "LinearL0(4096 -> 4096, droprate_init=0.5, lamba=0.00015625, temperature=0.6666666666666666, weight_decay=1.0, local_rep=False)\n",
            "LinearL0(4096 -> 1000, droprate_init=0.5, lamba=0.00015625, temperature=0.6666666666666666, weight_decay=1.0, local_rep=False)\n",
            "L0Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "Number of masked layers: 16\n",
            "128\n",
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 52.2036 Acc: 0.0766\n",
            "Total number of weights: 276739174\n",
            "Number of non-zero weights: 276739174\n",
            "Number of zero weights: 2\n",
            "Expected flops: 248126439424.0 | Expected L0 norm: 115086080.0\n",
            "val Loss: 51.8063 Acc: 0.0750\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 51.4088 Acc: 0.0984\n",
            "val Loss: 50.9114 Acc: 0.0750\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 50.4069 Acc: 0.1172\n",
            "val Loss: 49.7403 Acc: 0.0938\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 49.1767 Acc: 0.1125\n",
            "val Loss: 48.4951 Acc: 0.0938\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 48.2310 Acc: 0.1109\n",
            "val Loss: 47.8873 Acc: 0.1125\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 47.8426 Acc: 0.0938\n",
            "Total number of weights: 276739175\n",
            "Number of non-zero weights: 276739175\n",
            "Number of zero weights: 1\n",
            "Expected flops: 248030838784.0 | Expected L0 norm: 115060672.0\n",
            "val Loss: 47.6872 Acc: 0.1063\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 47.6933 Acc: 0.1141\n",
            "val Loss: 47.5875 Acc: 0.0938\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 47.6173 Acc: 0.1109\n",
            "val Loss: 47.5792 Acc: 0.0938\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 47.6042 Acc: 0.1203\n",
            "val Loss: 47.5707 Acc: 0.0938\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 47.6161 Acc: 0.0859\n",
            "val Loss: 47.5639 Acc: 0.0938\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 47.6012 Acc: 0.1156\n",
            "Total number of weights: 276739175\n",
            "Number of non-zero weights: 276739175\n",
            "Number of zero weights: 1\n",
            "Expected flops: 248004214784.0 | Expected L0 norm: 115053424.0\n",
            "val Loss: 47.5555 Acc: 0.0938\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 47.5762 Acc: 0.1109\n",
            "val Loss: 47.5472 Acc: 0.0938\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 47.5781 Acc: 0.0953\n",
            "val Loss: 47.5411 Acc: 0.0938\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 47.5591 Acc: 0.1281\n",
            "val Loss: 47.5335 Acc: 0.0938\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 47.5638 Acc: 0.1094\n",
            "val Loss: 47.5329 Acc: 0.0938\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 47.5705 Acc: 0.0875\n",
            "Total number of weights: 276739175\n",
            "Number of non-zero weights: 276739175\n",
            "Number of zero weights: 1\n",
            "Expected flops: 247998119936.0 | Expected L0 norm: 115051768.0\n",
            "val Loss: 47.5321 Acc: 0.0938\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 47.5581 Acc: 0.1000\n",
            "val Loss: 47.5314 Acc: 0.0938\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 47.5531 Acc: 0.1188\n",
            "val Loss: 47.5307 Acc: 0.0938\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 47.5589 Acc: 0.1063\n",
            "val Loss: 47.5300 Acc: 0.0938\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 47.5574 Acc: 0.1125\n",
            "val Loss: 47.5293 Acc: 0.0938\n",
            "\n",
            "Training complete in 10m 15s\n",
            "Best val Acc: 0.112500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0tfmvixSCzAH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Setup Routines"
      ]
    },
    {
      "metadata": {
        "id": "fF2ZhlgCC31h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline Model Setup"
      ]
    },
    {
      "metadata": {
        "id": "GeD0hZfpDHbr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_baseline = models.vgg16(pretrained=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_baseline = model_baseline.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model_baseline.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lc0YTkyADA63",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruned Model Setup"
      ]
    },
    {
      "metadata": {
        "id": "cr4wgE6eDKND",
        "colab_type": "code",
        "outputId": "0f2c5d59-5f0f-4a76-918f-858c3ab87e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "cell_type": "code",
      "source": [
        "# Test pruning all layers\n",
        "\n",
        "model_pruned = models.vgg16(pretrained=True)\n",
        "model_pruned.train()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_pruned = model_pruned.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model_pruned.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(28, 10, N_prune = 4, p = 2, pruning_metric = WEIGHT_NORM)\n",
        "\n",
        "t0 = time.time()\n",
        "model_pruned = PruneAllConv2DLayers(model_pruned, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c16a3b839ee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel_pruned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPruneAllConv2DLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pruned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Pruning took {} s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-68787035a1e4>\u001b[0m in \u001b[0;36mPruneAllConv2DLayers\u001b[0;34m(model, prune_settings)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mprune_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_prune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN_prune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mprune_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorms_botk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#                 n_botk, ind_botk = torch.topk(torch.from_numpy(prune_settings.norms_botk[ii]), N_prune, 0, largest=False, sorted=True, out=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "coPfbZmXE3T0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Driver Routines"
      ]
    },
    {
      "metadata": {
        "id": "DS7qbZ0JE6Fs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Iterative Pruning"
      ]
    },
    {
      "metadata": {
        "id": "E1q-h_KTE-kE",
        "colab_type": "code",
        "outputId": "0e0c512c-b3a7-4279-a6a5-c59195d75479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3329
        }
      },
      "cell_type": "code",
      "source": [
        "# ====== Dataset setup ======\n",
        "\n",
        "percent_data = 5.0\n",
        "percent_val = 20.0\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "# ====== Model setup ======\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# ====== Pruning setup ======\n",
        "\n",
        "N_prune = 0\n",
        "P_prune = 5\n",
        "p = 2\n",
        "prune_settings = UnitPruningSettings(N_prune=N_prune, \n",
        "                                     P_prune=P_prune, \n",
        "                                     p=p, \n",
        "                                     pruning_metric=WEIGHT_NORM)\n",
        "prune_settings.Setup(model)\n",
        "\n",
        "\n",
        "# ====== Begin training ======\n",
        "\n",
        "N_iter_outer = 12\n",
        "N_iter_inner = 10\n",
        "\n",
        "dataset = 'cifar10'\n",
        "\n",
        "datasets = ('cifar10',\\\n",
        "            'cifar100',\\\n",
        "            'mnist',\\\n",
        "            'fashionmnist',\\\n",
        "            'kmnist',\\\n",
        "            'emnist')\n",
        "\n",
        "# Import data\n",
        "dat = DatasetManager(dataset=dataset, \n",
        "                     percent_data=percent_data, \n",
        "                     percent_val=percent_val)\n",
        "\n",
        "dat.ImportDataset(batch_size=batch_size)\n",
        "\n",
        "for ii in range(0, N_iter_outer):\n",
        "    \n",
        "    print(\"\\n------ Outer iteration {}/{} ------\".format(ii+1, N_iter_outer))\n",
        "#     t0 = time.time()\n",
        "\n",
        "    # ------ Prune current model ------\n",
        "        \n",
        "    model = PruneAllConv2DLayers(model, prune_settings)\n",
        "    new_model = copy.deepcopy(model)\n",
        "    model = new_model\n",
        "    prune_settings.PrintPruningStatistics(1)\n",
        "    prune_settings.ResetNormContainers()\n",
        "\n",
        "    # ------ Train current model ------\n",
        "    \n",
        "    # Import data\n",
        "    dataset = datasets[ii%6]\n",
        "    print(\"Using dataset {}\".format(dataset))\n",
        "    dat = DatasetManager(dataset=dataset, percent_data=percent_data, percent_val=percent_val)\n",
        "    dat.ImportDataset(batch_size=batch_size)\n",
        "    \n",
        "    # Update optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, prune_settings, num_epochs=N_iter_inner)\n",
        "        \n",
        "#     print (\"Pruning took {} s\".format(time.time() - t0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "\n",
            "------ Outer iteration 1/12 ------\n",
            "Total number of filters before pruning: 4224.0\n",
            "Total number of filters after pruning: 4224.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 2.6531 Acc: 0.1340\n",
            "val Loss: 2.1361 Acc: 0.1360\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 2.1491 Acc: 0.2140\n",
            "val Loss: 1.8366 Acc: 0.3400\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.9439 Acc: 0.2970\n",
            "val Loss: 1.6386 Acc: 0.4080\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.7757 Acc: 0.3520\n",
            "val Loss: 1.5771 Acc: 0.4620\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 1.6294 Acc: 0.4115\n",
            "val Loss: 1.6836 Acc: 0.4080\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 1.5637 Acc: 0.4300\n",
            "val Loss: 1.5886 Acc: 0.4360\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 1.4617 Acc: 0.4885\n",
            "val Loss: 1.3764 Acc: 0.5100\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 1.1950 Acc: 0.5790\n",
            "val Loss: 1.0736 Acc: 0.6000\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 1.0365 Acc: 0.6370\n",
            "val Loss: 1.0394 Acc: 0.6200\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.9779 Acc: 0.6505\n",
            "val Loss: 1.0561 Acc: 0.6080\n",
            "\n",
            "Training complete in 14m 13s\n",
            "Best val Acc: 0.620000\n",
            "\n",
            "------ Outer iteration 2/12 ------\n",
            "Total number of filters before pruning: 4224.0\n",
            "Total number of filters after pruning: 4020.0\n",
            "Using dataset cifar100\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 5.2167 Acc: 0.0100\n",
            "val Loss: 4.8054 Acc: 0.0060\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 4.7152 Acc: 0.0250\n",
            "val Loss: 4.5201 Acc: 0.0340\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 4.3934 Acc: 0.0545\n",
            "val Loss: 4.2863 Acc: 0.0640\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 4.1576 Acc: 0.0720\n",
            "val Loss: 4.0493 Acc: 0.0580\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 3.9511 Acc: 0.0930\n",
            "val Loss: 3.9308 Acc: 0.0980\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 3.7890 Acc: 0.1115\n",
            "val Loss: 3.8960 Acc: 0.1040\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 3.6402 Acc: 0.1350\n",
            "val Loss: 3.8527 Acc: 0.1140\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 3.1604 Acc: 0.2115\n",
            "val Loss: 3.4759 Acc: 0.1600\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 2.9537 Acc: 0.2555\n",
            "val Loss: 3.4311 Acc: 0.1840\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 2.9029 Acc: 0.2665\n",
            "val Loss: 3.4137 Acc: 0.1780\n",
            "\n",
            "Training complete in 13m 49s\n",
            "Best val Acc: 0.184000\n",
            "\n",
            "------ Outer iteration 3/12 ------\n",
            "Total number of filters before pruning: 4020.0\n",
            "Total number of filters after pruning: 3822.0\n",
            "Using dataset mnist\n",
            "\n",
            "Full training set size: 60000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2400\n",
            "Active validation set size: 600\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.5715 Acc: 0.4854\n",
            "val Loss: 1.1058 Acc: 0.6183\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 0.9774 Acc: 0.6717\n",
            "val Loss: 1.1694 Acc: 0.6500\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.9283 Acc: 0.6921\n",
            "val Loss: 0.9775 Acc: 0.6650\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.8198 Acc: 0.7246\n",
            "val Loss: 0.7171 Acc: 0.7683\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.7224 Acc: 0.7625\n",
            "val Loss: 0.7990 Acc: 0.7400\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v7ORnILPDbVu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ]
    },
    {
      "metadata": {
        "id": "Icv-APyJDdB5",
        "colab_type": "code",
        "outputId": "ea77899f-a08b-4e47-a7f9-21f6c621718a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "PlotResults(prune_settings, \"pPruneActNorm5_pre_out_7_in_7_multiple_5percent\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd4VFXawH8nBVLIJCGBgKIgIiBS\nBBQFFFGwrA37Iq6yNmyL5Vtdy9pWXcvqoq6KZZdVXLti74pEsGEDKSJFCb0Fkkx6m/P98d47mdSZ\nhLkzd5Lze5555s6t5yQz971vV1prDAaDwdBxiYv2AAwGg8EQXYwgMBgMhg6OEQQGg8HQwTGCwGAw\nGDo4RhAYDAZDB8cIAoPBYOjgGEFgMBgMHRwjCAwGg6GDYwSBwWAwdHASoj2AUMjOztZ9+vRpcZ/S\n0lJSU1MjMyCX0ZHnDh17/mbuHXPuENr8f/jhh3ytdbdg54oJQdCnTx++//77FvfJzc1l/PjxkRmQ\ny+jIc4eOPX8z9/HRHkbUCGX+Sql1oZzLmIYMBoOhg2MEgcFgMHRwHBUESqlrlFLLlVLLlFIvKqWS\nlFL7KKUWKqXWKKVeVkp1cnIMBoPBYGgZxwSBUmpP4ErgIK31YCAemAzcBzyote4HFAAXOjUGg8Fg\nMATHadNQApCslEoAUoAtwFHAa9b22cApDo/BYDAYDC2gnGxMo5S6Cvg7UA58DFwFfGNpAyil9gI+\nsDSGhsdOA6YB5OTkjHzppZdavFZJSQldunQJ7wRihI48d+jY8zdz75hzh9Dmf+SRR/6gtT4o2Lkc\nCx9VSmUCk4B9gELgVeC4UI/XWj8FPAVw0EEH6WBhUh05lKwjzx069vzN3MdHexhRI5zzd9I0NBFY\nq7XeobWuBl4HxgIZlqkIoBewycExGNoBzzwDl1wCFRXRHonB0D5xUhCsBw5VSqUopRQwAfgZmAec\nYe0zFXjLwTEYYpwtW+DSS+Gpp2DmzGiPxmBonzgmCLTWCxGn8I/AUutaTwHXA/+nlFoDZAGznBqD\nIfb5xz+gslKW774bioqiOx6DoT3iaNSQ1vo2rfVArfVgrfW5WutKrfVvWutRWut+WusztdaVTo7B\nELts3QpPPCHLAwbAzp3wz39Gd0wGQ3vEZBYbXMv994tf4JRTYJalN86YAdu3R3dcBkN7wwgCgyvZ\nvh0ef1yWb70Vxo6FE0+E0lL4+9+jOzaDob1hBIHBlTzwAJSXw0knwfDhsu7vfwelREDk5UV1eAZD\nu8IIAoPr2LEDHntMlm+7rW790KFwzjlQXW2tr6yE118nacuWqIzTYGgvGEFgcB0zZkBZGZxwAowc\nWX/b3/4GCQnwv/9plp16C5x+OodOmQIjRsCdd8Ly5eBgtrzB0B4xgsDgKnbuhEcfleVbb228vW9f\nSS7TWnHzB2MgOZma5GRYtEgOGDwYBg6EG2+EtWsjO3iDIUYxgsDgKmbMgJISOO44GDWq6X1uPmUZ\nKZTyFqfw9V/e4Ks334R334ULLoCsLFi1iup7H4DTT4/s4A2GGMUIAoNr2LULHnlElgN9A/Xweulx\n+Wlcw4MA3Jh7LOU6iW+yTuDR4bP44/HbGdzbSxIVTF52c2QGbjDEODHRs9jQMXjoISguhmOOgUMP\nbWIHrWHaNFi9mmsP+JCZm//K558rFiw4HJ/P3ikOSAPgzerjqamoISHJfM0NhpYwGoHBFWzZAg8/\nLMtN+QYAePJJePll6NKFjDmzuP125d80ZAj88Y/iX/jmG9grbiOVJLHmB1OTwmAIhnlUMkSd6mo4\n6yzweuF3v5PksUYsWgRXXy3LTz0FAwYwvT8cfzysWbOA444bV2/3oclr2FDai6XfljFwbJbzkzAY\nYhijERiizl/+Al98AXvuCU8/3cQOXi+ceabkDVxyCZx9NiDJZf36QVKSr9EhQzI2ALBkceNtBoOh\nPkYQGKLKSy+JbyAxEV59FXJyGuzw7beiJvz6KwwbBg8+GNJ5h/aQgkRLVySGecQGQ/vDCAJD1Fi+\nHC68UJYfeghGjw7YuGwZnHoqHHIIfPUVZGeLpEhODuncQ/YW38DStR23laHBECpGEBiiQlERnHaa\nZBCfey5cdpm1Yc0a+MMfpJ7Em2/Kjf+GG2DlSthvv5DPP2DfGhKp4rd8DyUlzszBYGgvGEFgiDha\nS4TPqlVyv3/iCVC1NXDllbD//vD881JHYvp0+O03uOce6Nq1VddIzOnK/qwARLkwGAzN42Tz+gHA\nywGr+gK3As9a6/sAecBZWusCp8ZhcB//+Ic87Kenw5w5kJIC3HWvZJPFxUmG8K23Qu/ebb9IdjZD\nWMoShrF0aTN5CVFg40YJgCookAS6goK6V1aWVF1NMLF8hgjj2FdOa70SOBBAKRWPNKl/A7gBmKu1\nvlcpdYP1+XqnxmFwF198ATfdJMv/+59E/fD991JNDuD99+HYY3f/QtnZDGEBAEuX7v7pwsHcuVJI\nr7KFnnwnnwxHHRW5MRkMELk8ggnAr1rrdUqpScB4a/1sIBcjCDoMDz8MPh9ce630GqCsTHwCNTVw\n1VXhEQLg1wjAHYJg4UKYNEmEwJgxsM8+kJkpFq/MTMmT++YbSawzGCJNpATBZOBFazlHa21/3bcC\nDQMGDe2U0lJ54Acx/wOSRLByJQwaJL6AcJGVxVCWALBkifgllApyjEMsXy6Jb6WlIvNmzxYLWCB5\nuXl8Qx+2bq4F4qMyzlZRXCxNI844w1LrDLGM44JAKdUJOBm4seE2rbVWSjVZPF4pNQ2YBpCTk0Nu\nbm6L1ykpKQm6T3slVub++efdKCs7gIEDvfz2248UvvQtQx97DF9CAj9ecw0lCxe26bxNzT+huJix\nbCKDAnbtymTOnK/Izq4Kwyxax9atSUyfPpxduzozZkw+U6cuZ/78+l/5lLw8ur27BriLpR8sIvfg\n0MOcovK/15pBd9xB99xctixYwMrrrovs9S1i5XvvFGGdv9ba0RcwCfg44PNKoKe13BNYGewcI0eO\n1MGYN29e0H3aK7Ey99//XmvQ+v77tdY7dmjdo4esuPfe3Tpvk/OvrdU6Pl4fzucatP7ww926RJvY\nulXrfv1kiuPGaV1W1sROFRVaDxumZ3G+Bq3PG5/XqmtE5X//9NMyKdD61FMjf32LWPneO0Uo8we+\n1yHcpyMRPno2dWYhgLeBqdbyVOCtCIzBEGXKy6VlAMDpp2kpFbF1Kxx+uDgMwk1cHGRlRc1PUFgo\n7o41a6Tn8ttvN5MLd+ON8NNP9GArANvyXW4WWrMG/vSnus9lZdEbiyFsOCoIlFKpwNHA6wGr7wWO\nVkqtBiZanw3tnA8/FBv5QQfBPvNnw+uvQ1oaPPssxDt088vOrucniBRbt8KJJ8JPP0H//jL39PQm\ndvz4YymZER9PzoE9AdhW0ClyA20t1dUwZYr8I/v3l3WlpdEdkyEsOOoj0FqXAlkN1u1EoogMHYjX\nXpP3MybsksQxkJrRffo4d9EIRg75fDBvniTHvfmmBEH16gWffALduzdxwI4dMNVSjP/2N3J2JsNi\n2FaU5OxAd4fbboPvvoO99xZH8dFHG0HQTjCZxQbHqaiAd96R5TO2zZSIk9NOk9oSTpKdzWAkrXjF\nCrk5h5v8fEkCGzAAJk4Ugae1hIrOmyf3zEZoLUlzW7fCuHFwww107yla0fbS1IAmOy4iNxfuvVdM\nbs89J6ViwZiG2glGEBgc5+OP5d4/fDjs+83zsvLqq52P58zOxkMxvbsWU1kJq1eH79QrVsD558v9\n8LrrxHTeq5fkxa1bJ1pBs1GVjz8uDpOMDLmpxsfTKdtDJruo1fHs3Bm+ce4uixbB288XU/mHC0WA\n/fWv4tdJTZUdjEbQLjCCwOA4r74q72ceXQi//CK+gUjUfMjOBmBoN0lbCeYn2LEjuNbw7bdSFHXQ\nIHjmGTGbn3CCOIPXrpXKGPbDcpMsXw5//rMsP/UU7LWXLKenk8M2ALZtCzIvhykthVmzYNQoGDEC\nJv0hjaGb3ufT/afXtY9LSanb2RDzGEFgcJTKSrlJApyR9pEsTJggDQicxhIEQzx5QMt+ghdfFFt+\nWpqUw54+XRK/li0T4fDJJzLsQw6Rp/3OnaVi6po18nB/0klBagTV1sof4rTTxFZ2/vnSbMcmI6Mu\ncihKgmDpUgkI2mMPuOgicQdkpFTSl19ZxQCOXvEvJv8hgU2bqNMIjGmoXWDKWxkc5ZNPpMHYsGGw\n32JLNQhXGYlgZEmcwpBOq4BjmhUEWsPdd8tyRYWUevjmm7rtiYny5A8iKC6/XCxbPXqEMIbiYmm7\n9q9/SXMdgIED5XMg6enkILYrpwRBVZUEaW3YUL/g3a5d4utYs6Zu39Gj4ZI/VnLWzf2JL9vCP8/4\nhjvfG8HLL8N778Hfbk9iOokkVlaKkHMq8ssQEYwgMDiKP1rotFqY8al8iJQgsDWCIJFD8+fLk39O\nDixeLMvffw8//CDveXnQrRtcc41oARkZIVx77VqppjprlkhCkAJDV14pjuIuDRrmZGT4TUNbt7Zh\nriHw73/XTwFoSFqa+O8vuUTKg/OPh2HHehg1ihtfGc6U9SIA33wT/nyt4mm1iBf17xlcViYHG2IW\nIwgMjlFVBW9Z6YJn7veTdKPZbz+5IUYCSxD0L19Mp05yby4ubnzPeuQReb/kEnnK79FDIoBsCgvF\nEhLUmlVaCm+8IY/dn34qqgbAEUfIHfSkk5p/co6Aj+BFK61z6lRx3AcWvcvMlEhe2/RPcbHUCwe4\n4w5Qit69ZXrvvy+ms2W/HcADXMszpaVGEMQ4RhAYHGPuXLmJDh4MA36xJEKktAHwC4LEXdvZf39J\n8Fq2rH5LzA0b5Ak3IUEEQVO0qAH4fKJSPPuseMXtdmidOsHkyVJRdcSI4GOtJwg0EN6Iqo0b4csv\nISlJ0jcaKiSNePhh2LkTxo6FY46pt+n448VvMmkS7CTLOIzbAUYQGBzDNgudeSbwvuUojoIgID+f\nIYeJIFi6tL4geOIJMXH//vfiJG0RreVxfflykSjLl4sTJC+vbp9DD5VH7rPOal1XtcREcjoXQiVs\n21xLuH+a9v/ihBNCEAKFhfDPf8rynXc2Gebr8ch7EenGYdwOMILA4AjV1fKkDXDGxEL423diWxk/\nPnKDSEuTa5aUMGRgNZBYz09QUSERnBBQFrshWsOMGWLjWr5cPKsN2WsvMa6fd55klrWRHmllIgi2\nhD+j7GWrV+BZZ4Ww84wZIgyOPFJeTWCXzCgi3WgE7QAjCAyOMG+e3DMHDYJBGz8WE8oRR4TwOBpG\nlBKtYMsWhvYuArLr5RK88opEyxx4oDSLaZKXXqpfFC8jAw44oO41fLiYTxo2GGgDORmVkA/btofX\nLLRunURBpaSIRtAiO3fCQw/J8p13NrubLQi8eKA0PzwDNUQNIwgMjmAnkZ1xBvBRFMxCNpYgGNJ9\nG5DN0qV1TWoefVR2mT69mSTn8nK43mqed9dd8Mc/iv3IoYzo7lm1sAa27UzA5wuLbAHqzEInnlgX\n/t8s998vjuJjjxUB1wz1NAJjGop5TEKZIeyUlARkE5+hpfwmRE8QAHvEbSUzU+LmN2+W1pHffSdm\n/LPPbubYBx8Ub/KwYXDDDZIy7GBZjM6ZKWRQQG2tatIC1VZCNgtt21YXQnXHHS3uavsIvHjQJcY0\nFOsYQWAIO7NnS6To2LEwWC2XO2+PHnJDjTSWIFA78xkyRFYtXVqnDVx0UTN9ArZurWudOWNGZBKm\nAnIJwhVCunatCLzUVIn2aZH77pOn+5NOkvoSLZCYCCkJldSSQGlB5Du/GcKLEQSGsOLzSeQhSOi8\n3yx0zDHRaRpsZReTny9JUkigz8svi+nlssuaOe7mm0W1OflkOOqoiAzViTITtmZ28snNCDybTZtg\n5kxZDqIN2Hg6VQBQtNOBsq6GiGIEgSGsfPCBVPnce2845RSi6x+A+iGklkbwyCMS1XTSSc20Q/jp\nJ/jvfyW54P77IzVSR5LKQjYL3X23FIY6/XTxnodAelIlAN6C2t0YocENGEFgCCt2wMn06ZBQVSbJ\nVkpJE5NoYAuCnTv9gsCuG9RkyKjW8H//J+9XXFHXiSsShNk0tGYN/PijRNEed1wzO+3cCRdeKNqA\nUlJHO0TSk8UkVFSod3+whqjidKvKDKXUa0qpX5RSK5RSo5VSXZVSnyilVlvvmU6OwRA5li2Tygqp\nqXJvYf58ecocMUKK9USDAI1g8OC61fvv34zF55134LPPpOaCXXI5UgRoBOGoN2SbhSZNkoziemgt\n2dADB4r206mT+EIOOCD04aaISaioaPfHaoguTmsEDwMfaq0HAsOAFcANwFyt9X7AXOuzoR1gF9Sc\nOlXuo1E3C0E9QZCWVlfm6E9/asJlUVVVlzNw++2tywwOB2HWCJo1C61cKTW1p06VRIojj5RmDVdf\n3arzp3exBIE3Cr4fQ1hxTBAopdKBccAsAK11lda6EJgEzLZ2mw2c4tQYDJEjPx/+9z9ZtlsSu00Q\ngPhBzztPUgIa8fjj4uDo378FL7KDhNFHsHKluDrS0wNKBdXUiOln6FDJ+MvOlhCvuXPblBHt6SIm\noaISU4I61nEyoWwfYAfwtFJqGPADcBWQo7XeYu2zFchxcAyGCPHUU1Ky4fjjrXvKhg3Sz9Hu9BIt\nGgiCP/xBXo3YtavOPv7AA5FpnNOQMEYNvfKKvJ9yijTRAaTNpF1R9IILZNmOqmoD6R4jCNoLTgqC\nBGAEMF1rvVAp9TANzEBaa62UatLTpJSaBkwDyMnJITc3t8WLlZSUBN2nvRLtuVdXKx588FCgM0ce\n+RO5uQX0fO89BgD5Q4ey7MsvHb1+S/OPKy9nHFC7fTsLWvgb9Xr1VfoVFFAwfDg/dekizdojTMra\ntfSwNIL16yvJzf066DHNzf3ppw8CujBw4BJyc3eRsXgxw+6/H+LiWHrPPewaNarllm0hUFERD/Rn\n666aqHz/ov29jzZhnb/W2pEX0APIC/h8OPAesBLoaa3rCawMdq6RI0fqYMybNy/oPu2VaM/9+ee1\nBq0HDdLa57NWnnGGrJw50/Hrtzh/n0/rpCQZS2lp8/tdcIHs89hjYR9fyGzcqMvprEHrhASta2uD\nH9LU3Jcvl6lkZmpdWam13rVL6169ZOWtt4ZtuA9esVqD1lfu8WrYztkaov29jzahzB/4Xodwv3bM\nR6C13gpsUErZxscJwM/A28BUa91U4C2nxmBwHq3rQkavuirAAfv99/LeTPXKiGEXngO/eahJVq6U\n992oHrrbpKeTRCXpFFJTI+Uw2oJtFjr1VOiUqMXfsXGjNFy+5ZbwDbermISKKjoH2dPgdpwuOjcd\neF4p1Qn4DTgfcVC/opS6EFgHhFIY1+BSvvmmrmaP3/bu80mmKkDv3lEbm5+sLLkR5udLpltTuEEQ\npKZCfDw5tdsoIoNt21pvwt+0qS6ze/Jk4LnnJHwoNVWWE8L3k0/PknMVVbWUsmyIBRwVBFrrxcBB\nTWya4OR1DZHD1gYuuSSgzWF+vmRtZWUFqWsQIYJpBHb39tRUKSwXLZSC9HR67NrKKgawbZuU8Q4V\nreHii6WVwIknwsR918LpV8jGf/0L+vUL63A92Z0AKKp2wf/YsFuYzGJDm9m2DebMkXpsl18esGHj\nRnmP5k01kIDs4iaxtYH+/aNTDymQ3cgl+O9/pcRHZiY8NbMGdd65UlL6tNPg/PPDPtT0bpYgqAlW\n29rgdowgMLSZ336TNo8jRkCvXgEbbLNQvZVRJJhG4AazkE0bcwnWrYNrrpHlRx+FnrPvlSbFPXtK\nbK8DAi49R9KVvb4INhsyOIJpTGNoM16vvDdq7u5WjSAWBEGARhBqmQmtpaSH/fB/9r7fwnm3y8bZ\ns3crV6AlbEFQpD3yRBCJUt0GRzCCwNBmbEFgNynxY2sERhC0njZoBE88IcnB2dmSHK1OvUZuzFdf\n7Wixv/QM0TKKSJdubpFsQ2oIK8Y0ZGgzQQWBMQ21nlb6CDZvTuK662T58ceh+84V8NVXclNuoedw\nOEhKgk5UUkVnKnaaLmWxjBEEhjbTrCCIJdNQba3Ua4bIlpxujvT0kMtM+Hxw330DKS2VUNEzzkA8\nxiArIvCE7okrAaBoW4Xj1zI4hxEEhjbTLkxDeXlSdXTPPd1h2miFRvDII7BkSQY9elitN6urpbQ0\nWHXAnSc9XjQB747KiFzP4AxGEBjajF2HvlmNIBZMQ24yC0EjH4FupudLcTHcdJMsP/mk5Q9+913Y\nvl2SDw45JDLDTRRBULTD9C2OZYwgMLQZWyNITw9YWVwsr+TkJsKJokRA3+JGd1a3CYKMDJKoxJNY\nRnV182UmFi6UPvMDBng5+WRrpW0WuuCCiOVDpHcqB6Aovzoi1zM4gxEEhjbTpGko0FEc7eQsm+Rk\nSXuuqoLSBk5NFwoCgJxOIgGaMw99bRUmHTzY+ids3gzvvy8lJM491+lR+knvbDWw32X6FscyRhAY\n2kyTgsBtjmKb5sxDbhMElnqVkyBZ0M0Jgm++kfdBg6x/wuzZ4j0++WTo3t3pUfrxJIkmUFTgi9g1\nDeHHCAJDm2lRIzCCoG1YGkGPFhzGWgcKgiJZYZuFIuQktjEN7NsHRhAY2kyLGoFbHMU2TQkCrxe2\nbJEWXs1VJY00tkbgkyZ+TQmCVaukTl7PnpCTUwkLFkgI7J57RrwtaHqq9C22vwuG2CRoZrFSKg5p\nPL8HUA4s01pvd3pgBvfTZNRQLGkEq1bJ+377uac8gu0jqBaB2lSZCVsbOPRQyw0za5asmDo14vNI\n7yK+AdPAPrZpVhAopfYFrgcmAquR/sNJQH+lVBnwJDBba22Mgx2UJqOG3JZVbNOUIHCbWQj8UjWn\ncj3QtEZgO4pHj4b4khJ49VVZccEFkRhhPdLT5OdfVOoSQWpoEy1pBHcBjwOXWC3P/CilugNTgHOB\n2c4Nz+BWfD6JEgXpT+8nlpzFbhQEiYmQmkpOafOmoUBB0O2NeVLnZ/x42HffyI3Twn4IKCo1Zcti\nmWb/e1rrs1vYth14KNjJlVJ5QDFQC9RorQ9SSnUFXgb6AHnAWVrrNjblM0SL0lLxUVpNteqIJdOQ\nGwUBSJmJ0qbLTBQXw7JlEiU6ciRUX/y+bIiwk9jGk24VnitLjMr1DeEhZGexUqqfUuo5pdQcpdTo\nVlzjSK31gVpru1PZDcBcrfV+wFzrsyHGaNJRXFUld674eOjRIyrjapZYEgQtlJn47jvRxg48EJJ/\nXYbnl1/ksfz006MwUEjPlFuI6Vsc2zQrCJRSSQ1W3QncCFyNmIzayiTqzEmzgVN241yGKNGkINgi\n5gx69HCP89XGzi62u5T5fHXOYjcUmwukhTITgWYhv5P47LOj1hI0PUv+z95KIwhimZY0gneUUucF\nfK5GzDm9EVNPKGjgY6XUD0qpada6HK21dcdgK5DTivEaXEKTEUNuDR2FxhrBxo1iW+/WTXo7uomM\nDJKpIC25mqoq6UFs4xcEo2qlGT1EzSwEkN7VNLBvD7Tk4TkOuEwp9SFwN3AtcCWQDJwT4vkP01pv\nspzLnyilfgncqLXWSqkmM1EswTENICcnh9zc3BYvVFJSEnSf9ko05v7dd5nAMHy+XeTmLgGg27x5\nHADs6NSJ5REcTyjz75SfzxigcvNmvs7NJfP77xkGFPbowWKXfW/2r6wkB8hMKqK4PJt33vmWvfcu\nQ2tYsGAskEjn/HchP5/yrCwWFhdDlObQaetKYCJFVSkR/w525N88hHn+WusWX0A68ADwIrBvsP1b\nOM/tiDBZCfS01vUEVgY7duTIkToY8+bNC7pPeyUac3/1Va1B69NOC1g5Y4asnD49omMJaf4VFTK2\nhAStfT6tH3lEPl94oePjazWXXqo16LF9N2vQOjdXVq9aJUPOydHa99k8rUEXDB0a1aH6Fn6r46jR\noHVVVWSv3ZF/81qHNn/gex3C/bklH8EhSqnXEH/AM8DNwN+VUv9USgUtK6mUSlVKpdnLwDHAMuBt\nYKq121TgrdaJLoMbiKmsYpDs4bQ0qKmRwbvVUQz+mMweyWJ/sx3Ggf4BtX4dAJURrCvUFCo1BQ/y\nZTDZxbFLS6ahJ4HjgS7A01rrscBkpdQRSPhnsFz2HOANJRUoE4AXtNYfKqW+A15RSl0IrAPO2s05\nGKJATNUZssnOlvjL/Hx3CwJ/BdJdQGNBcOihwHpJOKuIsiAgNZV0iigkE6+3zidviC1aEgQ1iHM4\nFfB3ndBafw58HuzEWuvfkNIUDdfvBCa0dqAGdxFTlUdtsrNh7Vr3CwK73lC8OLbtMhN2aYnRo4H/\niSCozIlyrEVqKulsBuoCCAyxR0uCYApwCSIEzmthP0MzbNkitWByctxTmj9ctFhnyI2mIaiLHFq/\nXl4JCdC3b3TH1BS2RqDrQkhLSmDJEonKPegg4O8u0QhSUkhHvgxuEAQ+n2hOGzdKYb6dO+Xdfo0d\nC9dfH+1Ruo+WBMFqrfWfWzpYKaUsh4ShAStWwNChYpLOyID994dBA33sv0ch+3s2cciENLJG9on2\nMNtMozpDPp80RwF3awRQ92jdt6+UdHAbTVQgtRPJRoyQHju2aSjqGkFycp0gKPARrYLGXi8884z0\ncV6zpvn93nkHpk+3/oYGPy0JgnlKqTnAW1rr9fZKpVQn4DDE0TsPcSQbGvDeOz5qauKIUz4KC+P4\n+mv4+us4oCvQlWyVz4YdpSRlpUZ7qG2ikWkoP18yi7t2jVpyU1BsQfDll/LuRrMQ1GkEVRsAEQT1\nEsm0hnXiLI66RhAXhye+FGrtdpWRTSxbtQoefRSeflq0JpCK4qNGyVcxK0veu3YVTSA/X15uqTru\nFoLlEVwAvKiU2gcoRKqPxgMfAw9prRc5P8TYZP5Lm4C9eEZP5Wg+4WcGsYJB/JxyEM+UnUm+zmbr\n8vX0GddOBIHbHcVQ58n88Ud5d6sgsKOGytcCIgjq+Qd27pRkuIwMalOj//1JTywTQbCjikgJghUr\n4IYbhrBwYd268ePhyivhpJPE6leP8nIeuWEX+expBEETtFR0rgKYCcxUSiUC2UC51rqwuWMMgq9W\ns2CJ/JiPuKg/PSb/kR59+nAB+FBTAAAgAElEQVRUr17QuTPzk1ezrGI/vNvKozzSttNIELjdUQx1\nGkG11WjdrYLA1gjKmhEEllnILXez9E4VUAHenZFpYF9QAMcdB+vXZ5GUBH/4g5h7hg5t5oAtW+CU\nU8jecRewJ/kbK2BEwwo6HZuQasdqrauBLUF3NACw7OnvKKwdRe+4Dez96F8khj0AT6L1w9leEaUR\n7j7NagRudRRDnSCwcasgsDSClKItdOkiJo8dO6Qaxj77AG+5TBDYDewj0LdYa7joIpGFAwd6WbDA\n0+jfWo9Fi6SP88aNZCNRWPnryxDjhsHGtKp0gPkPielh3JCCRkIAwJNUCdiqdGxiR4j4ncWxYBqK\nFUFg1/YuLyene10sxujRVvSZ2zQCu29xBATBE0/A66/LA8gtt/zcshCYMwcOO0y01bFjye4iv7v8\nTbH7AOYURhCEm19+Yf7yrgCMm7pPk7vYP5xIqdJO0KxpKFY0gowMecR2I0rVmYeyavyrR9vF390m\nCFLke+x0A/slS+Caa2T5qadgjz2auaFrDXfdBWecAWVl0sJz7lyyM+Vvmb+lpunjOjBBBYFSarpS\nymXlGd2LfvAhPucIAMadkNbkPp4UKd7q3RWbX0it6wSBvztZrGkEAwa4O7nDdhhnVvpXHXqotWBF\nDLlFEHhSne9bXFoKv/89VFaKaej3v29ip+pqCa+aPBluuUX+v//4h4QUde5MtkcEVv520123IaH4\nCHKA75RSPwL/BT4yuQPNkJ/Pqme+Yjs55GTXsN9+Tf95PV3ki+gtjM0vpN2dLCUlIDojFpzFXbvW\nLbvVLGRjawRpZUAX4uPh4IOtbYEaQXX0tUp/A/ti5wwMV14Jv/wi+TgPP2yt9Plg8WL47DOYOxfm\nz6+LIe3SBV54QUKILLIzLI0gH0MDggoCrfXNSqlbkKJx5wOPKqVeAWZprX91eoAxxRNPML/qEADG\nHZnQ7AOnJ03kaKwW6WqxzpCbTUOJiXKDLSx0vyCwk8pSS4DuDB0qrgOgThD07t1y9lSESPdY32eH\nGti/8AL897+QlAQvv2wlg82Zw5gLL2yczjxgABx1lIQR7b9/vU3+lhSFLmua5AJCjRrSSqmtSCOZ\nGiATeE0p9YnW+i9ODjBmqKyERx9lPvcDMG5c87v6+7wWu9g00QKNBEFxsaxMSnJfk5eGZGfHhiCw\nNILB3bcBfTn6aGt9ZaUUH4qPh5493SEI7Ab2ZeFvYL9mDVx6qSw/9BAMGYKEUF14IZ2KiuTBY8IE\neR11VIsaaXZ30VjyizqFfZyxTtD/nFLqKqTWUD7wH+A6rXW1UioOWA0YQQDw4ouwbRvzEydAdcuC\nwO7z6i2JzSeTZiOGevVyt90d5E6ybl2AncWlWH/c0wauYN680XX+gUCnvEvagaZn2A3sw3uDLSkR\nX0Bxsfh9p9k9Dm+5BYqK2HXwwXRduDDk71x2Tyknkl9qQkcbEooI7wqcprVeF7hSa+1TSp3ozLBi\nDK1hxgzWsTfrq/cgIwMGD25+d0+W/Nm95eF/gooEMZlVbPPii5KZu8ce0R5Jy1gagSoqZPz4gPUu\nixgCSMsQgVRc2QmfD+LC4CqorIRTTpEk8H32gX//27rfL1kiH+LjWXPFFYxqxYNH1p4iAPLLU9Ha\n/c8skSSUf9kHwC77g1LKo5Q6BEBrvcKpgcUUn34KS5cyP/1kAA4/vOUfgydLnpy8FbGposZkVrFN\n587uFwIQYG9pYAN3WcQQQHxaCl0oRqMoLt7989XUwJQp4v/NyYGPP7bkotZw9dXiJL7iCsp6927V\neZO6e+hCMdW+hLCMsz0RiiB4HCgJ+FxirTPYzJgBwPz9LgDgiCNa3t3TTZLMvJWRLdAVLmIyqzjW\nsDSCep3rwZUagd2cBnY/AEJrMQG9/rr8CT76CPr1sza+8QbMmyfRX7fd1vqTZ2TUZRebyKF6hCII\n6pWa1lr7CNHJ3CFYvhw+/BBSUphfIMVOWvIPAHhypDpnUXVs1sKNadNQrNCcRhAYMeQWwtSTQGu4\n9loJ+09Ohvfeg2F2a6uKCtkIcOed9UOBQyUz0wiCZghFEPymlLpSKZVova4Cfgv1AkqpeKXUIqXU\nu9bnfZRSC5VSa5RSL1tlrWOTXbvgAtECtpx5Jat+jSc1FYYPb/mw9J4iALw17UQQxJJpKFaIUY1g\ndwTBPfeIcp2YKA//Y8YEbHzoIekuN3hwgNe4lRhB0CyhCIJLgTHAJmAjcAjQmv/EVUCgL+E+4EGt\ndT+gALiwFedyD9u2wZFHwrffQp8+LDj0OkA6IDUqgdsAzx5dAPD6ujg9SkdoMWrIEB6CaQTtTBA8\n/jj89a/iwH3uOTg2sCP6li1SMgLgwQeD/8CawwiCZgkqCLTW27XWk7XW3bXWOVrrKVrr7aGcXCnV\nCzgBCTtFSSf7o4DXrF1mA6e0behRZMMGsf8sWSLx6AsWMH+ZVV8oiFkIILV7KgofZaRSU1nr8GDD\nj9EIIkBTGoHWdYJgr70iP6bmSEnBg3wp2iIIvvsOrrhClp94As46q8EON90k6ewnnwwTJ7Z9nF26\nkK0k7mXH1tj73TlJKHkESchT+wEE1G7VWl8QwvkfQvIM7Io0WUCh1tousrMRiK27x6+/SvLKunVi\nwPz4Y+jenfnzZXMogkDFx+GhkCIyKN5aQmbv9OAHuYh6gqC6GrZvlzCpHj2iOq52RVMaQX6+vyFN\n/bTuKLObGsEHH9SVl25k9fnuO+lBmZgI//zn7o1TKbKTSqAc6UlA9Jv6uIVQdKz/Ab8AxwJ3AOdQ\n39TTJFaOwXat9Q9KqfGtHZhSahqWCSonJ4fc3NwW9y8pKQm6z+6SkpfHsGuvpfPOnXj3358ld95J\nzc8/4/1mFUuXHkZioo/y8i/IzQ1eQygtrh9FvgzmvbOAroN3z0QUibkHkpc3BMgiL28JX7++gtFa\nU5mVxddffBGxMQQS6flHgoSiIg4DqvPz+dKaW5eVKzkIKOnale+tdW6Ye+ratX5BsGjRr+TmbmjV\n8R98IN+nnj2Xk5u7o26D1gyfPp10YP1pp/Hbxo112idtm7unsxfK4eclm8nN3dSqY91GWP/3WusW\nX8Ai632J9Z4IfBPCcfcgT/x5SGmKMuB5JEM5wdpnNFLErsVzjRw5Ugdj3rx5QffZLX74QeusLK1B\n6/HjtfZ6/ZveektWjxsX+ukGd16pQeuf5qze7aE5PvcGjB0r850/X2v95Zfy4eCDIzqGQCI9/4hQ\nVSV/17g4rX0+Wff667LuxBP9u7li7r/9pu/krxq0vumm1h3q82ndvbtM69dfG2x8913Z0K2b1kVF\njY5ty9xf2/c6DVqfcsTOVh/rNkKZP/C9DnJ/1VqH5Cy2yxsWKqUGA+lA0I7ZWusbtda9tNZ9gMnA\nZ1rrc5CG92dYu00F3gphDNFlzRo4+mjJSD3+eHj//YD6y7TKLGTjSZRa6rHYpayeacg4ip0hMVGq\nzPl8dRU13Rg6CrtlGlq/XiyLWVlW9zUbreHWW2X5hhvCZgrLzhDfgHEW1ycUQfCU1Y/gZuBt4Gck\n8qetXA/8n1JqDeIzmLUb53KeggI48UQJFT3+eIlrS06ut0ubBEFnqTPv3VEZZE/3US9qyDiKnaOh\nn8CNEUOwW3kE334r76NGNSj58PbbUl+iR4+6qnNhwF+BtMAddZrcQos+AquwnFdrXQDMB/q25SJa\n61wg11r+DRjVlvNEnOpqCWFYuVKKlb30EnSqn/ZQXCzf1/j4gA5SIeCJ4S5lRiOIEBkZsHmzRA71\n6uXK8hJA/aihQg2EXsTnu+/kfVTgHcHnq8scvvFGq+50eOjWXcaW743d9CUnaFEj0JJF3HGri159\ntdQR6t4d3nmnnjnI5quvoLYWRo6UXhih4kmRwKmiGOtS1qg7mckqdg5bI7BDSN2qEcTFkd7JamDf\nymZLtkZQrxjsG2/ATz9JTai2Jo81Q9ceIgB2lXam1kSQ+gklauhTpdS1wMtAqb1Sa72r+UPaAY8+\nCjNnSpGyN9/022W1FpfB55+LSejTT2X3YPWFGmJ3dfJGoOF3OCkrkwe25GQxYxvTkIPYuQRuNw1h\n9eGusrvuhWZ2qa2F77+XZb8g8Png9ttl+a9/lR4XYSQhK51MdlGgu1JYKL4JQ2iCwO4OekXAOk0b\nzUQxwUcfwVVXyfKsWTB6NHPnSsPs+fOlL0gg2dlw9tmtu4THUi5irUuZKTgXQQKTyioqJJs9IUEa\n0riM9JRqKGqdj2DFCskT69NHlG4AXnsNli2ThLkLHSg6YBWeK6Ar+flGENiE0qpyn2D7tCtWrBC/\ngM8nTyTnnMOyZfC739W1h+3WTRzDRxwh74MHt75HiMfS+r0xVg7XFgTp6Yh6ZExDzhHoLHZhQ5pA\n0lMtU2crGtg38g/U1tbXBjo7UJ3XKjOxmv7k57u/UV2kCCWz+Lym1mutnw3/cKKM1ysRQl6vtES6\n4w5qaqSuXHU1TJ4sPqwBA3a/qYXHauYRa13K7Cc+jweJwauqkvaUYXToGSwCNQK3Ooot7D7cRSXx\nITd9aeQfePlleRDr3RvOP9+ZgZp6Q00Simko0I2TBEwAfgTanyB45x347TeJEJo9G+LiePhBeXLp\n1QuefDJ8mf2eriIAisoSw3PCCFHPNGT8A84SqBG42D8A0CmtM0mUU1GbTFmZpEAEIzB0lJoa+Nvf\nZMUttzSKzgsbGRlkI5nPRhDUEYppaHrgZ6VUBvCSYyOKJl9+Ke9TpkBKCmvWwM03y6onnghveZf0\nbLtLWTsQBMY/4AyBGoHLBYGdS1BBMl5vcEFQUSE1G+PiYMQI4IUXYNUq6NsXzmvSCBEejEbQJG3p\nLloKtE+/wVdfyfvYsfh8UgSrogLOOQdOOCG8l/LYgiDGupTVEwQbrJoybqqE2Z6IIY2gtdnFixeL\nEjBoEHTpXA133CEbbr3VCkdzCCMImiQUH8E7SJQQiOAYBLzi5KCigtcLS5fKl/Cgg/j3vyVEtFs3\n6YkRbuwuZd7q5CB7ugsjCCJIoEawc6cstxNBUM8s9OabUtV3v/3kqctJ0tPrBMGO1iW/tWdC8RE8\nELBcA6zTWm9sbueYZeFCiRQ6+GA25CdznfSZ4dFH69LSw4mnR2x2KasXNbTOmIYcJVAj2GWl7bhV\nELSyzEQjQQCSPNbWpjOhkpBAdnKplKLeWoPU0IwSNTXw7rsSop6UJH7JKAVdhPJXXw9s0VpXACil\nkpVSfbTWeY6OLNJYZiE9egyXXiqlIyZNgjPPdOZynp5iRC2qja0uZfWihoxG4Cy2RlBQEBOmodY0\np/GHjg6vhhvelw+TJjk0uPpke6pFEGyvJSqCYONG+M9/5LUpoBR2QoL4SnY3JLENhOIjeBUITH+t\ntda1LyxB8ILv97z/vjyMzZzp3P+kS8+0ui5l1Tr4AS7BmIYiiK0R5OWJsyozs8kyJ66gFaahggLx\nCyclweCiL8X0NXCgmIYiQHam5DxE3Efw2Wci7Hr3lgipTZugf39ZTkuTWmZ33x3hQQmhaAQJWusq\n+4PWuiqmG843RW0tfP012+nGVf87CJAm2nvs4dwlVafEui5l28vJ3DM2TER+QZCmTdSQ09gaQaVV\nodat2gDUMw0Fy5a3y0oMHw6J71tV6E8+2cHB1Se7qzx4RbQC6RdfSJtNrcUPeeaZcMklMH68PG2O\nHAknnSRhioMGwamnRm5shKYR7FBK+f9LSqlJQPvyty9fDsXFXJkyi50F8Uyc6Fw+SyCeOKkz791c\n4vzFwoRfEOiiumSyUILGDa0nJaV+FrGbBUErNAK/f+BgLeWmIaKCIKNbInHUUliS6K8W4DizZokQ\nmDJFNOmXXoIjj6wzOZxwAtx7ryyfe67E1kaQUATBpcBNSqn1Sqn1SD+BS5wdVoT58kve4mReLjuJ\nlBSpKRQJM50noQwA79Yy5y8WJvyCoHybLBizkHMoVacVgLsFQSucxX7/wJ6bJIEzOxsOPdThAdYR\n1zWDLCQKa1ckSmeWl8OcObJ8662Qk9P0ftddJ0KgtBROPpkn7vdy5JHwVgRadwUVBFrrX7XWhyJh\no4O01mO01mucH1rkKMxdzGU8DsA99zTolOQg6YnlABRti50uZf6oodLNsmDMQs5i+wnA3YIgRI1A\nawnQAzh4+3uycMIJka2fZBWegwj5Cd57T6JPRo5subiRUvIUesghsG4dH927mNzcyAiroIJAKXW3\nUipDa12itS5RSmUqpe5yfmiR49oPjmILezB6aClXXBF8/3Dh6Wy1q4yhLmX+qKFCK4rFaATOEisa\nQYhRQ5s2SfXejAzo9+VsWRlBsxBQL6lsx44IXO+FF+R9ypTg+yYlwRtv4NujF/N3HQDA+COcDyYJ\nxTT0O611of3B6lZ2fLCDlFJJSqlvlVI/KaWWK6X+Zq3fRym1UCm1Rin1crQdz5++sotZxb+nE5XM\nej4pog8mniQxUHrzq4Ls6R78pqFdebJgBIGzBGoEbutVHEiIpiG/f+DAStTCb6Sm0DHHRGCAAUQy\nu7igQDQCpaRqZSj07Mmyf37ELrLYi/X0eftfzo6R0ARBvFLKXwdBKZUMhFIXoRI4Sms9DDgQOE4p\ndSjS7/hBrXU/oABwoOh4aJSUwMXTZSq37fs8+w+ObCVQu0uZN0a6lNXrTrb9V1kwpiFniSGNIJSo\nIb9/oMsK+UJNmNC61n7hIJKmoddfl6CKI49sVRji5zsGAXBE529Qww90anR+QhEEzwNzlVIXKqUu\nBD4hhMqjWrDDYRKtlwaOAl6z1s8GTmn1qMPEzTdD3vZUDmQR103eEPHre1KtLmWtbO8XLcrLJdI2\nKQk6bc6TlUYjcBZbI0hIkEbubiVEH4G/9LTtH4i0WQgiqxE8/7y8t7J0Rm6uvI+//8TWtz9sA6FU\nH71PKfUTMNFadafW+qNQTq6Uigd+APoBjwG/AoVaa/sReCPQZA1jpdQ0YBpATk4OufZfphlKSkqC\n7hPIsmUe/vWv4cRTy3+5gJ/Tp1DQiuPDgfYVAJCXt6tVY29Ia+feVnbt6gSMITm5ivLVq0kGFm7e\nTHmE/24NidT8o8G+JSXsBZRnZ7NwwYJG290y96TNm+ljCYL8/Cpyc79qtI/PBwsXHgYkMGLxvwH4\nOiuLyjaOv61z96xd6xcEixdvJDfXmdiXTjt2MDo3F52YyFfdu1MT4li1hrlzxwCdSOq6lNzc8ib3\nC+v/XmvdqhdwGPBYK4/JAOZZx64JWL8XsCzY8SNHjtTBmDdvXtB9bMrLtR44UGvQ+sa4e7VWSuvC\nwpCPDxcPnvCJBq2vHPnFbp2nNXPfHVaulL/Zfvv5tE5MlA+lpRG5dktEav5R4bbb5O88blyTm10z\n961bdRlJGrTu3LnpXVaskKns1a1MFkL4XbdEm+e+fLmezbkatD7nnN0aQss88IDM87TTWnXYsmVy\n2B57aO3zNb9fKPMHvtch3KNDKkOtlBqulPqHUioPuBP4pZXCptASBKOBDKWUrYn0AjY1e6BDPPgg\n/PILDNi7jFt9t0mvyUCnXITwZMif31vSlmrgkccfMZRcIy3bsrJMZzKnsX0EbvYPAKSmkkQFCVRT\nWVmXDB2I31Hc5WdZiIZZCCJnGmpNtFAAfrPQ+MiVHWr2DqSU6q+Uuk0p9QvwCLABUFrrI7XWjwQ7\nsVKqm9XExnYwHw2sQATCGdZuU4EIpEvU5yWrrc4/j3qfJCph7NhIDwEAT6bVrrIsNtpV+iOGrPwH\n4x+IACedJHeECy6I9khaJjkZBS06jP35A/kfyoIrBIFDoZm//AI//ihFuVrZzOTzz+U9Aq4BPy35\nCH4BFgAnaiuBTCl1TSvO3ROYbfkJ4oBXtNbvKqV+Bl6ychEWAbPaNvS2sXGjZG+npsLEfEsijBkT\nySH48WRJ5UNveWyUbvILgngrBsBEDDnPvvvCvHnRHkVw4uMhKYn0iiJ2kk1RkfTysKmshFetUpVH\nFL8jDxHDhkVnrElJZHcqhioHexLY2sDpp0t0RYho7T5BcBowGZinlPoQaU8Z8l9Ma70EGN7E+t+A\nUa0cZ9j44AN5nzBB0/nr+fIhShpBencJXS2KkS5l9eoMgdEIDPVJSSG9ounIoZdfluStA3O2cMi2\nhXDyFVEpt2yTnVED2x0yDWnd5mihX36B7dslQKx/fwfG1gzNmoa01m9qrScDAxFzztVAd6XU40qp\nCGeAhA9bEBx/0Hb5ZubkRK6mRAM83eVJwVsV+hNDNPGXl6ixumUZQWAIpJkQUq3hX1ZO1JWJj8vT\n5EknRXx4gaR1TSSRKkrL4ihvOiin7Xz7rdRQ6tlTzHqtIFAbiKScDKXWUKnW+gWt9UmIc3cRUngu\n5qiqgk8+keXfpVjawJgxUXsy8berjJEuZX6NoNLKyzemIUMgzZSZ+Ppr+OEHyO5ay9kb/yEJZK28\nQYYblZlBN+R7bHcBDRu2NjB5cqtrKNmCINJ/nlaFq2itC7TWT2mtJzg1ICf54gvJJj7gANh7pSUR\nomQWAvDsIRmV3hjpUuaPGirdKgtGIzAE0kyZCVsbmHbQIgnOOO446Bxlc6hTkUM1NWIHg1ZHC2ld\nFzEUSf8AtFIQxDp+s9Dx+DuSRctRDJDaQ7qUlZJKbW3UhhEyfo3AazWkMYLAEEgTZSY2boTXXpMH\nY7vCLyeeGKUBBuCUIJg7V4z8/ftLtdEAFixouZLo6tVSkK97d2nYFkk6lCB432qN+rvDiqUZTefO\nMGJE1MYTl5LkV6VjoQKpXxAUWeU49mwyKdzQUWnCR/DEE1KW5PTToVfeF7Iyir85P07VG3rcEnZT\nptQzOX/4IYwbJ/X1mnvoi5Z/ADqQIFi3Dn7+WVqDjuVLWXnQQdFVUZWq61K2pTR64wgRvyDwFUhs\nYCvC4gwdgAamoYoKePJJ2XTl5TXiQFUK+vWL4iAtnNAIFiyQLjIpKXDxxfU22eaxH36Axx5r+vBo\nhI3adBhBYJuFjj4aOn1rPZlE0Sxk44kXAeDdGjuCIJ0iYxYyNKaBRvDSS3KTHTECxuT8Kvbz3r0h\nOTnKAyX8gkBr6TAGcO219SqN/vqraARx1t325pulL0PDw6PlH4AOJAj8ZqHfIXY8gMMPj9p4bOws\nXW8MdCnzawR4TcSQoTEpKfWihuyn4OnTQa1aKR9a6tAVScItCF57TVKnc3JEEATw5JNyoz/3XJg0\nSZqVXX11/cN/+02EQ3a29K6PNB1CEFRU1N37f3dogcT5JiZKjfAo4+kkvoGi7e4XBP6oIbxGIzA0\nJkAjmDcPFi2SG9vkycBKlwmCcPoIqqrgxhtl+fbbxf5sUVEB//2vLF92mQjH1FSRG/bDKdSZhcaN\nq9McIkmHEAQLFkBZmWS077n8Y6mHe/jhkW+I0QTpySIIvPnVUR5JcOppBEYQGBoSIAjsm+sll1iu\nJLcJgnBqBI8/LvafgQPhoovqbXr1VclTGDECRo2S2oF/+5tsu+IKuS9BdM1C0EEEQT2z0IdWwavj\njovaeALxJMdGl7J63ckoNqYhQ2MCnMVghYxeZn1or4KgsBDuuEOW77tPGggFYAcRXXZZXSTQlVfC\n0KGQlwd33inropVIZtOhBMHxv9PuEwR2l7ICdycSVFSIr6+zqqQzVUYjMDQmQCMAOOOMgAhjtwmC\ncJmG7r1XkgPGjWtUNmPxYsmqTk+Hs8+uW5+YKH4DpeCBB6Sl8fr10LWrVMSPBu1eEPz6K6xaJf+M\n0alLJGNjjz2i9xdvgCdNyuB6ixwqhxsm/BFDylowgsDQkAaC4MorrYWCAqnrlZrqntyTzEyykNoS\nO3aIxttq1q+Hhx6S5QceaBT8b2sDf/yjTD2QQw+FadPk4cruaX/44dHxD0AHEAR22Ogxx0DCpwHa\nQBQrHwbi8ch7kdcd42mOuhyCQlloRSNuQwchJYU0Srik7ydcdhmMHm2tt7WB/v1d87sjLY2UuEpS\nKKWqSkrPtJqbb5b62pMnw8EH19tUVATPPSfLl17a9OH33CNZxPa1o1l+qcMIguOPx3VmIYD0DPlh\nuL1LWb2IoZyc6NeKMbgP67H3if0fZubMgHu+28xCIIPbHfPQokVyp09MhL//vdHmZ58VR/BRRzVf\nLiIzE2bMqPscLUcxhNC8PpYpL4fPPpPl4w4rgYu/EN1r4sToDiwAT9fY6FJmIoYMQbHtH3YojM0v\nVmdbNwkCEIfxrnzW05v8/FZUo/f54P/+T+xJf/oT9O1bb7PW9Z3ELTFlikQ1FhZGr08POCgIlFJ7\nAc8COYAGntJaP6yU6gq8DPQB8oCztNYFTowhN1ecnCNGQI/lc8UgN2aMiGKX4OlqdylLjPJIWsYk\nkxmCYvevLm2QJe9GjQDaHjn017/KzaVrVzEPNeDzz2HFCmlHMGlSy6dSSuoxRRsn7RE1wJ+11oOA\nQ4ErlFKDgBuAuVrr/YC51mdHcLtZCMDTTUws3gp3t6s0GoEhKM1pBG4VBG0xDT3/vEQKxcfDK6+I\nMGiArQ1cfLFYjmIBxwSB1nqL1vpHa7kYaVy/JzAJmG3tNhs4xZnrS1gWuDNs1MbuUlZU5YL6Ky1g\n6gwZgtKURlBbC2vWyHIkey+GQms1gm+/hQsvlOWHHoIJjduybNkCr78ucqJB3TlXExEPpVKqD9K/\neCGQo7XeYm3aipiOws6uXWIJ6toVRmWskuyN7OxGNcKjTXqP2OhSZkxDhqDYGkGgIMjLkxIMe+7p\nikz+erRGEGzaBKecApWV6IunceKHV5CYSKNXr15y3zn55Nj6mTjuLFZKdQHmAFdrrb0qIHxMa62V\nUk1G8CqlpgHTAHJycsi1c7CboaSkpNE+zzwDO3d24reZL7AfsG3YMFbMn9/2yThA9foCYD+8NalB\n59gcTc093Cxd2hfYGw9eFuXnU+Tw9VpDJObvVtw09/iyMg4HaoqL+cIaU9dvvmEoUNC9Oz+FeZy7\nO/e+Xi/Z1rPwkiWbyc9YtPsAABzySURBVM1d1eR+cZWVHHjVVXi2bKFw2DCeHXAR7/27+TDYpKRa\nJk5cTG5ucZvHFgph/d9rrR17AYnAR8D/BaxbCfS0lnsCK4OdZ+TIkToY8+bNa37jccdpDVo/+2zQ\n80Sa2kKvVtRq0Lqmpm3naHHuYeLSS+VP+BiXaZ2X5/j1WkMk5u9WXDX3mhr5kiiltc8n62bMkHWX\nXhr2y+323O+5R7/CGRq0Pu20Zvbx+bQ++2yZwz77aL1jh548WT7edJPWVVWNX239HbeWUOYPfK9D\nuFc7ZhpS8ug/C1ihtQ6IluVtYKq1PBV4y6kxABJDakvNY45x9FJtIS4tVWr3AMWF7i0z4S30AeCh\n2CSTGZomPl7yS7SWcD1wr6MYQjMN3XMPvPiimLXefptttdnMmSNR6Jde2tg0lJjY6n71rsBJH8FY\n4FzgKKXUYut1PHAvcLRSajUw0frsHPPn18WQ5jjijtg94uLwKBEERZvd25zGbqXpyYyPnVAIQ+Rp\nGDnkZkEQLGpo0SIJFVVKooUGD2bWLKiulrJC7SlmwjEfgdb6C6A5Q1pjd7tTuDRaKJD0+BI21oB3\naxkM8UR7OE3i3VkFJJOeY9pTGlogJUUiNUpLISvL3YIgmEbw7rvyfuGFcPLJ1NbWxfxffnlkhhgp\n3F3XIBzEgCDwdynbWhZkz+jhNw3t4bLID4O7CNQIvF6Jp+zcWVpUuo2AwnM7d0rCcD1sk7J173jv\nPdiwAfbd11XFCcJC+xYE69ZJentampT7cymeTmJPtc0vbsQuiufZKz3KIzG4msBcglVWFE6/fu40\nnGdm0olqPKqY2tq6elqAhLx+/bUsjxsH1C8bEa0qoU7RzqbTgI8+kveJE11t1/YkVQHgza+K8kia\nx1smVkTPPllRHonB1QTmErjZLASQkQFAtmrCPPTddxJoMmgQdOvmb0DfubOUlW5vtG9BEANmIQBP\nsrSpLNrpznaVWoO3UkphePbtFuXRGFxNoGkoVgSBbzvQQBDYZiGrNvSTT8rHyZPF9dHeaL+CoLoa\nPv1Ulo89NrpjCUK6y7uUVVZCtU6kMxV07uuSxiIGdxJoGnK7IEhIgLQ0v8N4w4aAbXbvyCOOqNeA\nvr05iW3aryAoL4erroIzz3SnoyoATxfxUnkL3dmlzBScM4RMLJmGADIyGMJSQGoDvfsu8hD55Zey\nfdy4eg3oG/SfaTe0X0Hg8Uhn6FdeifZIgmJ3KbNvuG7D9l148EptXYOhOWxBUFJS5yx2syDIzOQW\n7uTMowvweqVG0F1XbEGXlUlHmR49mDlTdr38cvc0WAs37VcQxBAel3cpK/pVVGdPYrmo0wZDc9im\noVWrRCvv1s1V/T8akZlJKmW8fP0if6OxW/69N2fyKiVjjmHRIvjmG+l5bvcWbo+YX7UL8GRKaF1R\nqQtD7ADvb/nAHniS3OnMNrgIWyP48Ud5d7M2AH6HsSoq5KabpEvYlFNKmVNzBqs+mcg+lgO5qQb0\n7Ql3PoJ2MNKz3N2lzLteGtbbvgyDoVnsu+XixfLudkFgaysF0iTxhGNr+LbT4QzgF5ZuyODtt2Vz\ncw3o2wtGELgAT7Z0J3NllzKt8S5dB4h6bDC0iG0aKrZKMMeYIODHHxlQtoiFfadwwgmy6phjmm9A\n314wpiEX4G9XWRXdOj4ffSTJlP37Sx7NgPStJF81De8newHn4unTuC2fwVCPhvYTtwsCyzTkFwRW\n2Gj6USN5+0n47LP2GykUiBEELsDTQ56ivNXRa1f5+utw1lnSWdBG0Z0+PCTp9D7wDNsnauMzxAgp\nDTrtuV0Q2BpBoZg//fkD48cTF9f+ago1hxEELsAWBEW10Sno9uGHEhFRWwtnnlJF7Q8/8fOGLqyh\nH2vpC5ZrYM9e7TR2zhA+AjWChATo2zd6YwmFQNNQbS0sWCCfjzgiemOKAkYQuIAuPdNQ+CjVqdTW\nRrY+1+efw6mnSg7N1WduZMZXo1GbNkJyMlX3/JM1Ey5hxco4Cgvrwueqq6vZuHEjFXbzkSiTnp7O\nihUroj2MqOCmuSclJdGrSxf8IQ99+7q6xhdQ3zS0eLEk8/TtG1sNh8OAEQQuIC49jTSK8ZJOsVeT\nkRmZJ+9vv4UTT5S+PRedU86MdwajvEVwyCHw7LN06t+fQcCgwfWP27hxI2lpafTp0wflggyb4uJi\n0tLSoj2MqOCWuWut2blzJxszMvAbEN1uFoL6pqEAs1BHw0QNuYHERH+XMu/2yDxlL1kitfhKSmDK\nFHii81UiBI4+Gr74QjzGzVBRUUFWVpYrhIDBHSilyMrKoiJQnY0lQVBQUFdoroOZhcBBQaCU+q9S\nartSalnAuq5KqU+UUqutdxenHEYWT5y0qfRucb5d5cqVcr8vKIBJk+CZ6T8Q//R/RI1/5JGQsoeN\nEDA0RClVvwZDLAgC2zS0c2eH9Q+AsxrBM0DD+s83AHO11vsBc63PBsCTKN3JirY426Vs/XqJhNi+\nXYTByy/6SLz6Cqk1ffXVsfHjBeLj4znwwAM58MADGTt2LPfeG77W13l5eQwePDj4jobGBHZsiYXv\nkq0R5OeLeahPH9cXqXQCJ3sWz1dK9WmwehIw3lqeDeQC1zs1hlgivVM5VDhrGioogN/9DjZuhLFj\n4Y03oPPLz8LChVJM7pZbHLt2uElOTmaxlb3qFju5U2it0VoTFwttsWJNI0hOlm4zlVZ3wA6oDUDk\nfQQ5Wust1vJWICfC13ctns7OdimrrJTooJ9/hv33h3fegdSaIrjeksP/+Ie09Ixx+vTpw1/+8heG\nDBnCqFGjWLNmDSBP+UcddRRDhw5lwoQJrF+/HoBt27Zx6qmnMmzYMIYNG8ZXX30FQG1tLRdffDEH\nHHAAxxxzDOXl5Y2u9c4773DIIYcwfPhwJk6cyLZt2wAoKSnh/PPPZ8iQIQwdOpQ5c+YA8OGHHzJi\nxAiGDRvGhAkTALj99tt54IEH/OccPHgweXl55OXlMWDAAM477zwGDx7Mhg0buOyyyzjooIM44IAD\nuO222/zHfPfdd4wZM4Zhw4YxatQoiouLGTdunF9QAhx22GH89NNP4fxTN41S0ppy8GApOBcLBBbF\n66CCIGpRQ1prrZRqtgC/UmoaMA0gJyeHXNuR0wwlJSVB93EznSxn8Yql68jNzQ+yd32Czd3ng7//\nfX8+/zyHrKxKbrvtR376qZJ9Z85kr+3bKTrgABbtuWedsywI6enpFFslBNLsGtphpjhITe7y8nKG\nDh0KyBPzn//8Z04//XS01iQlJfHVV1/xwgsv8Kc//YlXX32Vyy67jLPOOotzzjmH//3vf1x++eW8\n+OKLXH755RxyyCE8++yz1NbWUlJSQmFhIatXr+Y///kPM2bMYOrUqTz33HNMblB+ctiwYXzyySco\npZg9ezZ33XUXd999N7feeivJycl+oVJQUMDatWu56KKL+OCDD+jTpw+7du2iuLiYyspKEhMT/X9P\nn89HSUkJAKtXr2bmzJk89thjANxwww107dqV2tpaTjrpJI477jj23XdfzjrrLJ5++mlGjhyJ1+ul\npqaGKVOm8NRTT3HfffexevVqysrK6Nu3r/86TlFRWcn8Rx9Fx8Wh7SgchwjXb/7gTp2wsx++SUqi\nIkbuI2G959lqpxMvoA+wLODzSqCntdwTWBnKeUaOHKmDMW/evKD7uJlr9v9Ag9YPTP6u1ccGm/u1\n12oNWqelab1okbXy55+1TkjQWimtf/ihVdf7+eef6z6IdyH8ryCkpqb6l71er3+5d+/e+tdff9Va\na11VVaW7du2qtdY6KytLV1VV+ddnZWVprbXOzs7WFRUV9c69du1a3a9fP//ne++9V995552NxrBk\nyRJ99NFH68GDB+v+/fvrY489Vmut9YgRI/SqVavq7fv222/rKVOmNDrHbbfdpu+//37/5wMOOECv\nXbtWr127Vvfp06fevo8//rgePny4HjJkiM7OztYvvvii/vrrr/WYMWManbe0tFTvu+++uqqqSl9/\n/fX6kUceabSPE9T7bjhM2H7zo0fLd26vvbT2+cJzzggQyvyB73UI99hIm4beBqZay1OBtyJ8fddi\nV/YsKgxvhc9//QseeEACgebMgQMPRG61V14JNTUwbZq0Xmor/9/emUdHVWd5/HOnWCWERQOiqNAO\nyhIIEBFQggRkGehWWTLK0gg00sNhcRQPB5WjzrR0OwdlcBk5giDiQSAugOOGwITBBWSJJCzK6XYi\nM4BCNBiIQcxy54/fS1kECAGSqkq9+zmnTtXv9169d2/yqm693+93v7e6QsElELqi6WJXN9WtWzf4\nOhAIUFxcfMY+06ZNY+rUqezevZuXXnrpohLsatWqRWnpr//z0GM0CMnSzcnJ4emnn2bjxo1kZ2cz\nZMiQCs932WWX0b9/f9auXUt6ejqjR4++YNt8Q9nQ0G23xW7lmfNQnctHVwBbgBtF5KCI/AF4Cugv\nIn8FbvfaBtAo3n35VWWVsrffdguBABYvdquEADdLvGGD+wA8+WTVnTBKWLVqVfC5Z8+eANxyyy2s\nXLkSgOXLl5OSkgJAv379WLBgAeDmBfLz8yt9nvz8fK6+2tVwfvXVV4P9/fv3Dw7ngBsa6tGjB5s3\nbyYnJweAvLw8wM1pZHra/ZmZmcHt5Tl+/DgNGjSgUaNGHDlyhA8++ACANm3a8O2337J9+3bATZyX\nBa2JEycyffp0unXrRpNoLg4Taa691j0HPyD+ozpXDY08x6Z+1XXOmky8J/F8/MSFxeaffoKsrEbk\n5UFurnt8/z3kflfC26tBNcCTAzcz9qsPYUq+WyK3caN785/+BFdcUcWehIeTJ0/SuXNnwI2rDx48\nOLiE9NixY3Tq1Im6deuyYsUKAJ5//nnGjx/P3LlzSUhI4JVXXgHg2WefZdKkSSxevJhAIMCCBQto\nUclynE888QRpaWk0adKEvn37Br/EZ8+ezZQpU0hMTCQQCPD4448zbNgwFi5cyLBhwygtLaVZs2as\nX7+e4cOHs2zZMjp06ED37t254RyJfElJSXTp0oW2bdtyzTXXcOuttwJQp04dVq1axbRp0zh58iT1\n69dnw4YNxMXFkZycTHx8POPHj7/4P7QfePxxSEmJ7RJk56My40eRfvhhjiB9gpsjGP6byo/XZ2er\ntmxZ8RjLH1mgpWfb0LmzalHRRdkaznHgylB+jiA3NzeC1oSXUN/Lc+jQIW3Tpo2WlJSEzZ4aOUdQ\nQ6nKOQLTGooS4pu6f8XxwsqJdGVkwF13uaGkli0LSU6+jIQESKh7nCveWEDC0T20anaSXr9rgjR5\nyFWVadz41+dbb7X6wzHMsmXLePTRR5k3b17NyD8wIop9E0QJjRJcdbL8SlQpW7kS7r0XfvkFRoyA\n++7bwYABvV2SwMCBcPQgdOzo9KWvuqq6TY8qvvnmm0ibEBWMHTuWsWPHRtoMo4ZgPxWihMpWKZs3\nD0aOdEFg+nQXFOrUKXWlxXr1cmnDvXrB5s2+CwKGYVwcFgiihPjmrjrZuaqUlZbCAw/AjBmuPXcu\nzJ/vahc03bIF+vVzGhJ33AEfffSrmJZhGMZ5sKGhKCG+hVszfrz4sjO2FRXBmDGQnu4EQpcuddLR\nqMKil+k4e7aLFBMmwEsv2di/YRgXhN0RRAlxV8YhlFKgcafVDQaXC5CeDvHxbth/1CjgwAGnIDdp\nElJaCg8/DC+/bEHAMIwLxgJBlPB3jeNpiNOBCZWDWbAAXnzRCSR++CH07VPqagZ06ADr1kGTJnw5\naxb8+c++yYpMTU1l3bp1p/XNnz+fyZMnV/i+uDhXE/rw4cOMGDHirPv06dOHHTt2VHic+fPnU1j4\nq1z44MGD+bGs+Llh1EAsEEQL9eoRj0srPp7rJHEzMmDaNLd50SLo2fhLl/gyfbrLJEtLg337ODJw\nYKSsjggjR44MZgmXsXLlSkaOPFcO4+lcddVVvPnmmxd9/vKB4P3336dxDZqTUdXTZC0MwwJBtCBC\nfMCrUvZdIV9/7ZaGlpTAzIdK+f3/znFCQZ995moHrF7txouuvDLChoefESNG8N577/HLL06y+8CB\nAxw+fJiUlBQKCgro168fXbt2pWPHjqxde6acVWjhmZMnT3LPPffQrl07hg4deprc9Nlkn5977jkO\nHz5MamoqqampgJOJ+P57pxg7b948EhMTSUxMZP78+cHztWvXrlpkrVNSUmJb1toID5XJOov0ww+Z\nxaqqPeruVFD9YMlhbd/eJQAPGVikxb+769eM4IkTVY8dO+194fY9NHs0QuKjOmTIEF2zZo2qqj7x\nxBM6Y8YMVVUtKirS/Px8VVXNzc3V66+/Xks9RckyxdKcnBzt0KGDqqo+88wzOn78eFVVzcrK0kAg\noNu3OwXYH374QVVVi4uL9bbbbtOsrCxVPTN7uay9Y8cOTUxM1IKCAj1x4oS2b99eMzMzNScnRwOB\ngH7hSb+mpaXpa6+9doZPeXl5QVsXLVqkDz74oKqqzpw5U++///7T9jt69Ki2bNlSs7OzT7O1IjVT\nEdEtW7YEt53Nv1OnTmnr1q1127Ztqqqan5+vRUVFunTp0qAN+/fv13N9Ji2zOHzUZPVRowLiazs1\nyfsevpx9+6D9DUW8frgPgf9c4wTi1q1zY0Q1aBiiuggdHnrrrbeCw0KqyiOPPEKnTp24/fbbOXTo\nUPCX9dnYvHkzY8aMAaBTp07BGgcA6enpdO3alS5durB371727dtXoU2ffPIJQ4cOpUGDBsTFxTFs\n2DA+9urgtm7dOqiNlJycfNbEt4MHDzJw4EA6duzI3Llz2bt3LwAbNmxgypQpwf2aNGnC1q1b6d27\nN61atQKgadOmFdoGcN1119GjR48K/du/fz8tWrSgW7duAMTHx1OrVi3S0tJ49913KSoqYsmSJYwb\nN+685zNqDhYIoohG9VwgOHikDk0bFfPOsd7E7/4U2rSBrVthwIAIW3gm1XVPcD7uvPNONm7cSGZm\nJoWFhSQnJwNOWTQ3N5edO3eya9cumjdvflHy0Bcq+3w+TNbaiGYsEEQR8fWKAAhICW/8fAfX5251\nGulbtsA5VCn9SlxcHKmpqUyYMOG0FUD5+fk0a9aM2rVrk5GRwYEDByo8Tu/evXn99dcB2LNnD9nZ\n2cC5ZZ8BGjZseNZKXykpKaxZs4bCwkJ++uknVq9eHZS7rgwXI2tddmdRVbLWN954o8la+xALBFFE\njxYHqEURL+pk+p76AMaNc1nCl18eadOikpEjR5KVlUVaWlqwb/To0ezYsYOOHTuybNky2rZtW+Ex\nJk+eTEFBAe3ateOxxx4L3lmEyj6PGjUqKPsMMGnSJAYNGhScLC6ja9eujBs3jptvvpnu3bszceJE\nunTpUml/ymStk5OTuSJEHnz27NkcO3aMxMREkpKSyMjIICEhgYULFzJmzBiSkpK4++67ARg+fDh5\neXl06NCBF154oVKy1qH+hcpaJyUl0b9//+CdgslaxzCVmUiI9MMvk8U6aZIWUs+NjvzlL5UumxfJ\nyeJooCIp5lgnnL5XRtbaJovDh00WxyrDhlG/0w3wxhswa5ZvEsSM6GfZsmV0796dOXPmmKx1DBIR\nPQIRGQQ8CwSAl1XVSlaCk5D2WXKYUTMwWevYJuyhXUQCwH8A/wC0B0aKSPtw22EYhmE4InGPdzPw\nN1X9H1X9BVgJ3BkBO4xLQCuzxtPwFXZN1Fwk3P88ERkBDFLViV7790B3VZ1abr9JwCSA5s2bJ5fX\nlilPQUFBUFTMb4Tb97i4OJo3b06jRo2QKJjHKCkpIRAIRNqMiBAtvqsq+fn5HDlyhIKCgrCc08+f\neaic/6mpqTtV9abzHStqNYtVdSGwEOCmm27SPn36VLj/pk2bON8+sUq4fS8qKuLgwYMcOnQobOes\niJ9//pl69Squ7BarRJPv9erVIykpidq1K1d3+1Lx82ceqtb/SASCQ8A1Ie2WXp9RQ6hduzatW7eO\ntBlBNm3adEHr9WMJP/tuVB2RmCPYDrQRkdYiUge4B3gnAnYYhmEYROCOQFWLRWQqsA63fHSJqu4N\ntx2GYRiGIyJzBKr6PvB+JM5tGIZhnE7YVw1dDCKSC1SsHgZXAN+HwZxoxM++g7/9N9/9S2X8v05V\nE853oBoRCCqDiOyozDKpWMTPvoO//Tff/ek7VK3/JhpiGIbhcywQGIZh+JxYCgQLI21ABPGz7+Bv\n/813/1Jl/sfMHIFhGIZxccTSHYFhGIZxEdT4QCAig0Rkv4j8TURmRdqe6kZElojIURHZE9LXVETW\ni8hfveeYLCgrIteISIaI7BORvSJyv9cf8/6LSD0R2SYiWZ7v/+L1txaRz73rf5WXrR+ziEhARL4Q\nkXe9ti/8F5FvRGS3iOwSkR1eX5Vd9zU6EPi0tsFSYFC5vlnARlVtA2z02rFIMTBDVdsDPYAp3v/b\nD/6fAvqqahLQGRgkIj2AfwP+XVX/HjgG/CGCNoaD+4EvQ9p+8j9VVTuHLBmtsuu+RgcCfFjbQFU3\nA3nluu8EXvVevwrcFVajwoSqfquqmd7rE7gvhKvxgf9eCdoyfefa3kOBvsCbXn9M+l6GiLQEhgAv\ne23BR/6fhSq77mt6ILga+L+Q9kGvz280V9VvvdffAc0jaUw4EJFWQBfgc3zivzcssgs4CqwHvgZ+\nVNVib5dYv/7nAzOBUq99Of7xX4GPRGSnV6sFqvC6j9p6BMbFoaoqIjG9FExE4oC3gH9W1eOhxXFi\n2X9VLQE6i0hjYDXQNsImhQ0R+S1wVFV3ikifSNsTAXqp6iERaQasF5GvQjde6nVf0+8IrLaB44iI\ntADwno9G2J5qQ0Rq44LAclV92+v2jf8AqvojkAH0BBqLSNkPuli+/m8F7hCRb3BDwH2BZ/GJ/6p6\nyHs+ivsRcDNVeN3X9EBgtQ0c7wD3eq/vBdZG0JZqwxsTXgx8qarzQjbFvP8ikuDdCSAi9YH+uDmS\nDGCEt1tM+g6gqg+raktVbYX7nP+Xqo7GB/6LSAMRaVj2GhgA7KEKr/san1AmIoNxY4dltQ3mRNik\nakVEVgB9cMqDR4DHgTVAOnAtTqX1H1W1/IRyjUdEegEfA7v5dZz4Edw8QUz7LyKdcBOCAdwPuHRV\n/VcR+Q3uF3JT4AtgjKqeipyl1Y83NPSQqv7WD/57Pq72mrWA11V1johcThVd9zU+EBiGYRiXRk0f\nGjIMwzAuEQsEhmEYPscCgWEYhs+xQGAYhuFzLBAYhmH4HAsEhq8QkQLvuZWIjKriYz9Srv1ZVR7f\nMKoLCwSGX2kFXFAgCMlgPRenBQJVveUCbTKMiGCBwPArTwEpnr77A56g21wR2S4i2SLyR3DJSyLy\nsYi8A+zz+tZ44l97ywTAROQpoL53vOVeX9ndh3jH3uNpyt8dcuxNIvKmiHwlIsslVDjJMMKEic4Z\nfmUWXnYqgPeFnq+q3USkLvCpiHzk7dsVSFTVHK89QVXzPKmH7SLylqrOEpGpqtr5LOcahqshkITL\nCN8uIpu9bV2ADsBh4FOcps4nVe+uYZwbuyMwDMcAYKwn8/w5TuK4jbdtW0gQAJguIlnAVpzoYRsq\nphewQlVLVPUI8N9At5BjH1TVUmAXbsjKMMKK3REYhkOAaaq67rROp2vzU7n27UBPVS0UkU1AvUs4\nb6guTgn2mTQigN0RGH7lBNAwpL0OmOzJXCMiN3hKj+VpBBzzgkBbXMnMMorK3l+Oj4G7vXmIBKA3\nsK1KvDCMKsB+fRh+JRso8YZ4luK07VsBmd6EbS5nL/33IfBPIvIlsB83PFTGQiBbRDI9ieQyVuNq\nB2ThKk3NVNXvvEBiGBHH1EcNwzB8jg0NGYZh+BwLBIZhGD7HAoFhGIbPsUBgGIbhcywQGIZh+BwL\nBIZhGD7HAoFhGIbPsUBgGIbhc/4fDyRHP0CYQ2EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "6wxXo2KqYnxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test"
      ]
    },
    {
      "metadata": {
        "id": "Nl5Oc1wTYpNe",
        "colab_type": "code",
        "outputId": "ccfeaaa4-b40c-4fda-8fdf-a0642910fe77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dataset = 'fashionmnist'\n",
        "print(\"Testing on dataset {}\".format(dataset))\n",
        "dat = DatasetManager(dataset=dataset, percent_data=percent_data, percent_val=percent_val)\n",
        "dat.ImportDataset(batch_size=batch_size)\n",
        "dataloader = dat.test_loader\n",
        "dataset_size = dat.testset.__len__()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "running_loss = 0.0\n",
        "running_corrects = 0.0\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "since = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "test_loss = running_loss / dataset_size\n",
        "test_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Testing complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "print('Test Loss: {:.4f} Acc: {:.4f}'.format(test_loss, test_acc))\n",
        "\n",
        "print('Accuracy of the network on test images: %d %%' %(100.0*correct/total))\n",
        "\n",
        "# print images\n",
        "# imshow(tv.utils.make_grid(images))\n",
        "# print('Ground truth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing on dataset fashionmnist\n",
            "\n",
            "Full training set size: 60000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2400\n",
            "Active validation set size: 600\n",
            "Active test set size: 500\n",
            "\n",
            "Testing complete in 0m 3s\n",
            "Test Loss: 3.0283 Acc: 0.0260\n",
            "Accuracy of the network on test images: 2 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CiyBa-v7do4e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "//////////////////////////////////////////////////////////////////\n",
        "\"pPruneActNorm5_pre_out_7_in_7_multiple_5percent\"\n",
        "Testing on dataset svhn\n",
        "Using downloaded and verified file: ./data/svhn/train_32x32.mat\n",
        "Using downloaded and verified file: ./data/svhn/test_32x32.mat\n",
        "\n",
        "Full training set size: 73257\n",
        "Full test set size: 26032\n",
        "\n",
        "Active training set size: 2930\n",
        "Active validation set size: 733\n",
        "Active test set size: 1302\n",
        "\n",
        "Testing complete in 0m 8s\n",
        "Test Loss: 2.7357 Acc: 0.1137\n",
        "Accuracy of the network on test images: 11 %\n",
        "//////////////////////////////////////////////////////////////////"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UQlUKtFYD5pc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Routines"
      ]
    },
    {
      "metadata": {
        "id": "VASgSU9KD-81",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Baseline Model"
      ]
    },
    {
      "metadata": {
        "id": "yNR6vmDhEBT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model_baseline.train()\n",
        "\n",
        "model_baseline = train_model(model_baseline, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_wR099MzELCI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Pruned Model"
      ]
    },
    {
      "metadata": {
        "id": "0eNeKM87EN6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}