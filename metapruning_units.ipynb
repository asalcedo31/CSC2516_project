{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metapruning_units.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "tYqrMVdpA1NX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision as tv\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.modules import Module\n",
        "import torchvision.models.vgg as tv_vgg\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.modules.utils import _pair as pair\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BhzbK8ntAmnN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Computation Routines"
      ]
    },
    {
      "metadata": {
        "id": "NuhlcxSIA94G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ]
    },
    {
      "metadata": {
        "id": "YC8SpzkJA8gK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DatasetManager:\n",
        "    \n",
        "    def __init__(self, dataset='cifar10', percent_data=10.0, percent_val=20.0, data_path='./data'):\n",
        "        \n",
        "        # 'dataset' can be 'cifar10', 'cifar100', 'mnist', 'fashionmnist', 'kmnist', 'emnist', 'stl10', 'svhn'.\n",
        "        # 'percent_data' is the percentage of the full training set to be used.\n",
        "        # 'percent_val' is the percentage of the *loaded* training set to be used as validation data.\n",
        "        \n",
        "        data_path = './data/{}'.format(dataset)\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.data_path = data_path\n",
        "        self.percent_data = percent_data\n",
        "        self.percent_val = percent_val\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "            \n",
        "        elif self.dataset == 'cifar10' or\\\n",
        "             self.dataset == 'cifar100' or\\\n",
        "             self.dataset == 'stl10' or\\\n",
        "             self.dataset == 'svhn':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "        \n",
        "        elif self.dataset == 'mnist' or\\\n",
        "             self.dataset == 'fashionmnist' or\\\n",
        "             self.dataset == 'kmnist' or\\\n",
        "             self.dataset == 'emnist':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "        return\n",
        "    \n",
        "    \n",
        "    def ImportDataset(self, batch_size=5):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "        \n",
        "            self.trainset = tv.datasets.ImageFolder(root=self.data_path,\n",
        "                             transform=self.transform)\n",
        "        \n",
        "        # todo\n",
        "        \n",
        "        elif self.dataset == 'cifar10':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR10(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR10(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "        \n",
        "        elif self.dataset == 'cifar100':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR100(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR100(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "             \n",
        "        elif self.dataset == 'mnist':\n",
        "\n",
        "            self.trainset = tv.datasets.MNIST(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.MNIST(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'fashionmnist':\n",
        "\n",
        "            self.trainset = tv.datasets.FashionMNIST(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.FashionMNIST(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'kmnist':\n",
        "\n",
        "            self.trainset = tv.datasets.KMNIST(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.KMNIST(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'emnist':\n",
        "\n",
        "            self.trainset = tv.datasets.EMNIST(root=self.data_path, split='balanced', train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.EMNIST(root=self.data_path, split='balanced', train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'stl10':\n",
        "\n",
        "            self.trainset = tv.datasets.STL10(root=self.data_path, split='train',\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.STL10(root=self.data_path, split='test',\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        elif self.dataset == 'svhn':\n",
        "\n",
        "            self.trainset = tv.datasets.SVHN(root=self.data_path, split='train',\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.SVHN(root=self.data_path, split='test',\n",
        "                                       download=True, transform=self.transform)\n",
        "\n",
        "        self.SplitData();\n",
        "        self.GenerateLoaders();\n",
        "                \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def SplitData(self):\n",
        "        \n",
        "        len_full = self.trainset.__len__()\n",
        "        len_train = int(np.round(len_full*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.trainset = torch.utils.data.random_split(self.trainset, (len_full-len_train, len_train))\n",
        "        \n",
        "        len_val = int(np.round(len_train*self.percent_val/100.0))\n",
        "        len_train = len_train - len_val\n",
        "        \n",
        "        self.valset, self.trainset = torch.utils.data.random_split(self.trainset, (len_val, len_train))\n",
        "         \n",
        "        len_full_test = self.testset.__len__()\n",
        "        len_test = int(np.round(len_full_test*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.testset = torch.utils.data.random_split(self.testset, (len_full_test-len_test, len_test))\n",
        "\n",
        "        print('\\nFull training set size: {}'.format(len_full))\n",
        "        print('Full test set size: {}'.format(len_full_test))\n",
        "        print('\\nActive training set size: {}'.format(len_train))\n",
        "        print('Active validation set size: {}'.format(len_val))\n",
        "        print('Active test set size: {}\\n'.format(len_test))\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def GenerateLoaders(self):\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.val_loader = torch.utils.data.DataLoader(self.valset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)          \n",
        "            \n",
        "        return\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUJWD_oDBKHD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training functions"
      ]
    },
    {
      "metadata": {
        "id": "34G6XlaFBMUW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def RecordLosses(phase, epoch_loss, epoch_acc, prune_settings):\n",
        "    \n",
        "    # Record losses for later use, plotting etc\n",
        "    if phase == 'train':\n",
        "        prune_settings.epoch_loss.append(epoch_loss)\n",
        "        prune_settings.epoch_acc.append(epoch_acc)\n",
        "    elif phase == 'val':\n",
        "        prune_settings.val_loss.append(epoch_loss)\n",
        "        prune_settings.val_acc.append(epoch_acc)\n",
        "\n",
        "    return prune_settings\n",
        "\n",
        "\n",
        "def PlotResults(prune_settings, savename=\"fig\"):\n",
        "    \n",
        "    # ====== Plot ======\n",
        "\n",
        "    # ------ Loss ------\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(1, len(prune_settings.epoch_loss)+1), \n",
        "             prune_settings.epoch_loss, \n",
        "             color='red', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Epoch loss')\n",
        "    plt.plot(np.arange(1, len(prune_settings.val_loss)+1), \n",
        "             prune_settings.val_loss, \n",
        "             color='blue', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Validation loss')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    savename_full = \"{}_loss.pdf\".format(savename)\n",
        "    plt.savefig(savename_full)\n",
        "    files.download(savename_full)\n",
        "\n",
        "    # ------ Accuracy ------\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(1, len(prune_settings.epoch_acc)+1), \n",
        "             np.asarray(prune_settings.epoch_acc)*100.0, \n",
        "             color='red', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Epoch accuracy')\n",
        "    plt.plot(np.arange(1, len(prune_settings.val_acc)+1), \n",
        "             np.asarray(prune_settings.val_acc)*100.0, \n",
        "             color='blue', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label = 'Validation accuracy')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    savename_full = \"{}_acc.pdf\".format(savename)\n",
        "    plt.savefig(savename_full)\n",
        "    files.download(savename_full)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def SaveResults(prune_settings, model):\n",
        "    \n",
        "    \n",
        "    \n",
        "    return\n",
        "\n",
        "\n",
        "def train_model(model, dat, criterion, optimizer, scheduler, prune_settings=0, num_epochs=25):\n",
        "    \n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                dataloader = dat.train_loader\n",
        "                dataset_size = dat.trainset.__len__()\n",
        "                \n",
        "                model.train()  # Set model to training mode\n",
        "                \n",
        "            else:\n",
        "                \n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                dataloader = dat.val_loader\n",
        "                dataset_size = dat.valset.__len__()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloader:\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                if prune_settings != 0:\n",
        "                    TrackConv2DNorms(model, prune_settings, inputs)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if training\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                \n",
        "            # Record losses for later use, plotting etc\n",
        "            prune_settings = RecordLosses(phase, epoch_loss, epoch_acc, prune_settings)\n",
        "\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # Record losses for later use, plotting etc\n",
        "    prune_settings.outer_iter_time.append(time_elapsed)\n",
        "    \n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z7C3UubBBXFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Filter Pruning functions"
      ]
    },
    {
      "metadata": {
        "id": "MaSebsFceMRD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning settings"
      ]
    },
    {
      "metadata": {
        "id": "jAZVF5O-BZNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Constants that define possible pruning metrics\n",
        "WEIGHT_NORM = 1\n",
        "ACT_NORM = 2\n",
        "\n",
        "# Class that contains various settings pertaining to how filters are pruned\n",
        "class UnitPruningSettings:\n",
        "    \n",
        "    def __init__(self, idx_layer=0, idx_filter=0, N_prune=1, P_prune=10, p=2, pruning_metric=WEIGHT_NORM):\n",
        "        \n",
        "        # EITHER N_prune OR P_prune will be used to decide how many filters to prune.\n",
        "        # If one is non-positive, the other is used.\n",
        "        # If neither is non-positive, priority is given to P_prune.\n",
        "        # If both are non-positive, no pruning will happen.\n",
        "\n",
        "        self.N_prune = N_prune # Number of filters allowed to be pruned in one pass\n",
        "        self.P_prune = P_prune; # Percent of filters of the current layer to prune\n",
        "        \n",
        "        self.idx_filter = idx_filter # Indices of the N_prune filters\n",
        "        self.idx_layer = idx_layer # Current layer under consideration\n",
        "        self.p = p # p-norm to use when computing which filters to remove\n",
        "        self.pruning_metric = pruning_metric\n",
        "        \n",
        "        self.norms_botk = []\n",
        "        self.idx_norms_botk = []\n",
        "        \n",
        "        # Various statistics will be stored and computed to keep track of how the network changes\n",
        "        \n",
        "        # Number of filters per layer in the original network\n",
        "        self.filters_per_layer_orig = []\n",
        "        \n",
        "        # Number of filters per layer after pruning - this gets updated every time the network is pruned\n",
        "        self.filters_per_layer_after = []\n",
        "        \n",
        "        # Time taken to prune in sec (running total, updated every time pruning happens)\n",
        "        self.prune_time = 0.0\n",
        "        self.outer_iter_time = []\n",
        "        \n",
        "        # Keep track of running epoch loss and validation loss, and corresponding accuracy\n",
        "        self.epoch_loss = []\n",
        "        self.val_loss = []\n",
        "        self.epoch_acc = []\n",
        "        self.val_acc = []\n",
        "        \n",
        "        return\n",
        "    \n",
        "    # Function to print the current pruning state of the model. Verbose can be 0, 1, or 2.\n",
        "    def PrintPruningStatistics(self, verbose=1):\n",
        "    \n",
        "        if verbose == 0:\n",
        "            return\n",
        "        \n",
        "        print(\"Total number of filters before pruning: {}\".format(sum(self.filters_per_layer_orig)))\n",
        "        print(\"Total number of filters after pruning: {}\".format(sum(self.filters_per_layer_after)))\n",
        "    \n",
        "        return\n",
        "    \n",
        "    # Function to set up and initialize based on a given model\n",
        "    def Setup(self, model):\n",
        "        \n",
        "        # Count the number of conv layers\n",
        "        self.N_layers = 0\n",
        "        \n",
        "        for layer, (name, module) in enumerate(model.features._modules.items()):\n",
        "            self.N_layers += 1\n",
        "                    \n",
        "        # Initialize storage containers\n",
        "        self.norms_botk = [None]*self.N_layers\n",
        "        self.idx_norms_botk = [None]*self.N_layers\n",
        "        \n",
        "    \n",
        "    # Function to reset norm containers\n",
        "    def ResetNormContainers(self):\n",
        "    \n",
        "        self.norms_botk = [None]*self.N_layers\n",
        "        self.idx_norms_botk = [None]*self.N_layers\n",
        "    \n",
        "        return\n",
        "\n",
        "    # Function to reset filter containers\n",
        "    def ResetFilterContainers(self):\n",
        "    \n",
        "        self.filters_per_layer_orig = []\n",
        "        self.filters_per_layer_after = []\n",
        "    \n",
        "        return\n",
        "\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qc41Ng14ezag",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning decisions"
      ]
    },
    {
      "metadata": {
        "id": "2F0n6VUTe2ND",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to compute the p-norm of weights in all filters of a given layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DWeightNorms(model, idx_layer, p):\n",
        "    \n",
        "    # Extract the layer of the model currently being considered\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    weights = conv.weight.data\n",
        "\n",
        "    # Compute norms of each filter\n",
        "    norms = weights.norm(p, dim=2).norm(p, dim=2).norm(p, dim=1)\n",
        "#     norms = norms/torch.max(torch.abs(norms))\n",
        "    \n",
        "    return norms\n",
        "\n",
        "\n",
        "# Function to compute the p-norm of activations in all filters per layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DActNorms(activation, prune_settings):\n",
        "    \n",
        "    p = prune_settings.p\n",
        "    \n",
        "    # Compute norms of each activation\n",
        "    norms = torch.norm(activation, p, dim=0).norm(p, dim=1).norm(p, dim=1)\n",
        "#     norms = norms/torch.max(torch.abs(norms))\n",
        "        \n",
        "    return norms\n",
        "\n",
        "\n",
        "# Function to track the p-norm of activations of all filters of during training.\n",
        "def TrackConv2DNorms(model, prune_settings, inputs):\n",
        "    \n",
        "    p = prune_settings.p\n",
        "    P_prune = prune_settings.P_prune\n",
        "\n",
        "    x = Variable(inputs)\n",
        "\n",
        "    ii = -1\n",
        "    for layer, (name, module) in enumerate(model.features._modules.items()):\n",
        "        ii += 1\n",
        "        x = module(x)\n",
        "        \n",
        "        if isinstance(module, torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            if prune_settings.pruning_metric == WEIGHT_NORM:\n",
        "                norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "            elif prune_settings.pruning_metric == ACT_NORM:\n",
        "                norms = ComputeConv2DActNorms(x, prune_settings)\n",
        "\n",
        "            # Use the given prune percentage to figure out how many filters to prune\n",
        "            if (P_prune >= 0):\n",
        "                N_prune = int(len(norms.float())*P_prune/100.0)\n",
        "                prune_settings.N_prune = N_prune\n",
        "\n",
        "#             n_botk, ind_botk = torch.topk(norms, N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            norms = norms.cpu().detach().numpy()\n",
        "    \n",
        "            # Store the norms for each filter\n",
        "#             if prune_settings.norms_botk[ii] is None:\n",
        "#                 prune_settings.norms_botk[ii] = norms\n",
        "#             else:\n",
        "#                 prune_settings.norms_botk[ii] += norms\n",
        "                \n",
        "            # Store normalized norms for each filter\n",
        "            if prune_settings.norms_botk[ii] is None:\n",
        "                prune_settings.norms_botk[ii] = norms/max(norms)\n",
        "            else:\n",
        "                prune_settings.norms_botk[ii] += norms/max(norms)\n",
        "                \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U59vXkzfehDX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning workers"
      ]
    },
    {
      "metadata": {
        "id": "RY5GZWxXej1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The following functions were adapted from https://github.com/jacobgil/pytorch-pruning/blob/master/prune.py\n",
        "\n",
        "def replace_layers(model, i, idx, layers):\n",
        "\tif i in idx:\n",
        "\t\treturn layers[idx.index(i)]\n",
        "\treturn model[i]\n",
        "\n",
        "\n",
        "# Function to prune a given convolution layer in the model provided.\n",
        "# Input \"idx_layers\" is the global index of the convolution layer to be pruned.\n",
        "# Input \"prune_settings\" is a data structure containing information on how pruning is performed.\n",
        "def PruneConvLayers(model, prune_settings):\n",
        "    \n",
        "    # Strategy: in order to prune a particular layer, the output of the previous layer \n",
        "    # and the inputs to the next layer must also be altered accordingly.\n",
        "\t\n",
        "    # Extract pruning settings for convenience\n",
        "    N_prune = prune_settings.N_prune\n",
        "    idx_filter = prune_settings.idx_filter\n",
        "    idx_layer = prune_settings.idx_layer\n",
        "    \n",
        "    if idx_layer >= len(model.features._modules.items()):\n",
        "        return\n",
        "        \n",
        "    # Extract the layer of the model currently being pruned\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    \n",
        "\n",
        "    # In case the list of target filters to delete has out-of-range entries, detect and ignore them\n",
        "    del_filters = []\n",
        "    for kk in range(0, len(idx_filter)):\n",
        "        if idx_filter[kk] >= conv.out_channels:\n",
        "            del_filters.extend(kk)\n",
        "    \n",
        "    if (len(del_filters) > 0):\n",
        "        idx_filter = np.delete(idx_filter, del_filters, 0)\n",
        "        N_prune = len(idx_filter)\n",
        "        prune_settings.N_prune = N_prune\n",
        "        print(\"[WARNING] Encountered an out-of-range target filter; it will be ignored.\")\n",
        "    \n",
        "    # Record pruning statistics\n",
        "    prune_settings.filters_per_layer_orig[idx_layer] = conv.out_channels\n",
        "    prune_settings.filters_per_layer_after[idx_layer] = conv.out_channels - N_prune\n",
        "    \n",
        "        \n",
        "    # To keep track of the succeeding convolution layer\n",
        "    next_conv = None\n",
        "    offset = 1\n",
        "    \n",
        "    # Figure out how many layers after this one are NOT conv layers, in order to skip pruning them\n",
        "    while idx_layer + offset < len(model.features._modules.items()):\n",
        "        \n",
        "        res =  list(model.features._modules.items())[idx_layer + offset]\n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            next_name, next_conv = res\n",
        "            break\n",
        "        offset = offset + 1\n",
        "    \n",
        "    # Create a new, replacement conv layer to remove a given number of filters.\n",
        "    # The rest of its settings should remain the same as the original conv layer.\n",
        "    new_conv = torch.nn.Conv2d(in_channels = conv.in_channels,\n",
        "                               out_channels = conv.out_channels - N_prune,\n",
        "\t\t\t                   kernel_size = conv.kernel_size,\n",
        "                               stride = conv.stride,\n",
        "                               padding = conv.padding,\n",
        "                               dilation = conv.dilation,\n",
        "                               groups = conv.groups,\n",
        "                               bias = True)\n",
        "    \n",
        "    new_conv.bias = conv.bias\n",
        "    \n",
        "    # Copy over the weights to the new conv layer, except the ones corresponding to the filter to be removed\n",
        "    old_weights = conv.weight.data.cpu().numpy()\n",
        "    new_weights = new_conv.weight.data.cpu().numpy()\n",
        "    \n",
        "    # Copy over the set of filters, excluding the ones to be removed\n",
        "    new_weights_temp = np.copy(old_weights)\n",
        "    new_weights_temp = np.delete(new_weights_temp, idx_filter, 0)\n",
        "    new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "    # Update weight data of the new conv layer\n",
        "    new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "    \n",
        "    # Now do the same thing for biases\n",
        "    old_biases = conv.bias.data.cpu().numpy()\n",
        "    new_biases = np.zeros(shape=(old_biases.shape[0] - N_prune), dtype=np.float32)\n",
        "    \n",
        "    new_biases_temp = np.copy(old_biases)\n",
        "    new_biases_temp = np.delete(new_biases_temp, idx_filter, 0)\n",
        "    new_biases[:] = new_biases_temp[:]\n",
        "        \n",
        "    new_conv.bias.data = torch.from_numpy(new_biases).cuda()\n",
        "    \n",
        "    # If there is a succeeding conv layer, adjust its input units and weights accordingly\n",
        "    if next_conv != None:\n",
        "        \n",
        "        next_new_conv = torch.nn.Conv2d(in_channels = next_conv.in_channels - N_prune,\n",
        "                                        out_channels =  next_conv.out_channels,\n",
        "                                        kernel_size = next_conv.kernel_size,\n",
        "                                        stride = next_conv.stride,\n",
        "                                        padding = next_conv.padding,\n",
        "                                        dilation = next_conv.dilation,\n",
        "                                        groups = next_conv.groups,\n",
        "                                        bias = True)\n",
        "        \n",
        "        next_new_conv.bias = next_conv.bias\n",
        "\n",
        "        old_weights = next_conv.weight.data.cpu().numpy()\n",
        "        new_weights = next_new_conv.weight.data.cpu().numpy()\n",
        "        \n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_filter, 1)\n",
        "        new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "        next_new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        # Now do the same thing for biases\n",
        "        next_new_conv.bias.data = next_conv.bias.data\n",
        "\n",
        "        # Update the actual model by replacing the existing filters with the new ones\n",
        "        features = torch.nn.Sequential(\n",
        "                *(replace_layers(model.features, i, [idx_layer, idx_layer + offset], \\\n",
        "                    [new_conv, next_new_conv]) for i, _ in enumerate(model.features)))\n",
        "        del model.features\n",
        "        del conv\n",
        "\n",
        "        model.features = features\n",
        "    \n",
        "    else:\n",
        "\n",
        "        # This is the last conv layer. This affects the first linear layer of the classifier.\n",
        "        model.features = torch.nn.Sequential(*(replace_layers(model.features, i, [idx_layer], [new_conv]) for i, _ in enumerate(model.features)))\n",
        "        idx_layer = 0\n",
        "        old_linear_layer = None\n",
        "\n",
        "        for _, module in model.classifier._modules.items():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                old_linear_layer = module\n",
        "                break\n",
        "            idx_layer = idx_layer + 1\n",
        "\n",
        "        if old_linear_layer == None:\n",
        "            raise BaseException(\"No linear layer found in classifier.\")\n",
        "            \n",
        "        params_per_input_channel = int(old_linear_layer.in_features/conv.out_channels)\n",
        "\n",
        "        new_linear_layer = torch.nn.Linear(old_linear_layer.in_features - N_prune*params_per_input_channel, \n",
        "                                           old_linear_layer.out_features)\n",
        "\n",
        "        old_weights = old_linear_layer.weight.data.cpu().numpy()\n",
        "        new_weights = new_linear_layer.weight.data.cpu().numpy()\t \t\n",
        "\n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        idx_expanded = np.zeros(shape=(N_prune*params_per_input_channel))\n",
        "        \n",
        "        for kk in range(0, len(idx_filter)):\n",
        "            idx_expanded[kk*params_per_input_channel:kk*params_per_input_channel+params_per_input_channel] = np.arange(idx_filter[kk]*params_per_input_channel, idx_filter[kk]*params_per_input_channel + params_per_input_channel)\n",
        "\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_expanded.astype(int), 1)\n",
        "        new_weights[:, :] = new_weights_temp[:, :]\n",
        "        \n",
        "        new_linear_layer.bias.data = old_linear_layer.bias.data\n",
        "        new_linear_layer.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        classifier = torch.nn.Sequential(*(replace_layers(model.classifier, i, [idx_layer], [new_linear_layer]) for i, _ in enumerate(model.classifier)))\n",
        "\n",
        "        del model.classifier\n",
        "        del next_conv\n",
        "        del conv\n",
        "        model.classifier = classifier\n",
        "        \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7h5EFGhe-PJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning driver"
      ]
    },
    {
      "metadata": {
        "id": "Xp8V5o_UfADH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to iterate through all conv2D layers of the network and determine \n",
        "# filters to be pruned, and then carry out the pruning.\n",
        "def PruneAllConv2DLayers(model, prune_settings):\n",
        "    \n",
        "    # Extract pruning settings for convenience\n",
        "    # Note that \"N_prune\" *consecutive* filters will get pruned\n",
        "    N_prune = prune_settings.N_prune\n",
        "    P_prune = prune_settings.P_prune\n",
        "    p = prune_settings.p\n",
        "    pruning_metric = prune_settings.pruning_metric\n",
        "    \n",
        "    # Count number of prunable layers for preallocation\n",
        "    N_layers = len(model.features._modules.items())       \n",
        "    prune_settings.filters_per_layer_orig = np.zeros(shape=(1, N_layers)).ravel()\n",
        "    prune_settings.filters_per_layer_after = np.zeros(shape=(1, N_layers)).ravel()\n",
        "\n",
        "    \n",
        "    # Find the N_prune filters to remove\n",
        "    ii = 0\n",
        "    while ii < len(model.features._modules.items()):\n",
        "        \n",
        "        res = list(model.features._modules.items())[ii]\n",
        "        \n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            _, conv = list(model.features._modules.items())[ii]\n",
        "            \n",
        "            # Record pruning statistics\n",
        "            prune_settings.filters_per_layer_orig[ii] = conv.out_channels\n",
        "            prune_settings.filters_per_layer_after[ii] = conv.out_channels\n",
        "            \n",
        "            # Compute values and indices of the N_prune smallest norms\n",
        "#             if pruning_metric == WEIGHT_NORM:\n",
        "#                 norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "#             elif pruning_metric == ACT_NORM:\n",
        "# #                 norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "#                 norms = ComputeConv2DActNorms(res[1], prune_settings)\n",
        "                \n",
        "        \n",
        "            if (P_prune >= 0):\n",
        "                N_prune = int(conv.out_channels*P_prune/100.0)\n",
        "                prune_settings.N_prune = N_prune\n",
        "            \n",
        "            if prune_settings.norms_botk[ii] is not None:\n",
        "                \n",
        "#                 n_botk, ind_botk = torch.topk(torch.from_numpy(prune_settings.norms_botk[ii]), N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            \n",
        "                norms = np.asarray(prune_settings.norms_botk[ii]).ravel()\n",
        "                ind_botk = np.argpartition(norms, N_prune)    \n",
        "                n_botk = norms[ind_botk[:N_prune]]\n",
        "                ind_botk = ind_botk[:N_prune]\n",
        "        \n",
        "                prune_settings.idx_layer = ii\n",
        "                prune_settings.idx_filter = ind_botk\n",
        "\n",
        "                model = PruneConvLayers(model, prune_settings)\n",
        "                \n",
        "        ii = ii + 1\n",
        "            \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cCEASFCwB3uP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Pruning"
      ]
    },
    {
      "metadata": {
        "id": "LASRjk-I2zXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "400be83d-16e1-4900-e601-8c273f1b652e"
      },
      "cell_type": "code",
      "source": [
        "# Test pruning\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.train()\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10, 12, 15, 16, 21), \n",
        "                                     N_prune=5, p=2, pruning_metric=WEIGHT_NORM)\n",
        "# prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10), \n",
        "#                                      N_prune=1, p=2, pruning_metric=WEIGHT_NORM)\n",
        "\n",
        "N_layers = len(model.features._modules.items())       \n",
        "prune_settings.filters_per_layer_orig = np.zeros(shape=(1, N_layers)).ravel()\n",
        "prune_settings.filters_per_layer_after = np.zeros(shape=(1, N_layers)).ravel()\n",
        "\n",
        "t0 = time.time()\n",
        "model = PruneConvLayers(model, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning took 6.038398027420044 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BgOIBqly2ntw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# L0 Masking Functions"
      ]
    },
    {
      "metadata": {
        "id": "zEl8diRM2sa0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Masked:\n",
        "    def make_mask(self, threshold, mask=None):\n",
        "        if mask is None:\n",
        "            print(\"new mask\", device)\n",
        "            self.mask = torch.ones(self.weight.size(), requires_grad=False).to(device)\n",
        "        else:\n",
        "            self.mask = mask      \n",
        "        self.zeros = torch.zeros(self.weight.size(), requires_grad=False).to(device)\n",
        "        self.threshold = threshold\n",
        "    \n",
        "    def set_threshold(self, prop=0.05):\n",
        "        unique_weights = torch.unique(self.weight*self.mask)\n",
        "        mask_size = self.mask.reshape(-1).size()[0]\n",
        "#     mask_size = mask_size[0]*mask_size[1]\n",
        "        mask_nonzero = torch.sum(self.mask.view([mask_size]))\n",
        "        mask_total = mask_size\n",
        "        print('nonzero proportion: {:.4f}'.format(mask_nonzero/mask_total))\n",
        "        self.threshold = torch.max(torch.topk(torch.abs(unique_weights),int(prop*unique_weights.size()[0]),largest=False)[0])    \n",
        "\n",
        "    def make_threshold_mask(self):\n",
        "        self.mask = torch.where(torch.abs(self.weight) >= self.threshold,self.mask,self.zeros).to(device)\n",
        "#     self.mask.requires_grad_(requires_grad=False)\n",
        "    def mask_weight(self):\n",
        "        self.weight = torch.nn.Parameter(self.weight*self.mask).to(device)\n",
        "    \n",
        "class MaskedLinear(torch.nn.Linear, Masked):\n",
        "    def __init__(self, in_features, out_features, bias=True, threshold=0.001, mask=None):\n",
        "        super(MaskedLinear, self).__init__(in_features,out_features)\n",
        "        self.make_mask(threshold,mask)\n",
        "    def forward(self, input):\n",
        "        self.make_threshold_mask()\n",
        "        self.mask_weight()\n",
        "        #     print(self.mask[125:135,125:135])\n",
        "        #     print(self.weight[125:135,125:135])\n",
        "        return F.linear(input, self.weight, self.bias)\n",
        "\n",
        "class MaskedConv(torch.nn.Conv2d, Masked):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, groups, bias=True, threshold=0.0001):\n",
        "        super(MaskedConv,self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.make_mask(threshold)    \n",
        "    def forward(self, input):\n",
        "        self.mask_weight()\n",
        "        return F.conv2d(input, self.weight, self.bias, self.stride,\n",
        "                    self.padding, self.dilation, self.groups)\n",
        "\n",
        "limit_a, limit_b, epsilon = -.1, 1.1, 1e-6\n",
        "device='cuda'\n",
        "\n",
        "class LinearL0(Module):\n",
        "    \"\"\"Implementation of L0 regularization for the input units of a fully connected layer\"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True, weight_decay=1., droprate_init=0.5, temperature=2./3.,\n",
        "                 lamba=1., local_rep=False, **kwargs):\n",
        "        \"\"\"\n",
        "        :param in_features: Input dimensionality\n",
        "        :param out_features: Output dimensionality\n",
        "        :param bias: Whether we use a bias\n",
        "        :param weight_decay: Strength of the L2 penalty\n",
        "        :param droprate_init: Dropout rate that the L0 gates will be initialized to\n",
        "        :param temperature: Temperature of the concrete distribution\n",
        "        :param lamba: Strength of the L0 penalty\n",
        "        :param local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
        "        \"\"\"\n",
        "        super(LinearL0, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.prior_prec = weight_decay\n",
        "        self.weights = torch.nn.Parameter(torch.Tensor(in_features, out_features).to(device))\n",
        "        #         self.qz_loga = torch.Tensor(in_features).to(device)\n",
        "        self.qz_loga = torch.nn.Parameter(torch.Tensor(in_features).to(device))\n",
        "        self.temperature = temperature\n",
        "        self.droprate_init = droprate_init if droprate_init != 0. else 0.5\n",
        "        self.lamba = lamba\n",
        "        self.use_bias = False\n",
        "        self.local_rep = local_rep\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
        "            self.use_bias = True\n",
        "        self.floatTensor = torch.FloatTensor if not torch.cuda.is_available() else torch.cuda.FloatTensor\n",
        "        self.reset_parameters()\n",
        "        print(self)\n",
        "        \n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.kaiming_normal(self.weights, mode='fan_out')\n",
        "\n",
        "        self.qz_loga.data.normal_(math.log(1 - self.droprate_init) - math.log(self.droprate_init), 1e-2)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias.data.fill_(0)\n",
        "\n",
        "    def constrain_parameters(self, **kwargs):\n",
        "        self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
        "\n",
        "    def cdf_qz(self, x):\n",
        "        \"\"\"Implements the CDF of the 'stretched' concrete distribution\"\"\"\n",
        "        xn = (x - limit_a) / (limit_b - limit_a)\n",
        "        logits = math.log(xn) - math.log(1 - xn)\n",
        "        return F.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=epsilon, max=1 - epsilon).to(device)\n",
        "\n",
        "    def quantile_concrete(self, x):\n",
        "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "        y = F.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature).to(device)\n",
        "        return y * (limit_b - limit_a) + limit_a\n",
        "\n",
        "    def _reg_w(self):\n",
        "        \"\"\"Expected L0 norm under the stochastic gates, takes into account and re-weights also a potential L2 penalty\"\"\"\n",
        "        logpw_col = torch.sum(- (.5 * self.prior_prec * self.weights.pow(2)) - self.lamba, 1).to(device)\n",
        "        logpw = torch.sum((1 - self.cdf_qz(0)) * logpw_col).to(device)\n",
        "        logpb = 0 if not self.use_bias else - torch.sum(.5 * self.prior_prec * self.bias.pow(2)).to(device)\n",
        "        return logpw + logpb\n",
        "        \n",
        "        \n",
        "    def regularization(self):\n",
        "        return self._reg_w()\n",
        "\n",
        "    def count_expected_flops_and_l0(self):\n",
        "        \"\"\"Measures the expected floating point operations (FLOPs) and the expected L0 norm\"\"\"\n",
        "        # dim_in multiplications and dim_in - 1 additions for each output neuron for the weights\n",
        "        # + the bias addition for each neuron\n",
        "        # total_flops = (2 * in_features - 1) * out_features + out_features\n",
        "        ppos = torch.sum(1 - self.cdf_qz(0))\n",
        "        expected_flops = (2 * ppos - 1) * self.out_features\n",
        "        expected_l0 = ppos * self.out_features\n",
        "        if self.use_bias:\n",
        "            expected_flops += self.out_features\n",
        "            expected_l0 += self.out_features\n",
        "#       return expected_flops.data[0], expected_l0.data[0]\n",
        "        return expected_flops, expected_l0\n",
        "\n",
        "    def get_eps(self, size):\n",
        "        \"\"\"Uniform random numbers for the concrete distribution\"\"\"\n",
        "        eps = self.floatTensor(size).uniform_(epsilon, 1-epsilon).to(device)\n",
        "        eps = Variable(eps)\n",
        "        return eps\n",
        "\n",
        "    def sample_z(self, batch_size, sample=True):\n",
        "        \"\"\"Sample the hard-concrete gates for training and use a deterministic value for testing\"\"\"\n",
        "        if sample:\n",
        "            eps = self.get_eps(self.floatTensor(batch_size, self.in_features).to(device))\n",
        "            z = self.quantile_concrete(eps)\n",
        "            return F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "        else:  # mode\n",
        "            pi = F.sigmoid(self.qz_loga).view(1, self.in_features).expand(batch_size, self.in_features).to(device)\n",
        "            return F.hardtanh(pi * (limit_b - limit_a) + limit_a, min_val=0, max_val=1).to(device)\n",
        "        \n",
        "    def sample_weights(self):\n",
        "        z = self.quantile_concrete(self.get_eps(self.floatTensor(self.in_features).to(device)))\n",
        "        mask = F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "        return mask.view(self.in_features, 1) * self.weights\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.local_rep or not self.training:\n",
        "            z = self.sample_z(input.size(0), sample=self.training)\n",
        "            xin = input.mul(z)\n",
        "            output = xin.mm(self.weights)\n",
        "        else:\n",
        "            weights = self.sample_weights()\n",
        "            output = input.mm(weights)\n",
        "        if self.use_bias:\n",
        "            output.add_(self.bias)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = ('{name}({in_features} -> {out_features}, droprate_init={droprate_init}, '\n",
        "            'lamba={lamba}, temperature={temperature}, weight_decay={prior_prec}, '\n",
        "            'local_rep={local_rep}')\n",
        "        if not self.use_bias:\n",
        "            s += ', bias=False'\n",
        "        s += ')'\n",
        "        return s.format(name=self.__class__.__name__, **self.__dict__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ikIYcgE-FbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class L0Conv2d(Module):\n",
        "    \"\"\"Implementation of L0 regularization for the feature maps of a convolutional layer\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,\n",
        "                 droprate_init=0.5, temperature=2./3., weight_decay=1., lamba=1., local_rep=False, **kwargs):\n",
        "        \"\"\"\n",
        "        :param in_channels: Number of input channels\n",
        "        :param out_channels: Number of output channels\n",
        "        :param kernel_size: Size of the kernel\n",
        "        :param stride: Stride for the convolution\n",
        "        :param padding: Padding for the convolution\n",
        "        :param dilation: Dilation factor for the convolution\n",
        "        :param groups: How many groups we will assume in the convolution\n",
        "        :param bias: Whether we will use a bias\n",
        "        :param droprate_init: Dropout rate that the L0 gates will be initialized to\n",
        "        :param temperature: Temperature of the concrete distribution\n",
        "        :param weight_decay: Strength of the L2 penalty\n",
        "        :param lamba: Strength of the L0 penalty\n",
        "        :param local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
        "        \"\"\"\n",
        "        super(L0Conv2d, self).__init__()\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = pair(kernel_size)\n",
        "        self.stride = pair(stride)\n",
        "        self.padding = pair(padding)\n",
        "        self.dilation = pair(dilation)\n",
        "        self.output_padding = pair(0)\n",
        "        self.groups = groups\n",
        "        self.prior_prec = weight_decay\n",
        "        self.lamba = lamba\n",
        "        self.droprate_init = droprate_init if droprate_init != 0. else 0.5\n",
        "        self.temperature = temperature\n",
        "        self.floatTensor = torch.FloatTensor if not torch.cuda.is_available() else torch.cuda.FloatTensor\n",
        "        self.use_bias = False\n",
        "        self.weights = Parameter(torch.Tensor(out_channels, in_channels // groups, *self.kernel_size).to(device))\n",
        "        self.qz_loga = Parameter(torch.Tensor(out_channels).to(device))\n",
        "        self.dim_z = out_channels\n",
        "        self.input_shape = None\n",
        "        self.local_rep = local_rep\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels).to(device))\n",
        "            self.use_bias = True\n",
        "\n",
        "        self.reset_parameters()\n",
        "        print(self)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_normal(self.weights, mode='fan_in')\n",
        "\n",
        "        self.qz_loga.data.normal_(math.log(1 - self.droprate_init) - math.log(self.droprate_init), 1e-2)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias.data.fill_(0)\n",
        "\n",
        "    def constrain_parameters(self, **kwargs):\n",
        "        self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
        "\n",
        "    def cdf_qz(self, x):\n",
        "        \"\"\"Implements the CDF of the 'stretched' concrete distribution\"\"\"\n",
        "        xn = (x - limit_a) / (limit_b - limit_a)\n",
        "        logits = math.log(xn) - math.log(1 - xn)\n",
        "        return F.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=epsilon, max=1 - epsilon)\n",
        "\n",
        "    def quantile_concrete(self, x):\n",
        "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "        y = F.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature)\n",
        "        return y * (limit_b - limit_a) + limit_a\n",
        "\n",
        "    def _reg_w(self):\n",
        "        \"\"\"Expected L0 norm under the stochastic gates, takes into account and re-weights also a potential L2 penalty\"\"\"\n",
        "        q0 = self.cdf_qz(0)\n",
        "        logpw_col = torch.sum(- (.5 * self.prior_prec * self.weights.pow(2)) - self.lamba, 3).sum(2).sum(1)\n",
        "        logpw = torch.sum((1 - q0) * logpw_col).to(device)\n",
        "        logpb = 0 if not self.use_bias else - torch.sum((1 - q0) * (.5 * self.prior_prec * self.bias.pow(2) -\n",
        "                                                                    self.lamba))\n",
        "        return logpw + logpb\n",
        "\n",
        "    def regularization(self):\n",
        "        return self._reg_w()\n",
        "\n",
        "    def count_expected_flops_and_l0(self):\n",
        "        \"\"\"Measures the expected floating point operations (FLOPs) and the expected L0 norm\"\"\"\n",
        "        ppos = torch.sum(1 - self.cdf_qz(0))\n",
        "        n = self.kernel_size[0] * self.kernel_size[1] * self.in_channels  # vector_length\n",
        "        flops_per_instance = n + (n - 1)  # (n: multiplications and n-1: additions)\n",
        "\n",
        "        num_instances_per_filter = ((self.input_shape[1] - self.kernel_size[0] + 2 * self.padding[0]) / self.stride[0]) + 1  # for rows\n",
        "        num_instances_per_filter *= ((self.input_shape[2] - self.kernel_size[1] + 2 * self.padding[1]) / self.stride[1]) + 1  # multiplying with cols\n",
        "\n",
        "        flops_per_filter = num_instances_per_filter * flops_per_instance\n",
        "        expected_flops = flops_per_filter * ppos  # multiply with number of filters\n",
        "        expected_l0 = n * ppos\n",
        "\n",
        "        if self.use_bias:\n",
        "            # since the gate is applied to the output we also reduce the bias computation\n",
        "            expected_flops += num_instances_per_filter * ppos\n",
        "            expected_l0 += ppos\n",
        "\n",
        "#         return expected_flops.data[0], expected_l0.data[0]\n",
        "        return expected_flops, expected_l0\n",
        "\n",
        "    def get_eps(self, size):\n",
        "        \"\"\"Uniform random numbers for the concrete distribution\"\"\"\n",
        "        eps = self.floatTensor(size).uniform_(epsilon, 1-epsilon).to(device)\n",
        "        eps = Variable(eps)\n",
        "        return eps\n",
        "\n",
        "    def sample_z(self, batch_size, sample=True):\n",
        "        \"\"\"Sample the hard-concrete gates for training and use a deterministic value for testing\"\"\"\n",
        "        if sample:\n",
        "            eps = self.get_eps(self.floatTensor(batch_size, self.dim_z)).to(device)\n",
        "            z = self.quantile_concrete(eps).view(batch_size, self.dim_z, 1, 1)\n",
        "            return F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "        else:  # mode\n",
        "            pi = F.sigmoid(self.qz_loga).view(1, self.dim_z, 1, 1)\n",
        "            return F.hardtanh(pi * (limit_b - limit_a) + limit_a, min_val=0, max_val=1).to(device)\n",
        "\n",
        "    def sample_weights(self):\n",
        "        z = self.quantile_concrete(self.get_eps(self.floatTensor(self.dim_z).to(device))).view(self.dim_z, 1, 1, 1)\n",
        "        return F.hardtanh(z, min_val=0, max_val=1).to(device) * self.weights\n",
        "\n",
        "    def forward(self, input_):\n",
        "        if self.input_shape is None:\n",
        "            self.input_shape = input_.size()\n",
        "        b = None if not self.use_bias else self.bias\n",
        "        if self.local_rep or not self.training:\n",
        "            output = F.conv2d(input_, self.weights, b, self.stride, self.padding, self.dilation, self.groups)\n",
        "            z = self.sample_z(output.size(0), sample=self.training)\n",
        "            return output.mul(z)\n",
        "        else:\n",
        "            weights = self.sample_weights()\n",
        "            output = F.conv2d(input_, weights, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = ('{name}({in_channels}, {out_channels}, kernel_size={kernel_size}, stride={stride}, '\n",
        "             'droprate_init={droprate_init}, temperature={temperature}, prior_prec={prior_prec}, '\n",
        "             'lamba={lamba}, local_rep={local_rep}')\n",
        "        if self.padding != (0,) * len(self.padding):\n",
        "            s += ', padding={padding}'\n",
        "        if self.dilation != (1,) * len(self.dilation):\n",
        "            s += ', dilation={dilation}'\n",
        "        if self.output_padding != (0,) * len(self.output_padding):\n",
        "            s += ', output_padding={output_padding}'\n",
        "        if self.groups != 1:\n",
        "            s += ', groups={groups}'\n",
        "        if not self.use_bias:\n",
        "            s += ', bias=False'\n",
        "        s += ')'\n",
        "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jyVxLH0h5HEQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mask_network(network, layers_to_mask, threshold=0.002, linear_masking=None, random_init=False, bias=True, masks=None):\n",
        "    \"\"\"\"\n",
        "    replaces linear layers with masked linear layers\n",
        "    replaces conv layers with masked conv layers\n",
        "    network is the initial sequential container\n",
        "    layers is a list of layers to mask\n",
        "    random init is a logical indicating whether to preserve the initial weights or to modify them\n",
        "    \"\"\"\n",
        "\n",
        "    network.masked_layers=[]\n",
        "    ii = -1\n",
        "    for layer, (name, module) in enumerate(network._modules.items()):\n",
        "        \n",
        "            ii += 1\n",
        "            \n",
        "            layer_mask = None\n",
        "            if masks is not None:\n",
        "                if name in masks:\n",
        "                    layer_mask = masks.get(name)      \n",
        "            if isinstance(module, torch.nn.Linear) and linear_masking is None:\n",
        "                masked_layer = MaskedLinear(layer.in_features, layer.out_features, bias=bias,threshold=threshold,mask=layer_mask)\n",
        "            elif isinstance(module, torch.nn.Linear) and linear_masking =='L0':\n",
        "                _, layer = list(network._modules.items())[ii]\n",
        "                masked_layer = LinearL0(layer.in_features, layer.out_features, bias=bias, lamba=0.1/640)\n",
        "                network.masked_layers.append(masked_layer)\n",
        "            elif isinstance(module, torch.nn.Conv2d):\n",
        "                _, layer = list(network._modules.items())[ii]\n",
        "                masked_layer = L0Conv2d(layer.in_channels, layer.out_channels, layer.kernel_size, layer.stride, layer.padding, layer.dilation, layer.groups, bias=bias, \n",
        "                                        droprate_init=0.5, temperature=2./3., weight_decay=1., lamba=0.1/640, local_rep=False)\n",
        "                network.masked_layers.append(masked_layer)\n",
        "            else:\n",
        "                continue\n",
        "            if random_init != True:\n",
        "                masked_layer.weight = copy.deepcopy(layer.weight)\n",
        "                masked_layer.bias = copy.deepcopy(layer.bias)\n",
        "                \n",
        "            network[int(name)] = masked_layer\n",
        "            \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YbnaQGqI6PAy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VGG_L0(tv_vgg.VGG):\n",
        "    def regularization(self):\n",
        "        regularization = 0.\n",
        "        for layer in self.layers:\n",
        "            regularization += - (1. / self.N) * layer.regularization()\n",
        "#         if torch.cuda.is_available():\n",
        "#             regularization = regularization.cuda()\n",
        "        return regularization\n",
        "    \n",
        "    def regularize(self, N):\n",
        "        regularization = 0.\n",
        "        for layer in self.masked_layers:\n",
        "            regularization += - (1. / N) * layer.regularization()          \n",
        "#         if torch.cuda.is_available():\n",
        "#             regularization = regularization.cuda()\n",
        "        return regularization\n",
        "\n",
        "    def clamp_parameters(self):\n",
        "        for layer in self.masked_layers:\n",
        "            layer.constrain_parameters()\n",
        "    \n",
        "    def get_exp_flops_l0(self):\n",
        "        total_flops = 0\n",
        "        total_l0 = 0\n",
        "#         print(self.masked_layers)\n",
        "        for layer in self.masked_layers:\n",
        "            exp_flops, exp_l0 = layer.count_expected_flops_and_l0()\n",
        "            total_flops += exp_flops\n",
        "            total_l0 += exp_l0\n",
        "        return total_flops, total_l0\n",
        "    \n",
        "    \n",
        "def vgg16_L0(pretrained=False, **kwargs):\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\")\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGG_L0(tv_vgg.make_layers(tv_vgg.cfg['D']), **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(tv_vgg.model_urls['vgg16']))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quImFgeUDM1U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def freeze_layers(model_ft, exclude=[]):\n",
        "#   children = list(model_ft.named_children())\n",
        "    for name,param in model_ft.named_parameters():   \n",
        "        if(name not in exclude):\n",
        "            param.requires_grad = False\n",
        "        \n",
        "def CountNonZeroWeights(model):\n",
        "    \n",
        "    total = 0\n",
        "    zeros = 0\n",
        "    nonzeros = 0\n",
        "    \n",
        "    tol = 1.0e-15\n",
        "    \n",
        "    for name, param in model.named_parameters():\n",
        "        if param is not None:\n",
        "            \n",
        "            param = torch.zeros(3,3)\n",
        "#             print(torch.min(torch.abs(param)))\n",
        "            nonzeros += torch.sum((param != 0.0).int())\n",
        "            zeros += torch.sum((param == 0.0).int())\n",
        "\n",
        "    total = zeros + nonzeros\n",
        "    \n",
        "    print(\"Total number of weights: {}\".format(total))\n",
        "    print(\"Number of non-zero weights: {}\".format(nonzeros))\n",
        "    print(\"Number of zero weights: {}\".format(zeros))\n",
        "    \n",
        "    return total, nonzeros, zeros\n",
        "\n",
        "\n",
        "def set_threshold(model, prop=0.05):\n",
        "    for child in model.named_children():    \n",
        "        for child in child[1].named_children():\n",
        "#       print(child)\n",
        "            if type(child[1]) == MaskedLinear or type(child[1]) == MaskedConv: \n",
        "                child[1].set_threshold(prop=prop)\n",
        "                print(\"layer {}  new threshold {:.4f}\".format(child[0], child[1].threshold))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nclNqiyMDzp9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model_prune(model, dloaders, dataset_sizes, criterion, optimizer, scheduler,prop=0.05, num_epochs=25, device='cuda',pruning='threshold'):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    print(len(dloaders['train']))\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "                data_idx = 0\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                data_idx = 1\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            i=0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dloaders[phase]:               \n",
        "#                 print(\"batch {} phase {}\".format(i, phase))\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    if pruning == 'L0':\n",
        "                        loss = criterion(outputs, labels, model)\n",
        "                    else:\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        model.clamp_parameters()\n",
        "                        exp_flops, exp_l0 = model.get_exp_flops_l0()\n",
        "                i+=1\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                           \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            if epoch % 5 == 0 and phase == 'train': \n",
        "                CountNonZeroWeights(model)\n",
        "                \n",
        "                if pruning == 'threshold':\n",
        "                    set_threshold(model,prop=prop)\n",
        "                elif pruning == 'L0':\n",
        "                    print(\"Expected flops: {} | Expected L0 norm: {}\".format(exp_flops.item(), exp_l0.item()))\n",
        "            \n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YC2Z_YkP68mR",
        "colab_type": "code",
        "outputId": "6caa2539-a91b-484a-c126-436e73fd1516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2490
        }
      },
      "cell_type": "code",
      "source": [
        "def run_normal_training_with_L0_pruning(this_trainset):\n",
        "#     print(this_trainset.__len__())  \n",
        "    _,mytrainset = torch.utils.data.random_split(this_trainset, (49200, 800))\n",
        "    # _,trainset = torch.utils.data.random_split(trainset,(49995,5))\n",
        "#     print(mytrainset.__len__())\n",
        "\n",
        "    mytrain_data, myval_data = torch.utils.data.random_split(mytrainset,(int(0.8*len(mytrainset)),int(0.2*len(mytrainset))))\n",
        "#     print(mytrain_data.__len__(),myval_data.__len__() )\n",
        "\n",
        "    mytrainloader = torch.utils.data.DataLoader(mytrain_data, batch_size=5,\n",
        "                                                shuffle=True, num_workers=0)\n",
        "    myvalloader = torch.utils.data.DataLoader(myval_data, batch_size=5,\n",
        "                                              shuffle=True, num_workers=0)\n",
        "    mydataloaders = {'train': mytrainloader, 'val': myvalloader}\n",
        "    image_datasets = {'train': mytrain_data,'val': myval_data}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}  \n",
        "\n",
        "    model_ft = vgg16_L0(pretrained=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    # ------ Mask classifier layers ------\n",
        "    \n",
        "#     freeze_layers(model_ft.features, exclude=[])\n",
        "    mask_network(model_ft.classifier, [0,3,6], linear_masking=\"L0\")\n",
        "    model_ft.masked_layers = model_ft.classifier.masked_layers\n",
        "    \n",
        "    # ------ Mask conv2D layers ------\n",
        "    \n",
        "    mask_network(model_ft.features, np.arange(0, 30), linear_masking=\"L0\")\n",
        "    model_ft.masked_layers.extend(model_ft.features.masked_layers)\n",
        "\n",
        "    print(\"Number of masked layers: {}\".format(len(model_ft.masked_layers)))\n",
        "\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    def loss_function(outputs, targets, model):\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss += model.regularize(640)\n",
        "        return loss\n",
        "    \n",
        "    # Observe that all parameters are being optimized\n",
        "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "#   [print(p) for p in model_ft.parameters()]\n",
        "#   return\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "    model_ft = train_model_prune(model_ft, mydataloaders, dataset_sizes, loss_function, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=20, pruning=\"L0\")\n",
        "\n",
        "    \n",
        "    \n",
        "transform = transforms.Compose(\n",
        "    [transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=5,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "\n",
        "run_normal_training_with_L0_pruning(trainset)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:85: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LinearL0(25088 -> 4096, droprate_init=0.5, lamba=0.00015625, temperature=0.6666666666666666, weight_decay=1.0, local_rep=False)\n",
            "LinearL0(4096 -> 4096, droprate_init=0.5, lamba=0.00015625, temperature=0.6666666666666666, weight_decay=1.0, local_rep=False)\n",
            "LinearL0(4096 -> 1000, droprate_init=0.5, lamba=0.00015625, temperature=0.6666666666666666, weight_decay=1.0, local_rep=False)\n",
            "L0Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "L0Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "Number of masked layers: 16\n",
            "128\n",
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 52.2036 Acc: 0.0766\n",
            "Total number of weights: 276739174\n",
            "Number of non-zero weights: 276739174\n",
            "Number of zero weights: 2\n",
            "Expected flops: 248126439424.0 | Expected L0 norm: 115086080.0\n",
            "val Loss: 51.8063 Acc: 0.0750\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 51.4088 Acc: 0.0984\n",
            "val Loss: 50.9114 Acc: 0.0750\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 50.4069 Acc: 0.1172\n",
            "val Loss: 49.7403 Acc: 0.0938\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 49.1767 Acc: 0.1125\n",
            "val Loss: 48.4951 Acc: 0.0938\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 48.2310 Acc: 0.1109\n",
            "val Loss: 47.8873 Acc: 0.1125\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 47.8426 Acc: 0.0938\n",
            "Total number of weights: 276739175\n",
            "Number of non-zero weights: 276739175\n",
            "Number of zero weights: 1\n",
            "Expected flops: 248030838784.0 | Expected L0 norm: 115060672.0\n",
            "val Loss: 47.6872 Acc: 0.1063\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 47.6933 Acc: 0.1141\n",
            "val Loss: 47.5875 Acc: 0.0938\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 47.6173 Acc: 0.1109\n",
            "val Loss: 47.5792 Acc: 0.0938\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 47.6042 Acc: 0.1203\n",
            "val Loss: 47.5707 Acc: 0.0938\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 47.6161 Acc: 0.0859\n",
            "val Loss: 47.5639 Acc: 0.0938\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 47.6012 Acc: 0.1156\n",
            "Total number of weights: 276739175\n",
            "Number of non-zero weights: 276739175\n",
            "Number of zero weights: 1\n",
            "Expected flops: 248004214784.0 | Expected L0 norm: 115053424.0\n",
            "val Loss: 47.5555 Acc: 0.0938\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 47.5762 Acc: 0.1109\n",
            "val Loss: 47.5472 Acc: 0.0938\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 47.5781 Acc: 0.0953\n",
            "val Loss: 47.5411 Acc: 0.0938\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 47.5591 Acc: 0.1281\n",
            "val Loss: 47.5335 Acc: 0.0938\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 47.5638 Acc: 0.1094\n",
            "val Loss: 47.5329 Acc: 0.0938\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 47.5705 Acc: 0.0875\n",
            "Total number of weights: 276739175\n",
            "Number of non-zero weights: 276739175\n",
            "Number of zero weights: 1\n",
            "Expected flops: 247998119936.0 | Expected L0 norm: 115051768.0\n",
            "val Loss: 47.5321 Acc: 0.0938\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 47.5581 Acc: 0.1000\n",
            "val Loss: 47.5314 Acc: 0.0938\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 47.5531 Acc: 0.1188\n",
            "val Loss: 47.5307 Acc: 0.0938\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 47.5589 Acc: 0.1063\n",
            "val Loss: 47.5300 Acc: 0.0938\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 47.5574 Acc: 0.1125\n",
            "val Loss: 47.5293 Acc: 0.0938\n",
            "\n",
            "Training complete in 10m 15s\n",
            "Best val Acc: 0.112500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0tfmvixSCzAH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Setup Routines"
      ]
    },
    {
      "metadata": {
        "id": "fF2ZhlgCC31h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline Model Setup"
      ]
    },
    {
      "metadata": {
        "id": "GeD0hZfpDHbr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_baseline = models.vgg16(pretrained=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_baseline = model_baseline.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model_baseline.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lc0YTkyADA63",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruned Model Setup"
      ]
    },
    {
      "metadata": {
        "id": "cr4wgE6eDKND",
        "colab_type": "code",
        "outputId": "0f2c5d59-5f0f-4a76-918f-858c3ab87e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "cell_type": "code",
      "source": [
        "# Test pruning all layers\n",
        "\n",
        "model_pruned = models.vgg16(pretrained=True)\n",
        "model_pruned.train()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_pruned = model_pruned.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model_pruned.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(28, 10, N_prune = 4, p = 2, pruning_metric = WEIGHT_NORM)\n",
        "\n",
        "t0 = time.time()\n",
        "model_pruned = PruneAllConv2DLayers(model_pruned, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c16a3b839ee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel_pruned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPruneAllConv2DLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pruned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Pruning took {} s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-68787035a1e4>\u001b[0m in \u001b[0;36mPruneAllConv2DLayers\u001b[0;34m(model, prune_settings)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mprune_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_prune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN_prune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mprune_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorms_botk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#                 n_botk, ind_botk = torch.topk(torch.from_numpy(prune_settings.norms_botk[ii]), N_prune, 0, largest=False, sorted=True, out=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "coPfbZmXE3T0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Driver Routines"
      ]
    },
    {
      "metadata": {
        "id": "DS7qbZ0JE6Fs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Iterative Pruning"
      ]
    },
    {
      "metadata": {
        "id": "E1q-h_KTE-kE",
        "colab_type": "code",
        "outputId": "137304bb-f00e-4bce-d2a3-2dee86be9489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16032
        }
      },
      "cell_type": "code",
      "source": [
        "# ====== Dataset setup ======\n",
        "\n",
        "percent_data = 5.0\n",
        "percent_val = 20.0\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "# ====== Model setup ======\n",
        "\n",
        "model = models.vgg16(pretrained=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# ====== Pruning setup ======\n",
        "\n",
        "N_prune = 0\n",
        "P_prune = 5\n",
        "p = 2\n",
        "prune_settings = UnitPruningSettings(N_prune=N_prune, \n",
        "                                     P_prune=P_prune, \n",
        "                                     p=p, \n",
        "                                     pruning_metric=ACT_NORM)\n",
        "prune_settings.Setup(model)\n",
        "\n",
        "\n",
        "# ====== Begin training ======\n",
        "\n",
        "N_iter_outer = 7\n",
        "N_iter_inner = 7\n",
        "\n",
        "dataset = 'cifar10'\n",
        "\n",
        "datasets = ('cifar10',\\\n",
        "            'cifar100',\\\n",
        "            'mnist',\\\n",
        "            'fashionmnist',\\\n",
        "            'kmnist',\\\n",
        "            'emnist',\\\n",
        "            'stl10')\n",
        "\n",
        "# Import data\n",
        "dat = DatasetManager(dataset=dataset, \n",
        "                     percent_data=percent_data, \n",
        "                     percent_val=percent_val)\n",
        "\n",
        "dat.ImportDataset(batch_size=batch_size)\n",
        "\n",
        "for ii in range(0, N_iter_outer):\n",
        "    \n",
        "    print(\"\\n------ Outer iteration {}/{} ------\".format(ii+1, N_iter_outer))\n",
        "#     t0 = time.time()\n",
        "\n",
        "    # ------ Prune current model ------\n",
        "        \n",
        "    model = PruneAllConv2DLayers(model, prune_settings)\n",
        "    new_model = copy.deepcopy(model)\n",
        "    model = new_model\n",
        "    prune_settings.PrintPruningStatistics(1)\n",
        "    prune_settings.ResetNormContainers()\n",
        "\n",
        "    # ------ Train current model ------\n",
        "    \n",
        "    # Import data\n",
        "    dataset = datasets[ii%7]\n",
        "    print(\"Using dataset {}\".format(dataset))\n",
        "    dat = DatasetManager(dataset=dataset, percent_data=percent_data, percent_val=percent_val)\n",
        "    dat.ImportDataset(batch_size=batch_size)\n",
        "    \n",
        "    # Update optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, prune_settings, num_epochs=N_iter_inner)\n",
        "        \n",
        "#     print (\"Pruning took {} s\".format(time.time() - t0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/170498071 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 169869312/170498071 [00:11<00:00, 7661424.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "\n",
            "------ Outer iteration 1/7 ------\n",
            "Total number of filters before pruning: 4224.0\n",
            "Total number of filters after pruning: 4224.0\n",
            "Using dataset cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r170500096it [00:30, 7661424.33it/s]                               "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 3.1424 Acc: 0.0885\n",
            "val Loss: 2.4477 Acc: 0.1480\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 2.3992 Acc: 0.1255\n",
            "val Loss: 2.2612 Acc: 0.1480\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 2.2709 Acc: 0.1535\n",
            "val Loss: 2.2943 Acc: 0.1580\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n",
            "train Loss: 2.1709 Acc: 0.2005\n",
            "val Loss: 2.0808 Acc: 0.2460\n",
            "\n",
            "Epoch 5/7\n",
            "----------\n",
            "train Loss: 2.1488 Acc: 0.2070\n",
            "val Loss: 2.1248 Acc: 0.2220\n",
            "\n",
            "Epoch 6/7\n",
            "----------\n",
            "train Loss: 2.1182 Acc: 0.2165\n",
            "val Loss: 2.0884 Acc: 0.2440\n",
            "\n",
            "Epoch 7/7\n",
            "----------\n",
            "train Loss: 2.0756 Acc: 0.2355\n",
            "val Loss: 1.9938 Acc: 0.2620\n",
            "\n",
            "Training complete in 9m 55s\n",
            "Best val Acc: 0.262000\n",
            "\n",
            "------ Outer iteration 2/7 ------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "  0%|          | 0/169001437 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total number of filters before pruning: 4224.0\n",
            "Total number of filters after pruning: 4020.0\n",
            "Using dataset cifar100\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar100/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 221184/169001437 [00:00<01:17, 2176064.65it/s]\u001b[A\n",
            "  1%|          | 1761280/169001437 [00:00<00:57, 2924044.69it/s]\u001b[A\n",
            "  1%|         | 2334720/169001437 [00:00<00:48, 3409267.63it/s]\u001b[A\n",
            "  2%|         | 3170304/169001437 [00:00<00:40, 4135807.84it/s]\u001b[A\n",
            "  2%|         | 4169728/169001437 [00:00<00:32, 5008937.57it/s]\u001b[A\n",
            "  3%|         | 5185536/169001437 [00:00<00:27, 5864098.86it/s]\u001b[A\n",
            "  4%|         | 6201344/169001437 [00:00<00:24, 6697877.88it/s]\u001b[A\n",
            "  4%|         | 7061504/169001437 [00:00<00:23, 6766390.91it/s]\u001b[A\n",
            "  5%|         | 7872512/169001437 [00:01<00:26, 6113976.90it/s]\u001b[A\n",
            "  5%|         | 8593408/169001437 [00:01<00:27, 5906976.93it/s]\u001b[A\n",
            "  5%|         | 9265152/169001437 [00:01<00:28, 5681418.15it/s]\u001b[A\n",
            "  6%|         | 9887744/169001437 [00:01<00:33, 4775474.65it/s]\u001b[A\n",
            "  6%|         | 10428416/169001437 [00:01<00:39, 3973210.83it/s]\u001b[A\n",
            "  6%|         | 10895360/169001437 [00:01<00:44, 3514481.27it/s]\u001b[A\n",
            "  7%|         | 11304960/169001437 [00:02<00:47, 3298274.15it/s]\u001b[A\n",
            "  7%|         | 11681792/169001437 [00:02<00:48, 3211064.71it/s]\u001b[A\n",
            "  7%|         | 12034048/169001437 [00:02<00:51, 3044652.97it/s]\u001b[A\n",
            "  7%|         | 12361728/169001437 [00:02<00:55, 2802649.76it/s]\u001b[A\n",
            "  7%|         | 12664832/169001437 [00:02<00:58, 2669601.90it/s]\u001b[A\n",
            "  8%|         | 12951552/169001437 [00:02<01:02, 2485505.75it/s]\u001b[A\n",
            "  8%|         | 13213696/169001437 [00:02<01:13, 2110156.69it/s]\u001b[A\n",
            "  8%|         | 13451264/169001437 [00:03<01:23, 1852085.41it/s]\u001b[A\n",
            "  8%|         | 13656064/169001437 [00:03<01:29, 1731152.75it/s]\u001b[A\n",
            "  8%|         | 13844480/169001437 [00:03<01:30, 1714117.81it/s]\u001b[A\n",
            "  8%|         | 14032896/169001437 [00:03<01:31, 1699846.16it/s]\u001b[A\n",
            "  8%|         | 14229504/169001437 [00:03<01:28, 1758236.65it/s]\u001b[A\n",
            "  9%|         | 14442496/169001437 [00:03<01:24, 1833762.29it/s]\u001b[A\n",
            "  9%|         | 14655488/169001437 [00:03<01:21, 1905428.69it/s]\u001b[A\n",
            "  9%|         | 14884864/169001437 [00:03<01:17, 1997551.86it/s]\u001b[A\n",
            "  9%|         | 15130624/169001437 [00:03<01:13, 2084307.37it/s]\u001b[A\n",
            "  9%|         | 15360000/169001437 [00:04<01:12, 2121848.81it/s]\u001b[A\n",
            "  9%|         | 15581184/169001437 [00:04<01:15, 2043974.75it/s]\u001b[A\n",
            "  9%|         | 15794176/169001437 [00:04<01:17, 1978433.82it/s]\u001b[A\n",
            "  9%|         | 15998976/169001437 [00:04<01:17, 1971191.33it/s]\u001b[A\n",
            " 10%|         | 16211968/169001437 [00:04<01:16, 2009229.36it/s]\u001b[A\n",
            " 10%|         | 16441344/169001437 [00:04<01:13, 2076145.82it/s]\u001b[A\n",
            " 10%|         | 16687104/169001437 [00:04<01:11, 2140472.28it/s]\u001b[A\n",
            " 10%|         | 16949248/169001437 [00:04<01:08, 2227421.18it/s]\u001b[A\n",
            " 10%|         | 17211392/169001437 [00:04<01:05, 2320132.96it/s]\u001b[A\n",
            " 10%|         | 17489920/169001437 [00:04<01:02, 2431983.60it/s]\u001b[A\n",
            " 11%|         | 17784832/169001437 [00:05<01:00, 2519026.58it/s]\u001b[A\n",
            " 11%|         | 18096128/169001437 [00:05<00:57, 2611093.74it/s]\u001b[A\n",
            " 11%|         | 18423808/169001437 [00:05<00:55, 2720579.63it/s]\u001b[A\n",
            " 11%|         | 18751488/169001437 [00:05<00:52, 2858899.11it/s]\u001b[A\n",
            " 11%|        | 19095552/169001437 [00:05<00:51, 2916904.38it/s]\u001b[A\n",
            " 12%|        | 19456000/169001437 [00:05<00:48, 3062456.04it/s]\u001b[A\n",
            " 12%|        | 19832832/169001437 [00:05<00:46, 3195884.46it/s]\u001b[A\n",
            " 12%|        | 20209664/169001437 [00:05<00:44, 3312921.81it/s]\u001b[A\n",
            " 12%|        | 20602880/169001437 [00:05<00:43, 3405741.03it/s]\u001b[A\n",
            " 12%|        | 20996096/169001437 [00:06<00:41, 3531393.74it/s]\u001b[A\n",
            " 13%|        | 21405696/169001437 [00:06<00:40, 3623908.75it/s]\u001b[A\n",
            " 13%|        | 21831680/169001437 [00:06<00:39, 3749632.31it/s]\u001b[A\n",
            " 13%|        | 22274048/169001437 [00:06<00:38, 3836408.77it/s]\u001b[A\n",
            " 13%|        | 22716416/169001437 [00:06<00:36, 3977659.73it/s]\u001b[A\n",
            " 14%|        | 23175168/169001437 [00:06<00:35, 4101992.08it/s]\u001b[A\n",
            " 14%|        | 23650304/169001437 [00:06<00:34, 4258274.75it/s]\u001b[A\n",
            " 14%|        | 24125440/169001437 [00:06<00:33, 4384110.96it/s]\u001b[A\n",
            " 15%|        | 24616960/169001437 [00:06<00:32, 4505445.82it/s]\u001b[A\n",
            " 15%|        | 25124864/169001437 [00:06<00:31, 4555172.90it/s]\u001b[A\n",
            " 15%|        | 25649152/169001437 [00:07<00:30, 4683436.59it/s]\u001b[A\n",
            " 15%|        | 26189824/169001437 [00:07<00:29, 4819297.78it/s]\u001b[A\n",
            " 16%|        | 26730496/169001437 [00:07<00:28, 4930633.12it/s]\u001b[A\n",
            " 16%|        | 27271168/169001437 [00:07<00:28, 5038197.60it/s]\u001b[A\n",
            " 16%|        | 27779072/169001437 [00:07<00:28, 4898484.56it/s]\u001b[A\n",
            " 17%|        | 28278784/169001437 [00:07<00:30, 4567979.83it/s]\u001b[A\n",
            " 17%|        | 28745728/169001437 [00:07<00:35, 3932650.26it/s]\u001b[A\n",
            " 17%|        | 29163520/169001437 [00:07<00:39, 3560077.65it/s]\u001b[A\n",
            " 17%|        | 29540352/169001437 [00:08<00:39, 3518631.66it/s]\u001b[A\n",
            " 18%|        | 29908992/169001437 [00:08<00:41, 3334634.39it/s]\u001b[A\n",
            " 18%|        | 30269440/169001437 [00:08<00:41, 3372736.47it/s]\u001b[A\n",
            " 18%|        | 30629888/169001437 [00:08<00:40, 3395227.50it/s]\u001b[A\n",
            " 18%|        | 31006720/169001437 [00:08<00:39, 3478426.96it/s]\u001b[A\n",
            " 19%|        | 31383552/169001437 [00:08<00:38, 3544950.31it/s]\u001b[A\n",
            " 19%|        | 31793152/169001437 [00:08<00:37, 3653294.94it/s]\u001b[A\n",
            " 19%|        | 32186368/169001437 [00:08<00:36, 3732413.01it/s]\u001b[A\n",
            " 19%|        | 32612352/169001437 [00:08<00:35, 3829308.20it/s]\u001b[A\n",
            " 20%|        | 33038336/169001437 [00:08<00:34, 3935509.78it/s]\u001b[A\n",
            " 20%|        | 33480704/169001437 [00:09<00:33, 4048234.01it/s]\u001b[A\n",
            " 20%|        | 33939456/169001437 [00:09<00:32, 4174736.65it/s]\u001b[A\n",
            " 20%|        | 34414592/169001437 [00:09<00:32, 4185494.93it/s]\u001b[A\n",
            " 21%|        | 34906112/169001437 [00:09<00:31, 4200272.67it/s]\u001b[A\n",
            " 21%|        | 35414016/169001437 [00:09<00:30, 4319794.09it/s]\u001b[A\n",
            " 21%|       | 35921920/169001437 [00:09<00:29, 4464787.97it/s]\u001b[A\n",
            " 22%|       | 36429824/169001437 [00:09<00:28, 4610297.15it/s]\u001b[A\n",
            " 22%|       | 36954112/169001437 [00:09<00:27, 4752691.76it/s]\u001b[A\n",
            " 22%|       | 37494784/169001437 [00:09<00:26, 4872408.72it/s]\u001b[A\n",
            " 23%|       | 38035456/169001437 [00:10<00:26, 4995727.08it/s]\u001b[A\n",
            " 23%|       | 38592512/169001437 [00:10<00:25, 5112369.52it/s]\u001b[A\n",
            " 23%|       | 39165952/169001437 [00:10<00:24, 5252538.51it/s]\u001b[A\n",
            " 24%|       | 39755776/169001437 [00:10<00:23, 5396527.08it/s]\u001b[A\n",
            " 24%|       | 40361984/169001437 [00:10<00:23, 5441208.58it/s]\u001b[A\n",
            " 24%|       | 40984576/169001437 [00:10<00:22, 5593022.26it/s]\u001b[A\n",
            " 25%|       | 41607168/169001437 [00:10<00:22, 5748403.38it/s]\u001b[A\n",
            " 25%|       | 42246144/169001437 [00:10<00:21, 5849651.41it/s]\u001b[A\n",
            " 25%|       | 42885120/169001437 [00:10<00:21, 5917201.17it/s]\u001b[A\n",
            " 26%|       | 43556864/169001437 [00:10<00:20, 6053330.08it/s]\u001b[A\n",
            " 26%|       | 44228608/169001437 [00:11<00:20, 6203240.85it/s]\u001b[A\n",
            " 27%|       | 44908544/169001437 [00:11<00:19, 6368373.80it/s]\u001b[A\n",
            " 27%|       | 45588480/169001437 [00:11<00:19, 6466348.00it/s]\u001b[A\n",
            " 27%|       | 46292992/169001437 [00:11<00:18, 6594412.88it/s]\u001b[A\n",
            " 28%|       | 46997504/169001437 [00:11<00:18, 6688440.32it/s]\u001b[A\n",
            " 28%|       | 47669248/169001437 [00:11<00:18, 6591318.68it/s]\u001b[A\n",
            " 29%|       | 48357376/169001437 [00:11<00:18, 6664357.53it/s]\u001b[A\n",
            " 29%|       | 49078272/169001437 [00:11<00:17, 6781352.08it/s]\u001b[A\n",
            " 29%|       | 49848320/169001437 [00:11<00:17, 6917044.13it/s]\u001b[A\n",
            " 30%|       | 50634752/169001437 [00:12<00:16, 7093312.07it/s]\u001b[A\n",
            " 30%|       | 51437568/169001437 [00:12<00:16, 7098256.99it/s]\u001b[A\n",
            " 31%|       | 52224000/169001437 [00:12<00:16, 7273543.97it/s]\u001b[A\n",
            " 31%|      | 53043200/169001437 [00:12<00:15, 7376431.43it/s]\u001b[A\n",
            " 32%|      | 53862400/169001437 [00:12<00:15, 7567936.86it/s]\u001b[A\n",
            " 32%|      | 54697984/169001437 [00:12<00:14, 7676372.64it/s]\u001b[A\n",
            " 33%|      | 55549952/169001437 [00:12<00:14, 7795064.85it/s]\u001b[A\n",
            " 33%|      | 56401920/169001437 [00:12<00:14, 7949479.35it/s]\u001b[A\n",
            " 34%|      | 57253888/169001437 [00:12<00:13, 8019750.98it/s]\u001b[A\n",
            " 34%|      | 58138624/169001437 [00:12<00:13, 8195111.20it/s]\u001b[A\n",
            " 35%|      | 59039744/169001437 [00:13<00:13, 8256423.88it/s]\u001b[A\n",
            " 35%|      | 59940864/169001437 [00:13<00:12, 8452155.26it/s]\u001b[A\n",
            " 36%|      | 60792832/169001437 [00:13<00:13, 8172960.80it/s]\u001b[A\n",
            " 36%|      | 61620224/169001437 [00:13<00:14, 7578117.36it/s]\u001b[A\n",
            " 37%|      | 62390272/169001437 [00:13<00:14, 7406926.11it/s]\u001b[A\n",
            " 37%|      | 63143936/169001437 [00:13<00:15, 7022003.68it/s]\u001b[A\n",
            " 38%|      | 63856640/169001437 [00:13<00:15, 6966947.73it/s]\u001b[A\n",
            " 38%|      | 64561152/169001437 [00:13<00:14, 6965901.19it/s]\u001b[A\n",
            " 39%|      | 65282048/169001437 [00:13<00:14, 7029116.99it/s]\u001b[A\n",
            " 39%|      | 66019328/169001437 [00:14<00:14, 7092060.36it/s]\u001b[A\n",
            " 40%|      | 66772992/169001437 [00:14<00:14, 7169491.72it/s]\u001b[A\n",
            " 40%|      | 67543040/169001437 [00:14<00:13, 7281150.92it/s]\u001b[A\n",
            " 40%|      | 68329472/169001437 [00:14<00:13, 7404110.70it/s]\u001b[A\n",
            " 41%|      | 69115904/169001437 [00:14<00:13, 7400236.08it/s]\u001b[A\n",
            " 41%|     | 69918720/169001437 [00:14<00:13, 7544597.85it/s]\u001b[A\n",
            " 42%|     | 70737920/169001437 [00:14<00:12, 7569868.10it/s]\u001b[A\n",
            " 42%|     | 71573504/169001437 [00:14<00:12, 7736802.16it/s]\u001b[A\n",
            " 43%|     | 72392704/169001437 [00:14<00:12, 7824851.37it/s]\u001b[A\n",
            " 43%|     | 73228288/169001437 [00:14<00:12, 7935113.73it/s]\u001b[A\n",
            " 44%|     | 74080256/169001437 [00:15<00:11, 8007281.93it/s]\u001b[A\n",
            " 44%|     | 74932224/169001437 [00:15<00:11, 8124329.11it/s]\u001b[A\n",
            " 45%|     | 75800576/169001437 [00:15<00:11, 8106711.23it/s]\u001b[A\n",
            " 45%|     | 76685312/169001437 [00:15<00:11, 8257977.21it/s]\u001b[A\n",
            " 46%|     | 77553664/169001437 [00:15<00:10, 8348651.06it/s]\u001b[A\n",
            " 46%|     | 78438400/169001437 [00:15<00:10, 8473047.10it/s]\u001b[A\n",
            " 47%|     | 79355904/169001437 [00:15<00:10, 8624490.71it/s]\u001b[A\n",
            " 47%|     | 80224256/169001437 [00:15<00:10, 8233535.17it/s]\u001b[A\n",
            " 48%|     | 81059840/169001437 [00:15<00:10, 8231265.81it/s]\u001b[A\n",
            " 48%|     | 81887232/169001437 [00:16<00:11, 7633606.00it/s]\u001b[A\n",
            " 49%|     | 82665472/169001437 [00:16<00:12, 7193169.70it/s]\u001b[A\n",
            " 49%|     | 83402752/169001437 [00:16<00:12, 7003431.78it/s]\u001b[A\n",
            " 50%|     | 84115456/169001437 [00:16<00:12, 7011164.52it/s]\u001b[A\n",
            " 50%|     | 84844544/169001437 [00:16<00:12, 7012075.48it/s]\u001b[A\n",
            " 51%|     | 85598208/169001437 [00:16<00:11, 7117445.05it/s]\u001b[A\n",
            " 51%|     | 86351872/169001437 [00:16<00:11, 7208101.16it/s]\u001b[A\n",
            " 52%|    | 87121920/169001437 [00:16<00:11, 7314971.67it/s]\u001b[A\n",
            " 52%|    | 87891968/169001437 [00:16<00:10, 7391118.02it/s]\u001b[A\n",
            " 52%|    | 88694784/169001437 [00:16<00:10, 7381509.31it/s]\u001b[A\n",
            " 53%|    | 89497600/169001437 [00:17<00:10, 7517938.60it/s]\u001b[A\n",
            " 53%|    | 90316800/169001437 [00:17<00:10, 7595333.48it/s]\u001b[A\n",
            " 54%|    | 91152384/169001437 [00:17<00:10, 7690312.65it/s]\u001b[A\n",
            " 54%|    | 91987968/169001437 [00:17<00:09, 7844061.82it/s]\u001b[A\n",
            " 55%|    | 92823552/169001437 [00:17<00:09, 7957243.80it/s]\u001b[A\n",
            " 55%|    | 93675520/169001437 [00:17<00:09, 8080496.39it/s]\u001b[A\n",
            " 56%|    | 94511104/169001437 [00:17<00:09, 8120073.53it/s]\u001b[A\n",
            " 56%|    | 95379456/169001437 [00:17<00:08, 8242306.07it/s]\u001b[A\n",
            " 57%|    | 96247808/169001437 [00:17<00:08, 8330378.97it/s]\u001b[A\n",
            " 57%|    | 97148928/169001437 [00:18<00:08, 8271928.24it/s]\u001b[A\n",
            " 58%|    | 98050048/169001437 [00:18<00:08, 8427955.71it/s]\u001b[A\n",
            " 59%|    | 98934784/169001437 [00:18<00:08, 8547369.93it/s]\u001b[A\n",
            " 59%|    | 99852288/169001437 [00:18<00:07, 8667280.74it/s]\u001b[A\n",
            " 60%|    | 100786176/169001437 [00:18<00:07, 8751679.76it/s]\u001b[A\n",
            " 60%|    | 101736448/169001437 [00:18<00:07, 8814881.79it/s]\u001b[A\n",
            " 61%|    | 102686720/169001437 [00:18<00:07, 8971255.08it/s]\u001b[A\n",
            " 61%|   | 103653376/169001437 [00:18<00:07, 9122689.04it/s]\u001b[A\n",
            " 62%|   | 104620032/169001437 [00:18<00:06, 9236275.80it/s]\u001b[A\n",
            " 62%|   | 105619456/169001437 [00:18<00:06, 9405338.37it/s]\u001b[A\n",
            " 63%|   | 106618880/169001437 [00:19<00:06, 9338279.04it/s]\u001b[A\n",
            " 64%|   | 107634688/169001437 [00:19<00:06, 9519940.02it/s]\u001b[A\n",
            " 64%|   | 108650496/169001437 [00:19<00:06, 9644746.24it/s]\u001b[A\n",
            " 65%|   | 109699072/169001437 [00:19<00:06, 9827512.81it/s]\u001b[A\n",
            " 66%|   | 110747648/169001437 [00:19<00:05, 9814478.55it/s]\u001b[A\n",
            " 66%|   | 111796224/169001437 [00:19<00:05, 9945472.51it/s]\u001b[A\n",
            " 67%|   | 112877568/169001437 [00:19<00:05, 10141782.19it/s]\u001b[A\n",
            " 67%|   | 113975296/169001437 [00:19<00:05, 10110199.91it/s]\u001b[A\n",
            " 68%|   | 115089408/169001437 [00:19<00:05, 10174702.29it/s]\u001b[A\n",
            " 69%|   | 116203520/169001437 [00:20<00:05, 10186019.51it/s]\u001b[A\n",
            " 69%|   | 117334016/169001437 [00:20<00:04, 10439988.25it/s]\u001b[A\n",
            " 70%|   | 118464512/169001437 [00:20<00:04, 10569578.96it/s]\u001b[A\n",
            " 71%|   | 119611392/169001437 [00:20<00:04, 10651764.25it/s]\u001b[A\n",
            " 71%|  | 120774656/169001437 [00:20<00:04, 10885443.12it/s]\u001b[A\n",
            " 72%|  | 121921536/169001437 [00:20<00:04, 10990857.26it/s]\u001b[A\n",
            " 73%|  | 123101184/169001437 [00:20<00:04, 11029057.24it/s]\u001b[A\n",
            " 74%|  | 124313600/169001437 [00:20<00:03, 11204753.27it/s]\u001b[A\n",
            " 74%|  | 125526016/169001437 [00:20<00:03, 11333290.61it/s]\u001b[A\n",
            " 75%|  | 126738432/169001437 [00:20<00:03, 11488874.16it/s]\u001b[A\n",
            " 76%|  | 127950848/169001437 [00:21<00:03, 11593422.58it/s]\u001b[A\n",
            " 76%|  | 129196032/169001437 [00:21<00:03, 11519146.70it/s]\u001b[A\n",
            " 77%|  | 130457600/169001437 [00:21<00:03, 11753574.33it/s]\u001b[A\n",
            " 78%|  | 131719168/169001437 [00:21<00:03, 11926827.42it/s]\u001b[A\n",
            " 79%|  | 132997120/169001437 [00:21<00:03, 11848101.55it/s]\u001b[A\n",
            " 79%|  | 134291456/169001437 [00:21<00:02, 12090774.92it/s]\u001b[A\n",
            " 80%|  | 135585792/169001437 [00:21<00:02, 12240982.43it/s]\u001b[A\n",
            " 81%|  | 136896512/169001437 [00:21<00:02, 12436774.73it/s]\u001b[A\n",
            " 82%| | 138190848/169001437 [00:21<00:02, 12518663.73it/s]\u001b[A\n",
            " 83%| | 139517952/169001437 [00:21<00:02, 12668962.81it/s]\u001b[A\n",
            " 83%| | 140877824/169001437 [00:22<00:02, 12662919.54it/s]\u001b[A\n",
            " 84%| | 142270464/169001437 [00:22<00:02, 12868384.34it/s]\u001b[A\n",
            " 85%| | 143663104/169001437 [00:22<00:01, 12917536.11it/s]\u001b[A\n",
            " 86%| | 145055744/169001437 [00:22<00:01, 13139753.05it/s]\u001b[A\n",
            " 87%| | 146415616/169001437 [00:22<00:01, 13198349.69it/s]\u001b[A\n",
            " 87%| | 147742720/169001437 [00:22<00:01, 12793402.47it/s]\u001b[A\n",
            " 88%| | 149028864/169001437 [00:22<00:01, 11749546.68it/s]\u001b[A\n",
            " 89%| | 150224896/169001437 [00:22<00:01, 11586906.96it/s]\u001b[A\n",
            " 90%| | 151404544/169001437 [00:22<00:01, 10833521.95it/s]\u001b[A\n",
            " 90%| | 152510464/169001437 [00:23<00:01, 10752658.08it/s]\u001b[A\n",
            " 91%| | 153600000/169001437 [00:23<00:01, 10708399.00it/s]\u001b[A\n",
            " 92%|| 154681344/169001437 [00:23<00:01, 10608474.58it/s]\u001b[A\n",
            " 92%|| 155754496/169001437 [00:23<00:01, 10594745.58it/s]\u001b[A\n",
            " 93%|| 156901376/169001437 [00:23<00:01, 10711060.17it/s]\u001b[A\n",
            " 94%|| 158064640/169001437 [00:23<00:01, 10801405.64it/s]\u001b[A\n",
            " 94%|| 159211520/169001437 [00:23<00:00, 10919309.13it/s]\u001b[A\n",
            " 95%|| 160407552/169001437 [00:23<00:00, 11004487.73it/s]\u001b[A\n",
            " 96%|| 161603584/169001437 [00:23<00:00, 11210833.12it/s]\u001b[A\n",
            " 96%|| 162816000/169001437 [00:24<00:00, 11156611.90it/s]\u001b[A\n",
            " 97%|| 164061184/169001437 [00:24<00:00, 11446349.36it/s]\u001b[A\n",
            " 98%|| 165306368/169001437 [00:24<00:00, 11656014.06it/s]\u001b[A\n",
            " 99%|| 166551552/169001437 [00:24<00:00, 11807320.77it/s]\u001b[A\n",
            " 99%|| 167813120/169001437 [00:24<00:00, 11725496.65it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2000\n",
            "Active validation set size: 500\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "169009152it [00:43, 11725496.65it/s]                               \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 6.1525 Acc: 0.0120\n",
            "val Loss: 5.1022 Acc: 0.0140\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 4.9636 Acc: 0.0085\n",
            "val Loss: 4.8801 Acc: 0.0080\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 4.8721 Acc: 0.0100\n",
            "val Loss: 4.7921 Acc: 0.0100\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n",
            "train Loss: 4.8028 Acc: 0.0070\n",
            "val Loss: 4.7595 Acc: 0.0120\n",
            "\n",
            "Epoch 5/7\n",
            "----------\n",
            "train Loss: 4.7108 Acc: 0.0185\n",
            "val Loss: 4.6444 Acc: 0.0200\n",
            "\n",
            "Epoch 6/7\n",
            "----------\n",
            "train Loss: 4.6601 Acc: 0.0200\n",
            "val Loss: 4.6024 Acc: 0.0180\n",
            "\n",
            "Epoch 7/7\n",
            "----------\n",
            "train Loss: 4.5937 Acc: 0.0225\n",
            "val Loss: 4.5943 Acc: 0.0160\n",
            "\n",
            "Training complete in 9m 34s\n",
            "Best val Acc: 0.020000\n",
            "\n",
            "------ Outer iteration 3/7 ------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total number of filters before pruning: 4020.0\n",
            "Total number of filters after pruning: 3822.0\n",
            "Using dataset mnist\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  1%|          | 98304/9912422 [00:00<00:11, 832258.88it/s]\u001b[A\u001b[A\n",
            "\n",
            " 16%|        | 1548288/9912422 [00:00<00:07, 1157093.26it/s]\u001b[A\u001b[A\n",
            "\n",
            " 98%|| 9674752/9912422 [00:00<00:00, 1642959.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "9920512it [00:00, 19543512.59it/s]                            \u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57%|    | 16384/28881 [00:00<00:00, 162600.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "32768it [00:00, 312846.17it/s]                           \u001b[A\u001b[A\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  1%|          | 16384/1648877 [00:00<00:13, 117022.70it/s]\u001b[A\u001b[A\n",
            "\n",
            " 26%|       | 425984/1648877 [00:00<00:07, 164841.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "1654784it [00:00, 5266439.65it/s]                           \u001b[A\u001b[A\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "8192it [00:00, 131298.89it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "\n",
            "Full training set size: 60000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2400\n",
            "Active validation set size: 600\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n",
            "train Loss: 2.4581 Acc: 0.0996\n",
            "val Loss: 2.3891 Acc: 0.1017\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 2.3922 Acc: 0.0925\n",
            "val Loss: 2.3326 Acc: 0.1017\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 2.3718 Acc: 0.1058\n",
            "val Loss: 2.3385 Acc: 0.1283\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n",
            "train Loss: 2.3697 Acc: 0.1054\n",
            "val Loss: 2.3325 Acc: 0.1183\n",
            "\n",
            "Epoch 5/7\n",
            "----------\n",
            "train Loss: 2.3646 Acc: 0.0963\n",
            "val Loss: 2.3824 Acc: 0.1033\n",
            "\n",
            "Epoch 6/7\n",
            "----------\n",
            "train Loss: 2.3629 Acc: 0.1004\n",
            "val Loss: 2.3456 Acc: 0.1017\n",
            "\n",
            "Epoch 7/7\n",
            "----------\n",
            "train Loss: 2.3577 Acc: 0.1029\n",
            "val Loss: 2.3485 Acc: 0.1017\n",
            "\n",
            "Training complete in 10m 30s\n",
            "Best val Acc: 0.128333\n",
            "\n",
            "------ Outer iteration 4/7 ------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total number of filters before pruning: 3822.0\n",
            "Total number of filters after pruning: 3637.0\n",
            "Using dataset fashionmnist\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/fashionmnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/26421880 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 16384/26421880 [00:00<02:49, 156000.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 49152/26421880 [00:00<02:23, 183362.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 106496/26421880 [00:00<01:54, 228922.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 229376/26421880 [00:00<01:26, 301665.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "  2%|         | 466944/26421880 [00:00<01:03, 407584.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "  4%|         | 950272/26421880 [00:00<00:45, 560914.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "  7%|         | 1835008/26421880 [00:01<00:31, 779005.53it/s]\u001b[A\u001b[A\n",
            "\n",
            " 14%|        | 3678208/26421880 [00:01<00:20, 1092037.34it/s]\u001b[A\u001b[A\n",
            "\n",
            " 26%|       | 6807552/26421880 [00:01<00:12, 1535860.63it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|      | 9945088/26421880 [00:01<00:07, 2146623.35it/s]\u001b[A\u001b[A\n",
            "\n",
            " 49%|     | 13025280/26421880 [00:01<00:04, 2972974.55it/s]\u001b[A\u001b[A\n",
            "\n",
            " 61%|    | 16031744/26421880 [00:01<00:02, 4065639.75it/s]\u001b[A\u001b[A\n",
            "\n",
            " 72%|  | 19136512/26421880 [00:01<00:01, 5483874.67it/s]\u001b[A\u001b[A\n",
            "\n",
            " 84%| | 22167552/26421880 [00:01<00:00, 7242378.35it/s]\u001b[A\u001b[A\n",
            "\n",
            " 96%|| 25280512/26421880 [00:01<00:00, 9361712.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "26427392it [00:01, 14036000.80it/s]                             \u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/fashionmnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/fashionmnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/29515 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 56%|    | 16384/29515 [00:00<00:00, 155177.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "32768it [00:00, 88388.48it/s]                            \u001b[A\u001b[A\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/fashionmnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/fashionmnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/4422102 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 16384/4422102 [00:00<00:28, 153608.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 49152/4422102 [00:00<00:24, 180629.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "  2%|         | 106496/4422102 [00:00<00:19, 225493.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "  5%|         | 229376/4422102 [00:00<00:14, 297141.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 11%|         | 466944/4422102 [00:00<00:09, 401465.84it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|       | 950272/4422102 [00:00<00:06, 552488.24it/s]\u001b[A\u001b[A\n",
            "\n",
            " 42%|     | 1843200/4422102 [00:00<00:03, 767497.60it/s]\u001b[A\u001b[A\n",
            "\n",
            " 84%| | 3702784/4422102 [00:01<00:00, 1076069.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "4423680it [00:01, 4051436.57it/s]                             \u001b[A\u001b[A\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/fashionmnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/fashionmnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/5148 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "8192it [00:00, 29181.58it/s]            \u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/fashionmnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "\n",
            "Full training set size: 60000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2400\n",
            "Active validation set size: 600\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n",
            "train Loss: 2.3768 Acc: 0.1054\n",
            "val Loss: 2.3301 Acc: 0.1067\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 2.3675 Acc: 0.0942\n",
            "val Loss: 2.3270 Acc: 0.1067\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 2.3517 Acc: 0.1075\n",
            "val Loss: 2.3108 Acc: 0.1200\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n",
            "train Loss: 2.1571 Acc: 0.1988\n",
            "val Loss: 2.2797 Acc: 0.1967\n",
            "\n",
            "Epoch 5/7\n",
            "----------\n",
            "train Loss: 1.8908 Acc: 0.3054\n",
            "val Loss: 1.6811 Acc: 0.3733\n",
            "\n",
            "Epoch 6/7\n",
            "----------\n",
            "train Loss: 1.6369 Acc: 0.4008\n",
            "val Loss: 1.5075 Acc: 0.5067\n",
            "\n",
            "Epoch 7/7\n",
            "----------\n",
            "train Loss: 1.4284 Acc: 0.4863\n",
            "val Loss: 1.3001 Acc: 0.5433\n",
            "\n",
            "Training complete in 9m 46s\n",
            "Best val Acc: 0.543333\n",
            "\n",
            "------ Outer iteration 5/7 ------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total number of filters before pruning: 3637.0\n",
            "Total number of filters after pruning: 3458.0\n",
            "Using dataset kmnist\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz to ./data/kmnist/KMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/18165135 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 16384/18165135 [00:00<02:20, 128916.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 40960/18165135 [00:00<02:06, 143068.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 90112/18165135 [00:00<01:42, 176332.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 172032/18165135 [00:00<01:19, 225360.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "  2%|         | 294912/18165135 [00:01<01:01, 292566.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "  2%|         | 417792/18165135 [00:01<00:48, 366424.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "  4%|         | 647168/18165135 [00:01<00:36, 484505.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "  4%|         | 794624/18165135 [00:01<00:29, 586989.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "  5%|         | 958464/18165135 [00:01<00:24, 701090.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "  6%|         | 1122304/18165135 [00:01<00:20, 811954.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "  7%|         | 1286144/18165135 [00:01<00:18, 912734.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "  8%|         | 1458176/18165135 [00:01<00:16, 1010764.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "  9%|         | 1638400/18165135 [00:02<00:14, 1105169.52it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|         | 1818624/18165135 [00:02<00:13, 1182367.56it/s]\u001b[A\u001b[A\n",
            "\n",
            " 11%|         | 2007040/18165135 [00:02<00:12, 1247634.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 12%|        | 2187264/18165135 [00:02<00:12, 1304366.84it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|        | 2375680/18165135 [00:02<00:11, 1351933.75it/s]\u001b[A\u001b[A\n",
            "\n",
            " 14%|        | 2572288/18165135 [00:02<00:11, 1386946.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 15%|        | 2768896/18165135 [00:02<00:10, 1429929.26it/s]\u001b[A\u001b[A\n",
            "\n",
            " 16%|        | 2965504/18165135 [00:02<00:10, 1479852.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 17%|        | 3162112/18165135 [00:03<00:10, 1497749.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19%|        | 3366912/18165135 [00:03<00:09, 1528266.68it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|        | 3571712/18165135 [00:03<00:09, 1550816.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|        | 3776512/18165135 [00:03<00:09, 1567239.27it/s]\u001b[A\u001b[A\n",
            "\n",
            " 22%|       | 3981312/18165135 [00:03<00:08, 1576969.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 23%|       | 4194304/18165135 [00:03<00:08, 1591240.42it/s]\u001b[A\u001b[A\n",
            "\n",
            " 24%|       | 4407296/18165135 [00:03<00:08, 1613810.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 25%|       | 4620288/18165135 [00:04<00:08, 1603021.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 27%|       | 4833280/18165135 [00:04<00:08, 1623051.44it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|       | 4997120/18165135 [00:04<00:11, 1099734.65it/s]\u001b[A\u001b[A\n",
            "\n",
            " 29%|       | 5267456/18165135 [00:04<00:09, 1312333.34it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30%|       | 5439488/18165135 [00:04<00:10, 1268724.72it/s]\u001b[A\u001b[A\n",
            "\n",
            " 31%|       | 5595136/18165135 [00:04<00:10, 1253834.00it/s]\u001b[A\u001b[A\n",
            "\n",
            " 32%|      | 5742592/18165135 [00:04<00:10, 1201923.18it/s]\u001b[A\u001b[A\n",
            "\n",
            " 32%|      | 5881856/18165135 [00:05<00:10, 1206941.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|      | 6012928/18165135 [00:05<00:10, 1118741.25it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34%|      | 6135808/18165135 [00:05<00:11, 1058581.65it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34%|      | 6250496/18165135 [00:05<00:11, 1013909.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35%|      | 6356992/18165135 [00:05<00:12, 978096.10it/s] \u001b[A\u001b[A\n",
            "\n",
            " 36%|      | 6471680/18165135 [00:05<00:12, 971001.99it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|      | 6594560/18165135 [00:05<00:11, 967729.47it/s]\u001b[A\u001b[A\n",
            "\n",
            " 37%|      | 6725632/18165135 [00:05<00:11, 984179.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|      | 6856704/18165135 [00:06<00:11, 995991.17it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|      | 6987776/18165135 [00:06<00:11, 1004204.59it/s]\u001b[A\u001b[A\n",
            "\n",
            " 39%|      | 7118848/18165135 [00:06<00:10, 1009877.53it/s]\u001b[A\u001b[A\n",
            "\n",
            " 40%|      | 7249920/18165135 [00:06<00:10, 1013852.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41%|      | 7380992/18165135 [00:06<00:10, 1016598.82it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41%|     | 7520256/18165135 [00:06<00:10, 1037191.02it/s]\u001b[A\u001b[A\n",
            "\n",
            " 42%|     | 7651328/18165135 [00:06<00:10, 1033460.90it/s]\u001b[A\u001b[A\n",
            "\n",
            " 43%|     | 7782400/18165135 [00:06<00:10, 1030493.00it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|     | 7921664/18165135 [00:07<00:09, 1046711.43it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|     | 8052736/18165135 [00:07<00:09, 1039782.25it/s]\u001b[A\u001b[A\n",
            "\n",
            " 45%|     | 8192000/18165135 [00:07<00:09, 1053789.30it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46%|     | 8331264/18165135 [00:07<00:09, 1063977.88it/s]\u001b[A\u001b[A\n",
            "\n",
            " 47%|     | 8462336/18165135 [00:07<00:09, 1051793.29it/s]\u001b[A\u001b[A\n",
            "\n",
            " 47%|     | 8601600/18165135 [00:07<00:09, 1061827.82it/s]\u001b[A\u001b[A\n",
            "\n",
            " 48%|     | 8740864/18165135 [00:07<00:08, 1069843.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 49%|     | 8871936/18165135 [00:07<00:08, 1066039.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50%|     | 9011200/18165135 [00:08<00:08, 1062409.77it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50%|     | 9150464/18165135 [00:08<00:08, 1070033.58it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51%|     | 9281536/18165135 [00:08<00:08, 1070583.29it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|    | 9412608/18165135 [00:08<00:08, 1057406.37it/s]\u001b[A\u001b[A\n",
            "\n",
            " 53%|    | 9551872/18165135 [00:08<00:08, 1050959.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 53%|    | 9682944/18165135 [00:08<00:07, 1062572.38it/s]\u001b[A\u001b[A\n",
            "\n",
            " 54%|    | 9822208/18165135 [00:08<00:07, 1065910.99it/s]\u001b[A\u001b[A\n",
            "\n",
            " 55%|    | 9961472/18165135 [00:08<00:07, 1069118.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 56%|    | 10100736/18165135 [00:09<00:07, 1063000.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 56%|    | 10231808/18165135 [00:09<00:07, 1070747.68it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57%|    | 10371072/18165135 [00:09<00:07, 1064351.40it/s]\u001b[A\u001b[A\n",
            "\n",
            " 58%|    | 10502144/18165135 [00:09<00:07, 1070610.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|    | 10641408/18165135 [00:09<00:07, 1070227.83it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|    | 10772480/18165135 [00:09<00:06, 1066309.58it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|    | 10911744/18165135 [00:09<00:06, 1066125.49it/s]\u001b[A\u001b[A\n",
            "\n",
            " 61%|    | 11051008/18165135 [00:10<00:06, 1068695.75it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62%|   | 11182080/18165135 [00:10<00:06, 1061539.59it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62%|   | 11321344/18165135 [00:10<00:06, 1070503.28it/s]\u001b[A\u001b[A\n",
            "\n",
            " 63%|   | 11460608/18165135 [00:10<00:06, 1083505.89it/s]\u001b[A\u001b[A\n",
            "\n",
            " 64%|   | 11599872/18165135 [00:10<00:06, 1086362.80it/s]\u001b[A\u001b[A\n",
            "\n",
            " 65%|   | 11747328/18165135 [00:10<00:05, 1087769.58it/s]\u001b[A\u001b[A\n",
            "\n",
            " 65%|   | 11886592/18165135 [00:10<00:05, 1102387.81it/s]\u001b[A\u001b[A\n",
            "\n",
            " 66%|   | 12025856/18165135 [00:10<00:05, 1105757.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|   | 12173312/18165135 [00:11<00:05, 1119837.17it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68%|   | 12320768/18165135 [00:11<00:05, 1130394.28it/s]\u001b[A\u001b[A\n",
            "\n",
            " 69%|   | 12468224/18165135 [00:11<00:05, 1138007.74it/s]\u001b[A\u001b[A\n",
            "\n",
            " 69%|   | 12615680/18165135 [00:11<00:04, 1143605.25it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70%|   | 12771328/18165135 [00:11<00:04, 1160673.85it/s]\u001b[A\u001b[A\n",
            "\n",
            " 71%|   | 12918784/18165135 [00:11<00:04, 1163831.32it/s]\u001b[A\u001b[A\n",
            "\n",
            " 72%|  | 13074432/18165135 [00:11<00:04, 1167353.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 73%|  | 13230080/18165135 [00:11<00:04, 1182702.90it/s]\u001b[A\u001b[A\n",
            "\n",
            " 74%|  | 13385728/18165135 [00:12<00:03, 1208093.65it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|  | 13549568/18165135 [00:12<00:03, 1216736.67it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|  | 13713408/18165135 [00:12<00:03, 1236158.83it/s]\u001b[A\u001b[A\n",
            "\n",
            " 76%|  | 13877248/18165135 [00:12<00:03, 1264586.81it/s]\u001b[A\u001b[A\n",
            "\n",
            " 77%|  | 14049280/18165135 [00:12<00:03, 1288880.46it/s]\u001b[A\u001b[A\n",
            "\n",
            " 78%|  | 14229504/18165135 [00:12<00:03, 1310485.45it/s]\u001b[A\u001b[A\n",
            "\n",
            " 79%|  | 14401536/18165135 [00:12<00:02, 1335586.18it/s]\u001b[A\u001b[A\n",
            "\n",
            " 80%|  | 14581760/18165135 [00:12<00:02, 1357860.99it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81%| | 14761984/18165135 [00:13<00:02, 1373774.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 82%| | 14958592/18165135 [00:13<00:02, 1420091.47it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%| | 15147008/18165135 [00:13<00:02, 1436760.46it/s]\u001b[A\u001b[A\n",
            "\n",
            " 85%| | 15351808/18165135 [00:13<00:01, 1476245.84it/s]\u001b[A\u001b[A\n",
            "\n",
            " 86%| | 15556608/18165135 [00:13<00:01, 1512872.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 87%| | 15769600/18165135 [00:13<00:01, 1556633.39it/s]\u001b[A\u001b[A\n",
            "\n",
            " 88%| | 15982592/18165135 [00:13<00:01, 1588625.78it/s]\u001b[A\u001b[A\n",
            "\n",
            " 89%| | 16203776/18165135 [00:13<00:01, 1629907.38it/s]\u001b[A\u001b[A\n",
            "\n",
            " 90%| | 16433152/18165135 [00:14<00:01, 1669416.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 92%|| 16670720/18165135 [00:14<00:00, 1723222.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 93%|| 16908288/18165135 [00:14<00:00, 1771699.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 94%|| 17162240/18165135 [00:14<00:00, 1822889.43it/s]\u001b[A\u001b[A\n",
            "\n",
            " 96%|| 17416192/18165135 [00:14<00:00, 1876472.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 97%|| 17678336/18165135 [00:14<00:00, 1926620.64it/s]\u001b[A\u001b[A\n",
            "\n",
            " 99%|| 17924096/18165135 [00:14<00:00, 1910432.40it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/kmnist/KMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz to ./data/kmnist/KMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/29497 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|    | 16384/29497 [00:00<00:00, 129164.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "32768it [00:00, 84349.79it/s]                            \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/kmnist/KMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz to ./data/kmnist/KMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/3041136 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  1%|          | 16384/3041136 [00:00<00:23, 128264.69it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  1%|          | 32768/3041136 [00:00<00:23, 128652.55it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  2%|         | 57344/3041136 [00:00<00:20, 142975.64it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  3%|         | 90112/3041136 [00:00<00:17, 164973.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|         | 131072/3041136 [00:00<00:15, 193212.24it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|         | 188416/3041136 [00:01<00:12, 233151.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|         | 253952/3041136 [00:01<00:09, 278929.72it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 11%|        | 344064/3041136 [00:01<00:07, 340928.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 15%|        | 466944/3041136 [00:01<00:06, 423084.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 21%|        | 638976/3041136 [00:01<00:04, 532840.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|       | 843776/3041136 [00:01<00:03, 666497.86it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 35%|      | 1064960/3041136 [00:01<00:02, 816897.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|     | 1286144/3041136 [00:01<00:01, 970716.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|     | 1523712/3041136 [00:02<00:01, 1134167.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 55%|    | 1679360/3041136 [00:02<00:01, 1134109.40it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 65%|   | 1982464/3041136 [00:02<00:00, 1363703.47it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 71%|   | 2162688/3041136 [00:02<00:00, 1379584.59it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|  | 2359296/3041136 [00:02<00:00, 1407927.73it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%| | 2555904/3041136 [00:02<00:00, 1446830.42it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 91%| | 2752512/3041136 [00:02<00:00, 1474234.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 97%|| 2957312/3041136 [00:02<00:00, 1532258.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "3047424it [00:02, 1034506.61it/s]                             \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/kmnist/KMNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz to ./data/kmnist/KMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/5120 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "8192it [00:00, 31457.44it/s]            \u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/kmnist/KMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "\n",
            "Full training set size: 60000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 2400\n",
            "Active validation set size: 600\n",
            "Active test set size: 500\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "18169856it [00:28, 1910432.40it/s]                              \u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 2.3502 Acc: 0.1196\n",
            "val Loss: 2.2378 Acc: 0.1300\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 2.1214 Acc: 0.2467\n",
            "val Loss: 1.9779 Acc: 0.3083\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 1.9983 Acc: 0.3008\n",
            "val Loss: 1.8726 Acc: 0.3533\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v7ORnILPDbVu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ]
    },
    {
      "metadata": {
        "id": "Icv-APyJDdB5",
        "colab_type": "code",
        "outputId": "dd4b57bd-13d6-460a-a99a-bb43bcdc8a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "cell_type": "code",
      "source": [
        "PlotResults(prune_settings, \"pPruneActNorm5_pre_out_7_in_7_cifar10_10percent\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-44129a34e238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPlotResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprune_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pPruneActNorm5_pre_out_7_in_7_cifar10_10percent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'prune_settings' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "UQlUKtFYD5pc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Routines"
      ]
    },
    {
      "metadata": {
        "id": "VASgSU9KD-81",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Baseline Model"
      ]
    },
    {
      "metadata": {
        "id": "yNR6vmDhEBT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model_baseline.train()\n",
        "\n",
        "model_baseline = train_model(model_baseline, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_wR099MzELCI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Pruned Model"
      ]
    },
    {
      "metadata": {
        "id": "0eNeKM87EN6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}