{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metapruning_units.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "tYqrMVdpA1NX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision as tv\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.modules import Module\n",
        "import torchvision.models.vgg as tv_vgg\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.modules.utils import _pair as pair\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BhzbK8ntAmnN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Computation Routines"
      ]
    },
    {
      "metadata": {
        "id": "NuhlcxSIA94G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ]
    },
    {
      "metadata": {
        "id": "YC8SpzkJA8gK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DatasetManager:\n",
        "    \n",
        "    def __init__(self, dataset = 'cifar10', percent_data = 10.0, percent_val = 20.0, data_path = './data'):\n",
        "        \n",
        "        # 'dataset' can be 'hymenoptera', 'cifar10', or 'cifar100'.\n",
        "        # 'percent_data' is the percentage of the full training set to be used.\n",
        "        # 'percent_val' is the percentage of the *loaded* training set to be used as validation data.\n",
        "        \n",
        "        self.dataset = dataset\n",
        "        self.data_path = data_path\n",
        "        self.percent_data = percent_data\n",
        "        self.percent_val = percent_val\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "            \n",
        "        elif self.dataset == 'cifar10' or self.dataset == 'cifar100':\n",
        "\n",
        "            self.transform = tv.transforms.Compose([\n",
        "                tv.transforms.RandomResizedCrop(224),\n",
        "                tv.transforms.RandomHorizontalFlip(),\n",
        "                tv.transforms.ToTensor(),\n",
        "                tv.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def ImportDataset(self, batch_size=5):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        if self.dataset == 'hymenoptera':\n",
        "        \n",
        "            self.trainset = tv.datasets.ImageFolder(root=self.data_path,\n",
        "                             transform=self.transform)\n",
        "        \n",
        "        # todo\n",
        "        \n",
        "        elif self.dataset == 'cifar10':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR10(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR10(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "        \n",
        "        elif self.dataset == 'cifar100':\n",
        "\n",
        "            self.trainset = tv.datasets.CIFAR100(root=self.data_path, train=True,\n",
        "                                        download=True, transform=self.transform)\n",
        "\n",
        "            self.testset = tv.datasets.CIFAR100(root=self.data_path, train=False,\n",
        "                                       download=True, transform=self.transform)\n",
        "             \n",
        "        self.SplitData();\n",
        "        self.GenerateLoaders();\n",
        "                \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def SplitData(self):\n",
        "        \n",
        "        len_full = self.trainset.__len__()\n",
        "        len_train = int(np.round(len_full*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.trainset = torch.utils.data.random_split(self.trainset, (len_full-len_train, len_train))\n",
        "        \n",
        "        len_val = int(np.round(len_train*self.percent_val/100.0))\n",
        "        len_train = len_train - len_val\n",
        "        \n",
        "        self.valset, self.trainset = torch.utils.data.random_split(self.trainset, (len_val, len_train))\n",
        "         \n",
        "        len_full_test = self.testset.__len__()\n",
        "        len_test = int(np.round(len_full_test*self.percent_data/100.0))\n",
        "        \n",
        "        _, self.testset = torch.utils.data.random_split(self.testset, (len_full_test-len_test, len_test))\n",
        "\n",
        "        print('\\nFull training set size: {}'.format(len_full))\n",
        "        print('Full test set size: {}'.format(len_full_test))\n",
        "        print('\\nActive training set size: {}'.format(len_train))\n",
        "        print('Active validation set size: {}'.format(len_val))\n",
        "        print('Active test set size: {}\\n'.format(len_test))\n",
        "        \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def GenerateLoaders(self):\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.val_loader = torch.utils.data.DataLoader(self.valset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=self.batch_size,\n",
        "                                          shuffle=True, num_workers=0)          \n",
        "            \n",
        "        return\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUJWD_oDBKHD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training functions"
      ]
    },
    {
      "metadata": {
        "id": "34G6XlaFBMUW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def RecordLosses(phase, epoch_loss, epoch_acc, prune_settings):\n",
        "    \n",
        "    # Record losses for later use, plotting etc\n",
        "    if phase == 'train':\n",
        "        prune_settings.epoch_loss.append(epoch_loss)\n",
        "        prune_settings.epoch_acc.append(epoch_acc)\n",
        "    elif phase == 'val':\n",
        "        prune_settings.val_loss.append(epoch_loss)\n",
        "        prune_settings.val_acc.append(epoch_acc)\n",
        "\n",
        "    return prune_settings\n",
        "\n",
        "\n",
        "def PlotResults(prune_settings):\n",
        "    \n",
        "    # ====== Plot ======\n",
        "\n",
        "    # ------ Loss ------\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(1, len(prune_settings.epoch_loss)+1), \n",
        "             prune_settings.epoch_loss, \n",
        "             color='red', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Epoch loss')\n",
        "    plt.plot(np.arange(1, len(prune_settings.val_loss)+1), \n",
        "             prune_settings.val_loss, \n",
        "             color='blue', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Validation loss')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "\n",
        "    # ------ Accuracy ------\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(1, len(prune_settings.epoch_acc)+1), \n",
        "             np.asarray(prune_settings.epoch_acc)*100.0, \n",
        "             color='red', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label='Epoch accuracy')\n",
        "    plt.plot(np.arange(1, len(prune_settings.val_acc)+1), \n",
        "             np.asarray(prune_settings.val_acc)*100.0, \n",
        "             color='blue', \n",
        "             marker='',  markersize=12, \n",
        "             linestyle='-', linewidth=2,\n",
        "             label = 'Validation accuracy')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def SaveResults(prune_settings, model):\n",
        "    \n",
        "    \n",
        "    \n",
        "    return\n",
        "\n",
        "\n",
        "def train_model(model, dat, criterion, optimizer, scheduler, prune_settings=0, num_epochs=25):\n",
        "    \n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                dataloader = dat.train_loader\n",
        "                dataset_size = dat.trainset.__len__()\n",
        "                \n",
        "                model.train()  # Set model to training mode\n",
        "                \n",
        "            else:\n",
        "                \n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                dataloader = dat.val_loader\n",
        "                dataset_size = dat.valset.__len__()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloader:\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                if prune_settings != 0:\n",
        "                    TrackConv2DNorms(model, prune_settings, inputs)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if training\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                \n",
        "            # Record losses for later use, plotting etc\n",
        "            prune_settings = RecordLosses(phase, epoch_loss, epoch_acc, prune_settings)\n",
        "\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # Record losses for later use, plotting etc\n",
        "    prune_settings.outer_iter_time.append(time_elapsed)\n",
        "    \n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z7C3UubBBXFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruning functions"
      ]
    },
    {
      "metadata": {
        "id": "MaSebsFceMRD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning settings"
      ]
    },
    {
      "metadata": {
        "id": "jAZVF5O-BZNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Constants that define possible pruning metrics\n",
        "WEIGHT_NORM = 1\n",
        "ACT_NORM = 2\n",
        "\n",
        "# Class that contains various settings pertaining to how filters are pruned\n",
        "class UnitPruningSettings:\n",
        "    \n",
        "    def __init__(self, idx_layer=0, idx_filter=0, N_prune=1, P_prune=10, p=2, pruning_metric=WEIGHT_NORM):\n",
        "        \n",
        "        # EITHER N_prune OR P_prune will be used to decide how many filters to prune.\n",
        "        # If one is non-positive, the other is used.\n",
        "        # If neither is non-positive, priority is given to P_prune.\n",
        "        # If both are non-positive, no pruning will happen.\n",
        "\n",
        "        self.N_prune = N_prune # Number of filters allowed to be pruned in one pass\n",
        "        self.P_prune = P_prune; # Percent of filters of the current layer to prune\n",
        "        \n",
        "        self.idx_filter = idx_filter # Indices of the N_prune filters\n",
        "        self.idx_layer = idx_layer # Current layer under consideration\n",
        "        self.p = p # p-norm to use when computing which filters to remove\n",
        "        self.pruning_metric = pruning_metric\n",
        "        \n",
        "        self.norms_botk = []\n",
        "        self.idx_norms_botk = []\n",
        "        \n",
        "        # Various statistics will be stored and computed to keep track of how the network changes\n",
        "        \n",
        "        # Number of filters per layer in the original network\n",
        "        self.filters_per_layer_orig = []\n",
        "        \n",
        "        # Number of filters per layer after pruning - this gets updated every time the network is pruned\n",
        "        self.filters_per_layer_after = []\n",
        "        \n",
        "        # Time taken to prune in sec (running total, updated every time pruning happens)\n",
        "        self.prune_time = 0.0\n",
        "        self.outer_iter_time = []\n",
        "        \n",
        "        # Keep track of running epoch loss and validation loss, and corresponding accuracy\n",
        "        self.epoch_loss = []\n",
        "        self.val_loss = []\n",
        "        self.epoch_acc = []\n",
        "        self.val_acc = []\n",
        "        \n",
        "        return\n",
        "    \n",
        "    # Function to print the current pruning state of the model. Verbose can be 0, 1, or 2.\n",
        "    def PrintPruningStatistics(self, verbose=1):\n",
        "    \n",
        "        if verbose == 0:\n",
        "            return\n",
        "        \n",
        "        print(\"Total number of filters before pruning: {}\".format(sum(self.filters_per_layer_orig)))\n",
        "        print(\"Total number of filters after pruning: {}\".format(sum(self.filters_per_layer_after)))\n",
        "    \n",
        "        return\n",
        "    \n",
        "    # Function to set up and initialize based on a given model\n",
        "    def Setup(self, model):\n",
        "        \n",
        "        # Count the number of conv layers\n",
        "        self.N_layers = 0\n",
        "        \n",
        "        for layer, (name, module) in enumerate(model.features._modules.items()):\n",
        "            self.N_layers += 1\n",
        "                    \n",
        "        # Initialize storage containers\n",
        "        self.norms_botk = [None]*self.N_layers\n",
        "        self.idx_norms_botk = [None]*self.N_layers\n",
        "        \n",
        "    \n",
        "    # Function to reset norm containers\n",
        "    def ResetNormContainers(self):\n",
        "    \n",
        "        self.norms_botk = [None]*self.N_layers\n",
        "        self.idx_norms_botk = [None]*self.N_layers\n",
        "    \n",
        "        return\n",
        "\n",
        "    # Function to reset filter containers\n",
        "    def ResetFilterContainers(self):\n",
        "    \n",
        "        self.filters_per_layer_orig = []\n",
        "        self.filters_per_layer_after = []\n",
        "    \n",
        "        return\n",
        "\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qc41Ng14ezag",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning decisions"
      ]
    },
    {
      "metadata": {
        "id": "2F0n6VUTe2ND",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to compute the p-norm of weights in all filters of a given layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DWeightNorms(model, idx_layer, p):\n",
        "    \n",
        "    # Extract the layer of the model currently being considered\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    weights = conv.weight.data\n",
        "\n",
        "    # Compute norms of each filter\n",
        "    norms = weights.norm(p, dim=2).norm(p, dim=2).norm(p, dim=1)\n",
        "    \n",
        "    return norms\n",
        "\n",
        "\n",
        "# Function to compute the p-norm of activations in all filters per layer.\n",
        "# The list of norms are returned in a list in the same order as that in which filters of that layer are stored.\n",
        "def ComputeConv2DActNorms(activation, prune_settings):\n",
        "    \n",
        "    p = prune_settings.p\n",
        "    \n",
        "    # Compute norms of each activation\n",
        "    norms = torch.norm(activation, p, dim=0).norm(p, dim=1).norm(p, dim=1)\n",
        "        \n",
        "    return norms\n",
        "\n",
        "\n",
        "# Function to track the p-norm of activations of all filters of during training.\n",
        "def TrackConv2DNorms(model, prune_settings, inputs):\n",
        "    \n",
        "    p = prune_settings.p\n",
        "    P_prune = prune_settings.P_prune\n",
        "\n",
        "    x = Variable(inputs)\n",
        "\n",
        "    ii = -1\n",
        "    for layer, (name, module) in enumerate(model.features._modules.items()):\n",
        "        ii += 1\n",
        "        x = module(x)\n",
        "        \n",
        "        if isinstance(module, torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            if prune_settings.pruning_metric == WEIGHT_NORM:\n",
        "                norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "            elif prune_settings.pruning_metric == ACT_NORM:\n",
        "                norms = ComputeConv2DActNorms(x, prune_settings)\n",
        "\n",
        "            # Use the given prune percentage to figure out how many filters to prune\n",
        "            if (P_prune >= 0):\n",
        "                N_prune = int(len(norms.float())*P_prune/100.0)\n",
        "                prune_settings.N_prune = N_prune\n",
        "\n",
        "#             n_botk, ind_botk = torch.topk(norms, N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            norms = norms.cpu().detach().numpy()\n",
        "    \n",
        "            # Store the norms for each filter\n",
        "#             if prune_settings.norms_botk[ii] is None:\n",
        "#                 prune_settings.norms_botk[ii] = norms\n",
        "#             else:\n",
        "#                 prune_settings.norms_botk[ii] += norms\n",
        "                \n",
        "            # Store normalized norms for each filter\n",
        "            if prune_settings.norms_botk[ii] is None:\n",
        "                prune_settings.norms_botk[ii] = norms/max(norms)\n",
        "            else:\n",
        "                prune_settings.norms_botk[ii] += norms/max(norms)\n",
        "                \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U59vXkzfehDX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning workers"
      ]
    },
    {
      "metadata": {
        "id": "RY5GZWxXej1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The following functions were adapted from https://github.com/jacobgil/pytorch-pruning/blob/master/prune.py\n",
        "\n",
        "def replace_layers(model, i, idx, layers):\n",
        "\tif i in idx:\n",
        "\t\treturn layers[idx.index(i)]\n",
        "\treturn model[i]\n",
        "\n",
        "\n",
        "# Function to prune a given convolution layer in the model provided.\n",
        "# Input \"idx_layers\" is the global index of the convolution layer to be pruned.\n",
        "# Input \"prune_settings\" is a data structure containing information on how pruning is performed.\n",
        "def PruneConvLayers(model, prune_settings):\n",
        "    \n",
        "    # Strategy: in order to prune a particular layer, the output of the previous layer \n",
        "    # and the inputs to the next layer must also be altered accordingly.\n",
        "\t\n",
        "    # Extract pruning settings for convenience\n",
        "    N_prune = prune_settings.N_prune\n",
        "    idx_filter = prune_settings.idx_filter\n",
        "    idx_layer = prune_settings.idx_layer\n",
        "    \n",
        "    if idx_layer >= len(model.features._modules.items()):\n",
        "        return\n",
        "        \n",
        "    # Extract the layer of the model currently being pruned\n",
        "    _, conv = list(model.features._modules.items())[idx_layer]\n",
        "    \n",
        "\n",
        "    # In case the list of target filters to delete has out-of-range entries, detect and ignore them\n",
        "    del_filters = []\n",
        "    for kk in range(0, len(idx_filter)):\n",
        "        if idx_filter[kk] >= conv.out_channels:\n",
        "            del_filters.extend(kk)\n",
        "    \n",
        "    if (len(del_filters) > 0):\n",
        "        idx_filter = np.delete(idx_filter, del_filters, 0)\n",
        "        N_prune = len(idx_filter)\n",
        "        prune_settings.N_prune = N_prune\n",
        "        print(\"[WARNING] Encountered an out-of-range target filter; it will be ignored.\")\n",
        "    \n",
        "    # Record pruning statistics\n",
        "    prune_settings.filters_per_layer_orig[idx_layer] = conv.out_channels\n",
        "    prune_settings.filters_per_layer_after[idx_layer] = conv.out_channels - N_prune\n",
        "    \n",
        "        \n",
        "    # To keep track of the succeeding convolution layer\n",
        "    next_conv = None\n",
        "    offset = 1\n",
        "    \n",
        "    # Figure out how many layers after this one are NOT conv layers, in order to skip pruning them\n",
        "    while idx_layer + offset < len(model.features._modules.items()):\n",
        "        \n",
        "        res =  list(model.features._modules.items())[idx_layer + offset]\n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            next_name, next_conv = res\n",
        "            break\n",
        "        offset = offset + 1\n",
        "    \n",
        "    # Create a new, replacement conv layer to remove a given number of filters.\n",
        "    # The rest of its settings should remain the same as the original conv layer.\n",
        "    new_conv = torch.nn.Conv2d(in_channels = conv.in_channels,\n",
        "                               out_channels = conv.out_channels - N_prune,\n",
        "\t\t\t                   kernel_size = conv.kernel_size,\n",
        "                               stride = conv.stride,\n",
        "                               padding = conv.padding,\n",
        "                               dilation = conv.dilation,\n",
        "                               groups = conv.groups,\n",
        "                               bias = True)\n",
        "    \n",
        "    new_conv.bias = conv.bias\n",
        "    \n",
        "    # Copy over the weights to the new conv layer, except the ones corresponding to the filter to be removed\n",
        "    old_weights = conv.weight.data.cpu().numpy()\n",
        "    new_weights = new_conv.weight.data.cpu().numpy()\n",
        "    \n",
        "    # Copy over the set of filters, excluding the ones to be removed\n",
        "    new_weights_temp = np.copy(old_weights)\n",
        "    new_weights_temp = np.delete(new_weights_temp, idx_filter, 0)\n",
        "    new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "    # Update weight data of the new conv layer\n",
        "    new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "    \n",
        "    # Now do the same thing for biases\n",
        "    old_biases = conv.bias.data.cpu().numpy()\n",
        "    new_biases = np.zeros(shape=(old_biases.shape[0] - N_prune), dtype=np.float32)\n",
        "    \n",
        "    new_biases_temp = np.copy(old_biases)\n",
        "    new_biases_temp = np.delete(new_biases_temp, idx_filter, 0)\n",
        "    new_biases[:] = new_biases_temp[:]\n",
        "        \n",
        "    new_conv.bias.data = torch.from_numpy(new_biases).cuda()\n",
        "    \n",
        "    # If there is a succeeding conv layer, adjust its input units and weights accordingly\n",
        "    if next_conv != None:\n",
        "        \n",
        "        next_new_conv = torch.nn.Conv2d(in_channels = next_conv.in_channels - N_prune,\n",
        "                                        out_channels =  next_conv.out_channels,\n",
        "                                        kernel_size = next_conv.kernel_size,\n",
        "                                        stride = next_conv.stride,\n",
        "                                        padding = next_conv.padding,\n",
        "                                        dilation = next_conv.dilation,\n",
        "                                        groups = next_conv.groups,\n",
        "                                        bias = True)\n",
        "        \n",
        "        next_new_conv.bias = next_conv.bias\n",
        "\n",
        "        old_weights = next_conv.weight.data.cpu().numpy()\n",
        "        new_weights = next_new_conv.weight.data.cpu().numpy()\n",
        "        \n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_filter, 1)\n",
        "        new_weights[:, :, :, :] = new_weights_temp[:, :, :, :]\n",
        "\n",
        "        next_new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        # Now do the same thing for biases\n",
        "        next_new_conv.bias.data = next_conv.bias.data\n",
        "\n",
        "        # Update the actual model by replacing the existing filters with the new ones\n",
        "        features = torch.nn.Sequential(\n",
        "                *(replace_layers(model.features, i, [idx_layer, idx_layer + offset], \\\n",
        "                    [new_conv, next_new_conv]) for i, _ in enumerate(model.features)))\n",
        "        del model.features\n",
        "        del conv\n",
        "\n",
        "        model.features = features\n",
        "    \n",
        "    else:\n",
        "\n",
        "        # This is the last conv layer. This affects the first linear layer of the classifier.\n",
        "        model.features = torch.nn.Sequential(*(replace_layers(model.features, i, [idx_layer], [new_conv]) for i, _ in enumerate(model.features)))\n",
        "        idx_layer = 0\n",
        "        old_linear_layer = None\n",
        "\n",
        "        for _, module in model.classifier._modules.items():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                old_linear_layer = module\n",
        "                break\n",
        "            idx_layer = idx_layer + 1\n",
        "\n",
        "        if old_linear_layer == None:\n",
        "            raise BaseException(\"No linear layer found in classifier.\")\n",
        "            \n",
        "        params_per_input_channel = int(old_linear_layer.in_features/conv.out_channels)\n",
        "\n",
        "        new_linear_layer = torch.nn.Linear(old_linear_layer.in_features - N_prune*params_per_input_channel, \n",
        "                                           old_linear_layer.out_features)\n",
        "\n",
        "        old_weights = old_linear_layer.weight.data.cpu().numpy()\n",
        "        new_weights = new_linear_layer.weight.data.cpu().numpy()\t \t\n",
        "\n",
        "        # Copy over the set of filters, excluding the ones to be removed\n",
        "        new_weights_temp = np.copy(old_weights)\n",
        "        idx_expanded = np.zeros(shape=(N_prune*params_per_input_channel))\n",
        "        \n",
        "        for kk in range(0, len(idx_filter)):\n",
        "            idx_expanded[kk*params_per_input_channel:kk*params_per_input_channel+params_per_input_channel] = np.arange(idx_filter[kk]*params_per_input_channel, idx_filter[kk]*params_per_input_channel + params_per_input_channel)\n",
        "\n",
        "        new_weights_temp = np.delete(new_weights_temp, idx_expanded.astype(int), 1)\n",
        "        new_weights[:, :] = new_weights_temp[:, :]\n",
        "        \n",
        "        new_linear_layer.bias.data = old_linear_layer.bias.data\n",
        "        new_linear_layer.weight.data = torch.from_numpy(new_weights).cuda()\n",
        "\n",
        "        classifier = torch.nn.Sequential(*(replace_layers(model.classifier, i, [idx_layer], [new_linear_layer]) for i, _ in enumerate(model.classifier)))\n",
        "\n",
        "        del model.classifier\n",
        "        del next_conv\n",
        "        del conv\n",
        "        model.classifier = classifier\n",
        "        \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7h5EFGhe-PJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pruning driver"
      ]
    },
    {
      "metadata": {
        "id": "Xp8V5o_UfADH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to iterate through all conv2D layers of the network and determine \n",
        "# filters to be pruned, and then carry out the pruning.\n",
        "def PruneAllConv2DLayers(model, prune_settings):\n",
        "    \n",
        "    # Extract pruning settings for convenience\n",
        "    # Note that \"N_prune\" *consecutive* filters will get pruned\n",
        "    N_prune = prune_settings.N_prune\n",
        "    P_prune = prune_settings.P_prune\n",
        "    p = prune_settings.p\n",
        "    pruning_metric = prune_settings.pruning_metric\n",
        "    \n",
        "    # Count number of prunable layers for preallocation\n",
        "    N_layers = len(model.features._modules.items())       \n",
        "    prune_settings.filters_per_layer_orig = np.zeros(shape=(1, N_layers)).ravel()\n",
        "    prune_settings.filters_per_layer_after = np.zeros(shape=(1, N_layers)).ravel()\n",
        "\n",
        "    \n",
        "    # Find the N_prune filters to remove\n",
        "    ii = 0\n",
        "    while ii < len(model.features._modules.items()):\n",
        "        \n",
        "        res = list(model.features._modules.items())[ii]\n",
        "        \n",
        "        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n",
        "            \n",
        "            _, conv = list(model.features._modules.items())[ii]\n",
        "            \n",
        "            # Record pruning statistics\n",
        "            prune_settings.filters_per_layer_orig[ii] = conv.out_channels\n",
        "            prune_settings.filters_per_layer_after[ii] = conv.out_channels\n",
        "            \n",
        "            # Compute values and indices of the N_prune smallest norms\n",
        "#             if pruning_metric == WEIGHT_NORM:\n",
        "#                 norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "#             elif pruning_metric == ACT_NORM:\n",
        "# #                 norms = ComputeConv2DWeightNorms(model, ii, p)\n",
        "#                 norms = ComputeConv2DActNorms(res[1], prune_settings)\n",
        "                \n",
        "        \n",
        "            if (P_prune >= 0):\n",
        "                N_prune = int(conv.out_channels*P_prune/100.0)\n",
        "                prune_settings.N_prune = N_prune\n",
        "            \n",
        "            if prune_settings.norms_botk[ii] is not None:\n",
        "                \n",
        "#                 n_botk, ind_botk = torch.topk(torch.from_numpy(prune_settings.norms_botk[ii]), N_prune, 0, largest=False, sorted=True, out=None)\n",
        "            \n",
        "                norms = np.asarray(prune_settings.norms_botk[ii]).ravel()\n",
        "                ind_botk = np.argpartition(norms, N_prune)    \n",
        "                n_botk = norms[ind_botk[:N_prune]]\n",
        "                ind_botk = ind_botk[:N_prune]\n",
        "        \n",
        "                prune_settings.idx_layer = ii\n",
        "                prune_settings.idx_filter = ind_botk\n",
        "\n",
        "                model = PruneConvLayers(model, prune_settings)\n",
        "                \n",
        "        ii = ii + 1\n",
        "            \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cCEASFCwB3uP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Pruning"
      ]
    },
    {
      "metadata": {
        "id": "LASRjk-I2zXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "d0c0ea4b-6d01-44c4-818a-f6d7acc19da9"
      },
      "cell_type": "code",
      "source": [
        "# Test pruning\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.train()\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10, 12, 15, 16, 21), \n",
        "                                     N_prune=5, p=2, pruning_metric=WEIGHT_NORM)\n",
        "# prune_settings = UnitPruningSettings(idx_layer=28, idx_filter=(10), \n",
        "#                                      N_prune=1, p=2, pruning_metric=WEIGHT_NORM)\n",
        "\n",
        "N_layers = len(model.features._modules.items())       \n",
        "prune_settings.filters_per_layer_orig = np.zeros(shape=(1, N_layers)).ravel()\n",
        "prune_settings.filters_per_layer_after = np.zeros(shape=(1, N_layers)).ravel()\n",
        "\n",
        "t0 = time.time()\n",
        "model = PruneConvLayers(model, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.torch/models/vgg16-397923af.pth\n",
            "553433881it [00:05, 109909910.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Pruning took 13.103468179702759 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BgOIBqly2ntw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Meta-Pruning Functions"
      ]
    },
    {
      "metadata": {
        "id": "zEl8diRM2sa0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Masked:\n",
        "    def make_mask(self, threshold, mask=None):\n",
        "        if mask is None:\n",
        "            print(\"new mask\", device)\n",
        "            self.mask = torch.ones(self.weight.size(), requires_grad=False).to(device)\n",
        "        else:\n",
        "            self.mask = mask      \n",
        "        self.zeros = torch.zeros(self.weight.size(), requires_grad=False).to(device)\n",
        "        self.threshold = threshold\n",
        "    \n",
        "    def set_threshold(self, prop=0.05):\n",
        "        unique_weights = torch.unique(self.weight*self.mask)\n",
        "        mask_size = self.mask.reshape(-1).size()[0]\n",
        "#     mask_size = mask_size[0]*mask_size[1]\n",
        "        mask_nonzero = torch.sum(self.mask.view([mask_size]))\n",
        "        mask_total = mask_size\n",
        "        print('nonzero proportion: {:.4f}'.format(mask_nonzero/mask_total))\n",
        "        self.threshold = torch.max(torch.topk(torch.abs(unique_weights),int(prop*unique_weights.size()[0]),largest=False)[0])    \n",
        "\n",
        "    def make_threshold_mask(self):\n",
        "        self.mask = torch.where(torch.abs(self.weight) >= self.threshold,self.mask,self.zeros).to(device)\n",
        "#     self.mask.requires_grad_(requires_grad=False)\n",
        "    def mask_weight(self):\n",
        "        self.weight = torch.nn.Parameter(self.weight*self.mask).to(device)\n",
        "    \n",
        "class MaskedLinear(torch.nn.Linear, Masked):\n",
        "    def __init__(self, in_features, out_features, bias=True, threshold=0.001, mask=None):\n",
        "        super(MaskedLinear, self).__init__(in_features,out_features)\n",
        "        self.make_mask(threshold,mask)\n",
        "    def forward(self, input):\n",
        "        self.make_threshold_mask()\n",
        "        self.mask_weight()\n",
        "        #     print(self.mask[125:135,125:135])\n",
        "        #     print(self.weight[125:135,125:135])\n",
        "        return F.linear(input, self.weight, self.bias)\n",
        "\n",
        "class MaskedConv(torch.nn.Conv2d, Masked):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, groups, bias=True, threshold=0.0001):\n",
        "        super(MaskedConv,self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.make_mask(threshold)    \n",
        "    def forward(self, input):\n",
        "        self.mask_weight()\n",
        "        return F.conv2d(input, self.weight, self.bias, self.stride,\n",
        "                    self.padding, self.dilation, self.groups)\n",
        "\n",
        "limit_a, limit_b, epsilon = -.1, 1.1, 1e-6\n",
        "device='cuda'\n",
        "\n",
        "class LinearL0(Module):\n",
        "    \"\"\"Implementation of L0 regularization for the input units of a fully connected layer\"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True, weight_decay=1., droprate_init=0.5, temperature=2./3.,\n",
        "                 lamba=1., local_rep=False, **kwargs):\n",
        "        \"\"\"\n",
        "        :param in_features: Input dimensionality\n",
        "        :param out_features: Output dimensionality\n",
        "        :param bias: Whether we use a bias\n",
        "        :param weight_decay: Strength of the L2 penalty\n",
        "        :param droprate_init: Dropout rate that the L0 gates will be initialized to\n",
        "        :param temperature: Temperature of the concrete distribution\n",
        "        :param lamba: Strength of the L0 penalty\n",
        "        :param local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
        "        \"\"\"\n",
        "        super(LinearL0, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.prior_prec = weight_decay\n",
        "        self.weights = torch.nn.Parameter(torch.Tensor(in_features, out_features).to(device))\n",
        "        #         self.qz_loga = torch.Tensor(in_features).to(device)\n",
        "        self.qz_loga = torch.nn.Parameter(torch.Tensor(in_features).to(device))\n",
        "        self.temperature = temperature\n",
        "        self.droprate_init = droprate_init if droprate_init != 0. else 0.5\n",
        "        self.lamba = lamba\n",
        "        self.use_bias = False\n",
        "        self.local_rep = local_rep\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
        "            self.use_bias = True\n",
        "        self.floatTensor = torch.FloatTensor if not torch.cuda.is_available() else torch.cuda.FloatTensor\n",
        "        self.reset_parameters()\n",
        "        \n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.kaiming_normal(self.weights, mode='fan_out')\n",
        "\n",
        "        self.qz_loga.data.normal_(math.log(1 - self.droprate_init) - math.log(self.droprate_init), 1e-2)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias.data.fill_(0)\n",
        "\n",
        "    def constrain_parameters(self, **kwargs):\n",
        "        self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
        "\n",
        "    def cdf_qz(self, x):\n",
        "        \"\"\"Implements the CDF of the 'stretched' concrete distribution\"\"\"\n",
        "        xn = (x - limit_a) / (limit_b - limit_a)\n",
        "        logits = math.log(xn) - math.log(1 - xn)\n",
        "        return F.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=epsilon, max=1 - epsilon).to(device)\n",
        "\n",
        "    def quantile_concrete(self, x):\n",
        "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "        y = F.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature).to(device)\n",
        "        return y * (limit_b - limit_a) + limit_a\n",
        "\n",
        "    def _reg_w(self):\n",
        "        \"\"\"Expected L0 norm under the stochastic gates, takes into account and re-weights also a potential L2 penalty\"\"\"\n",
        "        logpw_col = torch.sum(- (.5 * self.prior_prec * self.weights.pow(2)) - self.lamba, 1).to(device)\n",
        "        logpw = torch.sum((1 - self.cdf_qz(0)) * logpw_col).to(device)\n",
        "        logpb = 0 if not self.use_bias else - torch.sum(.5 * self.prior_prec * self.bias.pow(2)).to(device)\n",
        "        return logpw + logpb\n",
        "        \n",
        "        \n",
        "    def regularization(self):\n",
        "        return self._reg_w()\n",
        "\n",
        "    def count_expected_flops_and_l0(self):\n",
        "        \"\"\"Measures the expected floating point operations (FLOPs) and the expected L0 norm\"\"\"\n",
        "        # dim_in multiplications and dim_in - 1 additions for each output neuron for the weights\n",
        "        # + the bias addition for each neuron\n",
        "        # total_flops = (2 * in_features - 1) * out_features + out_features\n",
        "        ppos = torch.sum(1 - self.cdf_qz(0))\n",
        "        expected_flops = (2 * ppos - 1) * self.out_features\n",
        "        expected_l0 = ppos * self.out_features\n",
        "        if self.use_bias:\n",
        "            expected_flops += self.out_features\n",
        "            expected_l0 += self.out_features\n",
        "#       return expected_flops.data[0], expected_l0.data[0]\n",
        "        return expected_flops, expected_l0\n",
        "\n",
        "    def get_eps(self, size):\n",
        "        \"\"\"Uniform random numbers for the concrete distribution\"\"\"\n",
        "        eps = self.floatTensor(size).uniform_(epsilon, 1-epsilon).to(device)\n",
        "        eps = Variable(eps)\n",
        "        return eps\n",
        "\n",
        "    def sample_z(self, batch_size, sample=True):\n",
        "        \"\"\"Sample the hard-concrete gates for training and use a deterministic value for testing\"\"\"\n",
        "        if sample:\n",
        "            eps = self.get_eps(self.floatTensor(batch_size, self.in_features).to(device))\n",
        "            z = self.quantile_concrete(eps)\n",
        "            return F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "        else:  # mode\n",
        "            pi = F.sigmoid(self.qz_loga).view(1, self.in_features).expand(batch_size, self.in_features).to(device)\n",
        "            return F.hardtanh(pi * (limit_b - limit_a) + limit_a, min_val=0, max_val=1).to(device)\n",
        "        \n",
        "    def sample_weights(self):\n",
        "        z = self.quantile_concrete(self.get_eps(self.floatTensor(self.in_features).to(device)))\n",
        "        mask = F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "        return mask.view(self.in_features, 1) * self.weights\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.local_rep or not self.training:\n",
        "            z = self.sample_z(input.size(0), sample=self.training)\n",
        "            xin = input.mul(z)\n",
        "            output = xin.mm(self.weights)\n",
        "        else:\n",
        "            weights = self.sample_weights()\n",
        "            output = input.mm(weights)\n",
        "        if self.use_bias:\n",
        "            output.add_(self.bias)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = ('{name}({in_features} -> {out_features}, droprate_init={droprate_init}, '\n",
        "            'lamba={lamba}, temperature={temperature}, weight_decay={prior_prec}, '\n",
        "            'local_rep={local_rep}')\n",
        "        if not self.use_bias:\n",
        "            s += ', bias=False'\n",
        "        s += ')'\n",
        "        return s.format(name=self.__class__.__name__, **self.__dict__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ikIYcgE-FbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class L0Conv2d(Module):\n",
        "    \"\"\"Implementation of L0 regularization for the feature maps of a convolutional layer\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,\n",
        "                 droprate_init=0.5, temperature=2./3., weight_decay=1., lamba=1., local_rep=False, **kwargs):\n",
        "        \"\"\"\n",
        "        :param in_channels: Number of input channels\n",
        "        :param out_channels: Number of output channels\n",
        "        :param kernel_size: Size of the kernel\n",
        "        :param stride: Stride for the convolution\n",
        "        :param padding: Padding for the convolution\n",
        "        :param dilation: Dilation factor for the convolution\n",
        "        :param groups: How many groups we will assume in the convolution\n",
        "        :param bias: Whether we will use a bias\n",
        "        :param droprate_init: Dropout rate that the L0 gates will be initialized to\n",
        "        :param temperature: Temperature of the concrete distribution\n",
        "        :param weight_decay: Strength of the L2 penalty\n",
        "        :param lamba: Strength of the L0 penalty\n",
        "        :param local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
        "        \"\"\"\n",
        "        super(L0Conv2d, self).__init__()\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = pair(kernel_size)\n",
        "        self.stride = pair(stride)\n",
        "        self.padding = pair(padding)\n",
        "        self.dilation = pair(dilation)\n",
        "        self.output_padding = pair(0)\n",
        "        self.groups = groups\n",
        "        self.prior_prec = weight_decay\n",
        "        self.lamba = lamba\n",
        "        self.droprate_init = droprate_init if droprate_init != 0. else 0.5\n",
        "        self.temperature = temperature\n",
        "        self.floatTensor = torch.FloatTensor if not torch.cuda.is_available() else torch.cuda.FloatTensor\n",
        "        self.use_bias = False\n",
        "        self.weights = Parameter(torch.Tensor(out_channels, in_channels // groups, *self.kernel_size).to(device))\n",
        "        self.qz_loga = Parameter(torch.Tensor(out_channels).to(device))\n",
        "        self.dim_z = out_channels\n",
        "        self.input_shape = None\n",
        "        self.local_rep = local_rep\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels).to(device))\n",
        "            self.use_bias = True\n",
        "\n",
        "        self.reset_parameters()\n",
        "        print(self)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_normal(self.weights, mode='fan_in')\n",
        "\n",
        "        self.qz_loga.data.normal_(math.log(1 - self.droprate_init) - math.log(self.droprate_init), 1e-2)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias.data.fill_(0)\n",
        "\n",
        "    def constrain_parameters(self, **kwargs):\n",
        "        self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
        "\n",
        "    def cdf_qz(self, x):\n",
        "        \"\"\"Implements the CDF of the 'stretched' concrete distribution\"\"\"\n",
        "        xn = (x - limit_a) / (limit_b - limit_a)\n",
        "        logits = math.log(xn) - math.log(1 - xn)\n",
        "        return F.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=epsilon, max=1 - epsilon)\n",
        "\n",
        "    def quantile_concrete(self, x):\n",
        "        \"\"\"Implements the quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "        y = F.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature)\n",
        "        return y * (limit_b - limit_a) + limit_a\n",
        "\n",
        "    def _reg_w(self):\n",
        "        \"\"\"Expected L0 norm under the stochastic gates, takes into account and re-weights also a potential L2 penalty\"\"\"\n",
        "        q0 = self.cdf_qz(0)\n",
        "        logpw_col = torch.sum(- (.5 * self.prior_prec * self.weights.pow(2)) - self.lamba, 3).sum(2).sum(1)\n",
        "        logpw = torch.sum((1 - q0) * logpw_col).to(device)\n",
        "        logpb = 0 if not self.use_bias else - torch.sum((1 - q0) * (.5 * self.prior_prec * self.bias.pow(2) -\n",
        "                                                                    self.lamba))\n",
        "        return logpw + logpb\n",
        "\n",
        "    def regularization(self):\n",
        "        return self._reg_w()\n",
        "\n",
        "    def count_expected_flops_and_l0(self):\n",
        "        \"\"\"Measures the expected floating point operations (FLOPs) and the expected L0 norm\"\"\"\n",
        "        ppos = torch.sum(1 - self.cdf_qz(0))\n",
        "        n = self.kernel_size[0] * self.kernel_size[1] * self.in_channels  # vector_length\n",
        "        flops_per_instance = n + (n - 1)  # (n: multiplications and n-1: additions)\n",
        "\n",
        "        num_instances_per_filter = ((self.input_shape[1] - self.kernel_size[0] + 2 * self.padding[0]) / self.stride[0]) + 1  # for rows\n",
        "        num_instances_per_filter *= ((self.input_shape[2] - self.kernel_size[1] + 2 * self.padding[1]) / self.stride[1]) + 1  # multiplying with cols\n",
        "\n",
        "        flops_per_filter = num_instances_per_filter * flops_per_instance\n",
        "        expected_flops = flops_per_filter * ppos  # multiply with number of filters\n",
        "        expected_l0 = n * ppos\n",
        "\n",
        "        if self.use_bias:\n",
        "            # since the gate is applied to the output we also reduce the bias computation\n",
        "            expected_flops += num_instances_per_filter * ppos\n",
        "            expected_l0 += ppos\n",
        "\n",
        "#         return expected_flops.data[0], expected_l0.data[0]\n",
        "        return expected_flops, expected_l0\n",
        "\n",
        "    def get_eps(self, size):\n",
        "        \"\"\"Uniform random numbers for the concrete distribution\"\"\"\n",
        "        eps = self.floatTensor(size).uniform_(epsilon, 1-epsilon).to(device)\n",
        "        eps = Variable(eps)\n",
        "        return eps\n",
        "\n",
        "    def sample_z(self, batch_size, sample=True):\n",
        "        \"\"\"Sample the hard-concrete gates for training and use a deterministic value for testing\"\"\"\n",
        "        if sample:\n",
        "            eps = self.get_eps(self.floatTensor(batch_size, self.dim_z)).to(device)\n",
        "            z = self.quantile_concrete(eps).view(batch_size, self.dim_z, 1, 1)\n",
        "            return F.hardtanh(z, min_val=0, max_val=1).to(device)\n",
        "        else:  # mode\n",
        "            pi = F.sigmoid(self.qz_loga).view(1, self.dim_z, 1, 1)\n",
        "            return F.hardtanh(pi * (limit_b - limit_a) + limit_a, min_val=0, max_val=1).to(device)\n",
        "\n",
        "    def sample_weights(self):\n",
        "        z = self.quantile_concrete(self.get_eps(self.floatTensor(self.dim_z).to(device))).view(self.dim_z, 1, 1, 1)\n",
        "        return F.hardtanh(z, min_val=0, max_val=1).to(device) * self.weights\n",
        "\n",
        "    def forward(self, input_):\n",
        "        if self.input_shape is None:\n",
        "            self.input_shape = input_.size()\n",
        "        b = None if not self.use_bias else self.bias\n",
        "        if self.local_rep or not self.training:\n",
        "            output = F.conv2d(input_, self.weights, b, self.stride, self.padding, self.dilation, self.groups)\n",
        "            z = self.sample_z(output.size(0), sample=self.training)\n",
        "            return output.mul(z)\n",
        "        else:\n",
        "            weights = self.sample_weights()\n",
        "            output = F.conv2d(input_, weights, None, self.stride, self.padding, self.dilation, self.groups)\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = ('{name}({in_channels}, {out_channels}, kernel_size={kernel_size}, stride={stride}, '\n",
        "             'droprate_init={droprate_init}, temperature={temperature}, prior_prec={prior_prec}, '\n",
        "             'lamba={lamba}, local_rep={local_rep}')\n",
        "        if self.padding != (0,) * len(self.padding):\n",
        "            s += ', padding={padding}'\n",
        "        if self.dilation != (1,) * len(self.dilation):\n",
        "            s += ', dilation={dilation}'\n",
        "        if self.output_padding != (0,) * len(self.output_padding):\n",
        "            s += ', output_padding={output_padding}'\n",
        "        if self.groups != 1:\n",
        "            s += ', groups={groups}'\n",
        "        if not self.use_bias:\n",
        "            s += ', bias=False'\n",
        "        s += ')'\n",
        "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jyVxLH0h5HEQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def MaskConv2DLayers(network, layers_to_mask, threshold=0.002, linear_masking=None, random_init=False, bias=True, masks=None):\n",
        "    \"\"\"\"\n",
        "    replaces conv2d layers with masked conv2d layers\n",
        "    network is the initial sequential container\n",
        "    layers is a list of layers to mask\n",
        "    random init is a logical indicating whether to preserve the initial weights or to modify them\n",
        "    \"\"\"\n",
        "    network.masked_layers=[]\n",
        "    for name,layer in network.named_children():   \n",
        "        if int(name) in layers_to_mask:\n",
        "            layer_mask = None\n",
        "            if masks is not None:\n",
        "                if name in masks:\n",
        "                    layer_mask = masks.get(name)      \n",
        "            if type(layer)== torch.nn.Linear and linear_masking is None:\n",
        "                masked_layer = MaskedLinear(layer.in_features, layer.out_features, bias=bias,threshold=threshold,mask=layer_mask)\n",
        "            elif type(layer)== torch.nn.Linear and linear_masking =='L0':\n",
        "                masked_layer = LinearL0(layer.in_features, layer.out_features, bias=bias, lamba=0.1/640)\n",
        "                network.masked_layers.append(masked_layer)\n",
        "            elif type(layer)== torch.nn.Conv2d:\n",
        "#                 masked_layer = MaskedConv(layer.in_channels, layer.out_channels, layer.kernel_size, layer.stride, layer.padding, layer.dilation, layer.groups, bias=bias, threshold=threshold)\n",
        "                masked_layer = L0Conv2d(layer.in_channels, layer.out_channels, layer.kernel_size, layer.stride, layer.padding, layer.dilation, layer.groups, bias=bias, \n",
        "                                        droprate_init=0.5, temperature=2./3., weight_decay=1., lamba=1., local_rep=False)\n",
        "                network.masked_layers.append(masked_layer)\n",
        "            if random_init != True:\n",
        "                masked_layer.weight = copy.deepcopy(layer.weight)\n",
        "                masked_layer.bias = copy.deepcopy(layer.bias)\n",
        "            network[int(name)] = masked_layer\n",
        "            \n",
        "            \n",
        "def mask_network(network, layers_to_mask, threshold=0.002, linear_masking=None, random_init=False, bias=True, masks=None):\n",
        "    \"\"\"\"\n",
        "    replaces linear layers with masked linear layers\n",
        "    replaces conv layers with masked conv layers\n",
        "    network is the initial sequential container\n",
        "    layers is a list of layers to mask\n",
        "    random init is a logical indicating whether to preserve the initial weights or to modify them\n",
        "    \"\"\"\n",
        "    network.masked_layers=[]\n",
        "    for name,layer in network.named_children():   \n",
        "        if int(name) in layers_to_mask:\n",
        "            print(\"Here\")\n",
        "            layer_mask = None\n",
        "            if masks is not None:\n",
        "                if name in masks:\n",
        "                    layer_mask = masks.get(name)      \n",
        "            if type(layer)== torch.nn.Linear and linear_masking is None:\n",
        "                masked_layer = MaskedLinear(layer.in_features, layer.out_features, bias=bias,threshold=threshold,mask=layer_mask)\n",
        "            elif type(layer)== torch.nn.Linear and linear_masking =='L0':\n",
        "                masked_layer = LinearL0(layer.in_features, layer.out_features, bias=bias, lamba=0.1/640)\n",
        "                network.masked_layers.append(masked_layer)\n",
        "            elif type(layer)== torch.nn.Conv2d:\n",
        "                print(\"Here\")\n",
        "#                 masked_layer = MaskedConv(layer.in_channels, layer.out_channels, layer.kernel_size, layer.stride, layer.padding, layer.dilation,layer.groups, bias=bias, threshold=threshold)\n",
        "                masked_layer = L0Conv2d(layer.in_channels, layer.out_channels, layer.kernel_size, layer.stride, layer.padding, layer.dilation, layer.groups, bias=bias, \n",
        "                                        droprate_init=0.5, temperature=2./3., weight_decay=1., lamba=0.1/640, local_rep=False)\n",
        "                network.masked_layers.append(masked_layer)\n",
        "            else:\n",
        "                continue\n",
        "            if random_init != True:\n",
        "                masked_layer.weight = copy.deepcopy(layer.weight)\n",
        "                masked_layer.bias = copy.deepcopy(layer.bias)\n",
        "            network[int(name)] = masked_layer\n",
        "            \n",
        "            return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YbnaQGqI6PAy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VGG_L0(tv_vgg.VGG):\n",
        "    def regularization(self):\n",
        "        regularization = 0.\n",
        "        for layer in self.layers:\n",
        "            regularization += - (1. / self.N) * layer.regularization()\n",
        "#         if torch.cuda.is_available():\n",
        "#             regularization = regularization.cuda()\n",
        "        return regularization\n",
        "    \n",
        "    def regularize(self, N):\n",
        "        regularization = 0.\n",
        "        for layer in self.masked_layers:\n",
        "            regularization += - (1. / N) * layer.regularization()          \n",
        "#         if torch.cuda.is_available():\n",
        "#             regularization = regularization.cuda()\n",
        "        return regularization\n",
        "\n",
        "    def clamp_parameters(self):\n",
        "        for layer in self.masked_layers:\n",
        "            layer.constrain_parameters()\n",
        "    \n",
        "    def get_exp_flops_l0(self):\n",
        "        total_flops = 0\n",
        "        total_l0 = 0\n",
        "#         print(self.masked_layers)\n",
        "        for layer in self.masked_layers:\n",
        "            exp_flops, exp_l0 = layer.count_expected_flops_and_l0()\n",
        "            total_flops += exp_flops\n",
        "            total_l0 += exp_l0\n",
        "        return total_flops, total_l0\n",
        "    \n",
        "    \n",
        "def vgg16_L0(pretrained=False, **kwargs):\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\")\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGG_L0(tv_vgg.make_layers(tv_vgg.cfg['D']), **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(tv_vgg.model_urls['vgg16']))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quImFgeUDM1U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def freeze_layers(model_ft, exclude=[]):\n",
        "#   children = list(model_ft.named_children())\n",
        "    for name,param in model_ft.named_parameters():   \n",
        "        if(name not in  exclude):\n",
        "            param.requires_grad = False\n",
        "        \n",
        "def countNonZeroWeights(model):\n",
        "    nonzeros = 0\n",
        "    weights = 0\n",
        "    for name,param in model.named_parameters():\n",
        "        if param is not None:\n",
        "            nonzeros += torch.sum((param != 0).int()).data[0]\n",
        "            weights += torch.sum(param).data[0]\n",
        "    \n",
        "    return nonzeros, weights\n",
        "\n",
        "def set_threshold(model, prop=0.05):\n",
        "    for child in model.named_children():    \n",
        "        for child in child[1].named_children():\n",
        "#       print(child)\n",
        "            if type(child[1]) == MaskedLinear or type(child[1]) == MaskedConv: \n",
        "                child[1].set_threshold(prop=prop)\n",
        "                print(\"layer {}  new threshold {:.4f}\".format(child[0], child[1].threshold))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nclNqiyMDzp9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model_prune(model, dloaders, dataset_sizes, criterion, optimizer, scheduler,prop=0.05, num_epochs=25, device='cuda',pruning='threshold'):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    print(len(dloaders['train']))\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "                data_idx = 0\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                data_idx = 1\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            i=0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dloaders[phase]:               \n",
        "#                 print(\"batch {} phase {}\".format(i, phase))\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    if pruning == 'L0':\n",
        "                        loss = criterion(outputs, labels, model)\n",
        "                    else:\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        model.clamp_parameters()\n",
        "                        exp_flops, exp_l0 = model.get_exp_flops_l0()\n",
        "                i+=1\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                           \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            if epoch % 5 == 0 and phase == 'train': \n",
        "                if pruning == 'threshold':\n",
        "                    set_threshold(model,prop=prop)\n",
        "                elif pruning == 'L0':\n",
        "                    print(\"Expected flops: {} | Expected L0 norm: {}\".format(exp_flops.item(), exp_l0.item()))\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YC2Z_YkP68mR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2162
        },
        "outputId": "67e144c1-d5c8-4831-b759-ee6fd89bf963"
      },
      "cell_type": "code",
      "source": [
        "def run_normal_training_with_L0_pruning(this_trainset):\n",
        "    print(this_trainset.__len__())  \n",
        "    _,mytrainset = torch.utils.data.random_split(this_trainset, (49200, 800))\n",
        "    # _,trainset = torch.utils.data.random_split(trainset,(49995,5))\n",
        "    print(mytrainset.__len__())\n",
        "\n",
        "    mytrain_data, myval_data = torch.utils.data.random_split(mytrainset,(int(0.8*len(mytrainset)),int(0.2*len(mytrainset))))\n",
        "    print(mytrain_data.__len__(),myval_data.__len__() )\n",
        "\n",
        "    mytrainloader = torch.utils.data.DataLoader(mytrain_data, batch_size=5,\n",
        "                                                shuffle=True, num_workers=0)\n",
        "    myvalloader = torch.utils.data.DataLoader(myval_data, batch_size=5,\n",
        "                                              shuffle=True, num_workers=0)\n",
        "    mydataloaders = {'train': mytrainloader, 'val': myvalloader}\n",
        "    image_datasets = {'train': mytrain_data,'val': myval_data}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}  \n",
        "\n",
        "    model_ft = vgg16_L0(pretrained=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model_ft = model_ft.to(device)\n",
        "\n",
        "#     freeze_layers(model_ft.features, exclude=[])\n",
        "#     mask_network(model_ft.classifier, [0,3,6], linear_masking=\"L0\")\n",
        "#     model_ft.masked_layers = model_ft.classifier.masked_layers\n",
        "    \n",
        "#     mask_network(model_ft.features, [0,1,3,6], linear_masking=\"L0\")\n",
        "    mask_network(model_ft.features, np.arange(0, 30), linear_masking=\"L0\")\n",
        "#     mask_network(model_ft.features, [], linear_masking=\"L0\")\n",
        "    model_ft.masked_layers = model_ft.features.masked_layers\n",
        "\n",
        "    print(model_ft.features.masked_layers)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    def loss_function(outputs, targets, model):\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss += model.regularize(640)\n",
        "        return loss\n",
        "    \n",
        "    # Observe that all parameters are being optimized\n",
        "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "#   [print(p) for p in model_ft.parameters()]\n",
        "#   return\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "    model_ft = train_model_prune(model_ft, mydataloaders, dataset_sizes, loss_function, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=20, pruning=\"L0\")\n",
        "\n",
        "    \n",
        "    \n",
        "transform = transforms.Compose(\n",
        "    [transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=5,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "\n",
        "run_normal_training_with_L0_pruning(trainset)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "50000\n",
            "800\n",
            "640 160\n",
            "Here\n",
            "Here\n",
            "L0Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))\n",
            "[L0Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), droprate_init=0.5, temperature=0.6666666666666666, prior_prec=1.0, lamba=0.00015625, local_rep=False, padding=(1, 1))]\n",
            "128\n",
            "Epoch 0/19\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 3.4146 Acc: 0.1016\n",
            "1931457.0 1490.32177734375\n",
            "Expected flops: 1931457.0 | Expected L0 norm: 1490.32177734375\n",
            "val Loss: 2.5215 Acc: 0.1750\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 2.5804 Acc: 0.1391\n",
            "val Loss: 2.5024 Acc: 0.1063\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 2.4708 Acc: 0.1500\n",
            "val Loss: 2.3439 Acc: 0.1875\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 2.3271 Acc: 0.1828\n",
            "val Loss: 2.2956 Acc: 0.1938\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 2.3138 Acc: 0.1969\n",
            "val Loss: 2.2919 Acc: 0.1875\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 2.2441 Acc: 0.2172\n",
            "1931690.625 1490.501953125\n",
            "Expected flops: 1931690.625 | Expected L0 norm: 1490.501953125\n",
            "val Loss: 2.3463 Acc: 0.1875\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 2.2385 Acc: 0.2141\n",
            "val Loss: 2.0220 Acc: 0.2625\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 2.0928 Acc: 0.2609\n",
            "val Loss: 2.0703 Acc: 0.2438\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 2.0617 Acc: 0.2891\n",
            "val Loss: 2.0752 Acc: 0.2125\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 2.0615 Acc: 0.2797\n",
            "val Loss: 2.1190 Acc: 0.2000\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 2.0019 Acc: 0.3063\n",
            "1931662.5 1490.4803466796875\n",
            "Expected flops: 1931662.5 | Expected L0 norm: 1490.4803466796875\n",
            "val Loss: 2.1237 Acc: 0.2438\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 2.0332 Acc: 0.2938\n",
            "val Loss: 2.0843 Acc: 0.2250\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 1.9972 Acc: 0.2906\n",
            "val Loss: 2.1337 Acc: 0.2188\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 1.9854 Acc: 0.3141\n",
            "val Loss: 2.0713 Acc: 0.2188\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 2.0057 Acc: 0.3141\n",
            "val Loss: 2.0740 Acc: 0.2438\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 2.0496 Acc: 0.3031\n",
            "1931692.75 1490.503662109375\n",
            "Expected flops: 1931692.75 | Expected L0 norm: 1490.503662109375\n",
            "val Loss: 2.0592 Acc: 0.2000\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 1.9546 Acc: 0.3266\n",
            "val Loss: 1.9741 Acc: 0.3000\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 2.0225 Acc: 0.2938\n",
            "val Loss: 2.0496 Acc: 0.2438\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 1.9899 Acc: 0.2859\n",
            "val Loss: 1.9819 Acc: 0.2625\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 1.9683 Acc: 0.3250\n",
            "val Loss: 2.0324 Acc: 0.2938\n",
            "\n",
            "Training complete in 6m 58s\n",
            "Best val Acc: 0.300000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0tfmvixSCzAH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Setup Routines"
      ]
    },
    {
      "metadata": {
        "id": "fF2ZhlgCC31h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline Model Setup"
      ]
    },
    {
      "metadata": {
        "id": "GeD0hZfpDHbr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_baseline = models.vgg16(pretrained=False)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_baseline = model_baseline.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model_baseline.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lc0YTkyADA63",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruned Model Setup"
      ]
    },
    {
      "metadata": {
        "id": "cr4wgE6eDKND",
        "colab_type": "code",
        "outputId": "0f2c5d59-5f0f-4a76-918f-858c3ab87e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "cell_type": "code",
      "source": [
        "# Test pruning all layers\n",
        "\n",
        "model_pruned = models.vgg16(pretrained=True)\n",
        "model_pruned.train()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_pruned = model_pruned.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model_pruned.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Pruning setup\n",
        "prune_settings = UnitPruningSettings(28, 10, N_prune = 4, p = 2, pruning_metric = WEIGHT_NORM)\n",
        "\n",
        "t0 = time.time()\n",
        "model_pruned = PruneAllConv2DLayers(model_pruned, prune_settings)\n",
        "print (\"Pruning took {} s\".format(time.time() - t0))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c16a3b839ee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel_pruned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPruneAllConv2DLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pruned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Pruning took {} s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-68787035a1e4>\u001b[0m in \u001b[0;36mPruneAllConv2DLayers\u001b[0;34m(model, prune_settings)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mprune_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_prune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN_prune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mprune_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorms_botk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#                 n_botk, ind_botk = torch.topk(torch.from_numpy(prune_settings.norms_botk[ii]), N_prune, 0, largest=False, sorted=True, out=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "coPfbZmXE3T0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Driver Routines"
      ]
    },
    {
      "metadata": {
        "id": "DS7qbZ0JE6Fs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Iterative Pruning"
      ]
    },
    {
      "metadata": {
        "id": "E1q-h_KTE-kE",
        "colab_type": "code",
        "outputId": "e7f3895e-f73e-49bf-986d-4ddb1980eb5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6356
        }
      },
      "cell_type": "code",
      "source": [
        "# ====== Dataset setup ======\n",
        "\n",
        "percent_data = 1.0\n",
        "percent_val = 20.0\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "# ====== Model setup ======\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# ====== Pruning setup ======\n",
        "\n",
        "N_prune = 0\n",
        "P_prune = 5\n",
        "p = 2\n",
        "prune_settings = UnitPruningSettings(N_prune=N_prune, \n",
        "                                     P_prune=P_prune, \n",
        "                                     p=p, \n",
        "                                     pruning_metric=ACT_NORM)\n",
        "prune_settings.Setup(model)\n",
        "\n",
        "\n",
        "# ====== Begin training ======\n",
        "\n",
        "N_iter_outer = 7\n",
        "N_iter_inner = 7\n",
        "\n",
        "# Import data\n",
        "dat = DatasetManager(dataset='cifar10', \n",
        "                     percent_data=percent_data, \n",
        "                     percent_val=percent_val)\n",
        "\n",
        "dat.ImportDataset(batch_size=batch_size)\n",
        "\n",
        "for ii in range(0, N_iter_outer):\n",
        "    \n",
        "    print(\"\\n------ Outer iteration {}/{} ------\".format(ii+1, N_iter_outer))\n",
        "#     t0 = time.time()\n",
        "\n",
        "    # ------ Prune current model ------\n",
        "        \n",
        "    model = PruneAllConv2DLayers(model, prune_settings)\n",
        "    new_model = copy.deepcopy(model)\n",
        "    model = new_model\n",
        "    prune_settings.PrintPruningStatistics(1)\n",
        "    prune_settings.ResetNormContainers()\n",
        "\n",
        "    # ------ Train current model ------\n",
        "    \n",
        "    # Import data\n",
        "    dat = DatasetManager(dataset='cifar10', percent_data=percent_data, percent_val=percent_val)\n",
        "    dat.ImportDataset(batch_size=batch_size)\n",
        "    \n",
        "    # Update optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, prune_settings, num_epochs=N_iter_inner)\n",
        "        \n",
        "#     print (\"Pruning took {} s\".format(time.time() - t0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "\n",
            "------ Outer iteration 1/7 ------\n",
            "Total number of filters before pruning: 4224.0\n",
            "Total number of filters after pruning: 4224.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n",
            "train Loss: 3.5972 Acc: 0.0775\n",
            "val Loss: 2.4606 Acc: 0.1000\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 2.4134 Acc: 0.1575\n",
            "val Loss: 2.3843 Acc: 0.2000\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 2.4047 Acc: 0.1600\n",
            "val Loss: 2.3911 Acc: 0.1500\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n",
            "train Loss: 2.2510 Acc: 0.1900\n",
            "val Loss: 2.2885 Acc: 0.2200\n",
            "\n",
            "Epoch 5/7\n",
            "----------\n",
            "train Loss: 2.2062 Acc: 0.2100\n",
            "val Loss: 2.0295 Acc: 0.2300\n",
            "\n",
            "Epoch 6/7\n",
            "----------\n",
            "train Loss: 2.0671 Acc: 0.2425\n",
            "val Loss: 2.0786 Acc: 0.2800\n",
            "\n",
            "Epoch 7/7\n",
            "----------\n",
            "train Loss: 2.0907 Acc: 0.2675\n",
            "val Loss: 1.9225 Acc: 0.3300\n",
            "\n",
            "Training complete in 1m 56s\n",
            "Best val Acc: 0.330000\n",
            "\n",
            "------ Outer iteration 2/7 ------\n",
            "Total number of filters before pruning: 4224.0\n",
            "Total number of filters after pruning: 4020.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n",
            "train Loss: 2.0320 Acc: 0.2575\n",
            "val Loss: 2.3711 Acc: 0.2200\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 2.0066 Acc: 0.2850\n",
            "val Loss: 1.9299 Acc: 0.2000\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 1.8616 Acc: 0.3375\n",
            "val Loss: 1.8122 Acc: 0.4000\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n",
            "train Loss: 1.8364 Acc: 0.3150\n",
            "val Loss: 1.9098 Acc: 0.3100\n",
            "\n",
            "Epoch 5/7\n",
            "----------\n",
            "train Loss: 1.8022 Acc: 0.3500\n",
            "val Loss: 1.8290 Acc: 0.3100\n",
            "\n",
            "Epoch 6/7\n",
            "----------\n",
            "train Loss: 1.6533 Acc: 0.4125\n",
            "val Loss: 1.9864 Acc: 0.3000\n",
            "\n",
            "Epoch 7/7\n",
            "----------\n",
            "train Loss: 1.6234 Acc: 0.4475\n",
            "val Loss: 1.9069 Acc: 0.3100\n",
            "\n",
            "Training complete in 1m 54s\n",
            "Best val Acc: 0.400000\n",
            "\n",
            "------ Outer iteration 3/7 ------\n",
            "Total number of filters before pruning: 4020.0\n",
            "Total number of filters after pruning: 3822.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n",
            "train Loss: 1.9650 Acc: 0.2975\n",
            "val Loss: 1.8824 Acc: 0.3000\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 1.8477 Acc: 0.3475\n",
            "val Loss: 1.7693 Acc: 0.2900\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 1.8160 Acc: 0.3325\n",
            "val Loss: 1.5054 Acc: 0.4400\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n",
            "train Loss: 1.8471 Acc: 0.3125\n",
            "val Loss: 1.7032 Acc: 0.3600\n",
            "\n",
            "Epoch 5/7\n",
            "----------\n",
            "train Loss: 1.7477 Acc: 0.3650\n",
            "val Loss: 1.7540 Acc: 0.3800\n",
            "\n",
            "Epoch 6/7\n",
            "----------\n",
            "train Loss: 1.5897 Acc: 0.4275\n",
            "val Loss: 1.5631 Acc: 0.4200\n",
            "\n",
            "Epoch 7/7\n",
            "----------\n",
            "train Loss: 1.5385 Acc: 0.4725\n",
            "val Loss: 1.8595 Acc: 0.3800\n",
            "\n",
            "Training complete in 1m 48s\n",
            "Best val Acc: 0.440000\n",
            "\n",
            "------ Outer iteration 4/7 ------\n",
            "Total number of filters before pruning: 3822.0\n",
            "Total number of filters after pruning: 3637.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n",
            "train Loss: 2.0312 Acc: 0.2575\n",
            "val Loss: 1.8992 Acc: 0.2600\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 1.9073 Acc: 0.2950\n",
            "val Loss: 1.7771 Acc: 0.3800\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 1.8140 Acc: 0.3225\n",
            "val Loss: 2.1002 Acc: 0.3300\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n",
            "train Loss: 1.8549 Acc: 0.3625\n",
            "val Loss: 1.7457 Acc: 0.3200\n",
            "\n",
            "Epoch 5/7\n",
            "----------\n",
            "train Loss: 1.7818 Acc: 0.3225\n",
            "val Loss: 1.9306 Acc: 0.3300\n",
            "\n",
            "Epoch 6/7\n",
            "----------\n",
            "train Loss: 1.6693 Acc: 0.4000\n",
            "val Loss: 1.7140 Acc: 0.3200\n",
            "\n",
            "Epoch 7/7\n",
            "----------\n",
            "train Loss: 1.6361 Acc: 0.4000\n",
            "val Loss: 1.8905 Acc: 0.2400\n",
            "\n",
            "Training complete in 1m 40s\n",
            "Best val Acc: 0.380000\n",
            "\n",
            "------ Outer iteration 5/7 ------\n",
            "Total number of filters before pruning: 3637.0\n",
            "Total number of filters after pruning: 3458.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n",
            "train Loss: 1.9604 Acc: 0.2925\n",
            "val Loss: 1.6156 Acc: 0.3400\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 1.7532 Acc: 0.3625\n",
            "val Loss: 1.7997 Acc: 0.3000\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 1.7254 Acc: 0.3800\n",
            "val Loss: 1.6051 Acc: 0.4200\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n",
            "train Loss: 1.6882 Acc: 0.3700\n",
            "val Loss: 1.6214 Acc: 0.4300\n",
            "\n",
            "Epoch 5/7\n",
            "----------\n",
            "train Loss: 1.8109 Acc: 0.3400\n",
            "val Loss: 1.7242 Acc: 0.3900\n",
            "\n",
            "Epoch 6/7\n",
            "----------\n",
            "train Loss: 1.4690 Acc: 0.4675\n",
            "val Loss: 1.6809 Acc: 0.3800\n",
            "\n",
            "Epoch 7/7\n",
            "----------\n",
            "train Loss: 1.4390 Acc: 0.4800\n",
            "val Loss: 1.7194 Acc: 0.3500\n",
            "\n",
            "Training complete in 1m 36s\n",
            "Best val Acc: 0.430000\n",
            "\n",
            "------ Outer iteration 6/7 ------\n",
            "Total number of filters before pruning: 3458.0\n",
            "Total number of filters after pruning: 3294.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n",
            "train Loss: 1.8685 Acc: 0.3125\n",
            "val Loss: 1.5110 Acc: 0.3900\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 1.7591 Acc: 0.3800\n",
            "val Loss: 1.6985 Acc: 0.3600\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 1.6929 Acc: 0.3575\n",
            "val Loss: 1.6048 Acc: 0.4000\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n",
            "train Loss: 1.6072 Acc: 0.4050\n",
            "val Loss: 1.8045 Acc: 0.2700\n",
            "\n",
            "Epoch 5/7\n",
            "----------\n",
            "train Loss: 1.5841 Acc: 0.4425\n",
            "val Loss: 1.6910 Acc: 0.4600\n",
            "\n",
            "Epoch 6/7\n",
            "----------\n",
            "train Loss: 1.3782 Acc: 0.5175\n",
            "val Loss: 1.6707 Acc: 0.4000\n",
            "\n",
            "Epoch 7/7\n",
            "----------\n",
            "train Loss: 1.4880 Acc: 0.4875\n",
            "val Loss: 1.4173 Acc: 0.5400\n",
            "\n",
            "Training complete in 1m 31s\n",
            "Best val Acc: 0.540000\n",
            "\n",
            "------ Outer iteration 7/7 ------\n",
            "Total number of filters before pruning: 3294.0\n",
            "Total number of filters after pruning: 3136.0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Full training set size: 50000\n",
            "Full test set size: 10000\n",
            "\n",
            "Active training set size: 400\n",
            "Active validation set size: 100\n",
            "Active test set size: 100\n",
            "\n",
            "Epoch 1/7\n",
            "----------\n",
            "train Loss: 1.7780 Acc: 0.3725\n",
            "val Loss: 1.5784 Acc: 0.4400\n",
            "\n",
            "Epoch 2/7\n",
            "----------\n",
            "train Loss: 1.5815 Acc: 0.4400\n",
            "val Loss: 1.6695 Acc: 0.4100\n",
            "\n",
            "Epoch 3/7\n",
            "----------\n",
            "train Loss: 1.5619 Acc: 0.4100\n",
            "val Loss: 1.5876 Acc: 0.4400\n",
            "\n",
            "Epoch 4/7\n",
            "----------\n",
            "train Loss: 1.5182 Acc: 0.4650\n",
            "val Loss: 1.6680 Acc: 0.4300\n",
            "\n",
            "Epoch 5/7\n",
            "----------\n",
            "train Loss: 1.4080 Acc: 0.5000\n",
            "val Loss: 1.6273 Acc: 0.3600\n",
            "\n",
            "Epoch 6/7\n",
            "----------\n",
            "train Loss: 1.3604 Acc: 0.4925\n",
            "val Loss: 1.5009 Acc: 0.4300\n",
            "\n",
            "Epoch 7/7\n",
            "----------\n",
            "train Loss: 1.3093 Acc: 0.5225\n",
            "val Loss: 1.6153 Acc: 0.3700\n",
            "\n",
            "Training complete in 1m 24s\n",
            "Best val Acc: 0.440000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v7ORnILPDbVu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ]
    },
    {
      "metadata": {
        "id": "Icv-APyJDdB5",
        "colab_type": "code",
        "outputId": "ae865fec-498b-4fea-c852-0971b058b110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "PlotResults(prune_settings)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXl4FMXWxt+TnTVBQARBdhUChF0W\n2V3YXEBRWWRRRBBRP0FFvSjoVVG4ioqKCioiCoiCiiAgBtkU2SObIpvsOyEhJCSZ8/1xuqdnksnM\nJJnJJJnze55+Zrq7urqqe6beqlNVp4iZoSiKoigAEBLoBCiKoiiFBxUFRVEUxY6KgqIoimJHRUFR\nFEWxo6KgKIqi2FFRUBRFUeyoKCiKoih2VBQURVEUOyoKiqIoip2wQCcgt1SoUIFr1KjhNszFixdR\nqlSpgklQISOY8w4Ed/6DOe9AcOffm7xv2rTpNDNX9BRXkROFGjVqYOPGjW7DrFy5Eh07diyYBBUy\ngjnvQHDnP5jzDgR3/r3JOxEd9CYuNR8piqIodlQUFEVRFDsqCoqiKIqdItenoChKwZKeno7Dhw8j\nNTU10ElxS3R0NHbt2hXoZAQEx7xHRUWhatWqCA8Pz1NcKgqKorjl8OHDKFOmDGrUqAEiCnRyciQp\nKQllypQJdDICgpl3ZsaZM2dw+PBh1KxZM09xqflIURS3pKamonz58oVaEBSBiFC+fPl8tepUFBRF\n8YgKQtEhv+8qeEThzTeBG28Evvsu0ClRFEUptASPKBw4AKxdK5+KohQpQkND0bhxY/s2ceJEn8V9\n4MABNGjQwGO48ePHY/LkyT67b2EleDqaY2Lk8/z5wKZDUZRcU6JECWzdujXQyQgKgqeloKKgKMWO\nGjVq4Omnn0bDhg3RsWNH/PPPPwCk9t+5c2c0atQIXbp0wb///gsAOHHiBHr16oW4uDjExcVh3bp1\nAIDMzEw89NBDiI2NxS233IJLly65ve/WrVvRqlUrNGrUCL169cK5c+cAAO+88w7q16+PRo0a4b77\n7gMA/Prrr/YWTpMmTZCUlOSvx+ETVBQURfEeIv9sHrh06ZKT+Wju3Ln2c9HR0fjzzz8xbNgwPPHE\nEwCAUaNGYdCgQUhISED//v3x2GOPAQAee+wxdOjQAdu2bcPmzZsRGxsLANizZw9GjhyJHTt2ICYm\nBt98843b9AwcOBCvv/46EhIS0LBhQ0yYMAEAMHHiRGzZsgUJCQmYNm0aAGDy5Ml47733sHXrVqxe\nvRolSpTI/XMvQFQUFEUp9JjmI3O799577ef69u0LAOjTpw9+++03AMBvv/2Gfv36AQDuv/9+rFmz\nBgDwyy+/YMSIEQCknyI6OhoAULNmTTRu3BgA0KxZMxxw0/eYmJiI8+fPo0OHDgCAQYMGYdWqVQCA\nRo0aoX///vjiiy8QFibW+bZt2+LJJ5/EO++8g/Pnz9uPF1b8JgpEFEVEfxDRNiLaQUQTXIQZTESn\niGirsQ31V3pUFBTFBzD7Z8sHjkMw8zocMzIy0v49NDQUGRkZeYrnxx9/xMiRI7F582a0aNECGRkZ\nGDt2LKZPn45Lly6hbdu22L17d57iLij82VJIA9CZmeMANAbQlYhauQg3l5kbG9t0v6VGRUFRiiWm\nKembb75B69atAQBt2rTBnDlzAACzZ89Gu3btAABdunTBBx98AED6ERITE3N9v+joaJQrVw6rV68G\nAMyaNQsdOnSAzWbDoUOH0KlTJ7z++utITExEcnIy9u7di4YNG+KZZ55BixYtCr0o+K0dw8wMINnY\nDTe2/FUJ8oPRTFRRUJSih9mnYNK1a1f7sNRz586hUaNGCAsLw7x58wAA7777LoYMGYJJkyahYsWK\n+PTTTwEAb7/9NoYNG4YZM2YgNDQUH3zwASpXrpzr9MycORPDhw9HSkoKatWqhU8//RSZmZkYMGAA\nEhMTwcx47LHHEBMTg3HjxiE+Ph4hISGIjY1Ft27dfPBE/Agz+20DEApgK0QcXndxfjCAYwASAMwH\nUM1TnM2aNWNPxMfHZz94+rQ0VMuV83h9UcZl3oOIYM6/v/K+c+dOv8TrC6pXr86nTp1iZuYLFy4E\nODWBI2veXb0zABvZi3Lbrz0ezJwJoDERxQBYQEQNmHm7Q5AfAHzFzGlE9DCAmQA6Z42HiIYBGAYA\nlSpVwsqVK93eNzk5OVsYysxEBwCcmIhff/kFCCmefeyu8h5MBHP+/ZX36OjoQjuMkpmRnJyMyMhI\nZGZmFtp0+puseU9NTc3zb4E4n508Xt+I6AUAKczsckogEYUCOMvM0e7iad68Oed5Oc4yZYDkZCAx\nEShb1tukFymCeUlCILjz76+879q1C/Xq1fN5vL5GvaRaeXf1zohoEzM39xSXP0cfVTRaCCCiEgBu\nBrA7SxhHY97tAPzrDF07mxVFUdziTxtKZQDxRJQAYAOA5cy8iIheIqLbjTCPGcNVtwF4DNLH4D9U\nFBRFUdziz9FHCQCauDj+gsP3ZwE86680ZENFQVEUxS3Fs7c1J1QUFEVR3BJcoqBzFRSlyNGpUycs\nXbrU6diUKVPs7ipyonTp0gCAo0eP4u6773YZpmPHjvA0cGXKlClISUmx73fv3h3nfVCGFFZX3MEl\nCmZLIQ+zGBVFCQx9+/a1z042mTNnjt3nkSeqVKmC+fPn5/n+WUVh8eLFiDHLkmJIcIqCthQUpchw\n991348cff8Tly5cBiFvso0ePol27dkhOTkaXLl3QtGlTtGrVCt+5WFnRcRGdS5cu4b777kO9evXQ\nq1cvJxfZI0aMQPPmzREbG4sXX3wRgLjCPnr0KDp16oROnToBEHfdp0+fBgC8+eabaNCgARo0aIAp\nU6bY71evXr2i64rbmxluhWnL84xmZuZJk2RW85NPeoyjqBLMM3qZgzv/BTGj2V8e8TzRo0cPXrhw\nITMzv/baazx69GhmZk5PT+fExERmZt6/fz/Xrl2bbTYbMzOXKlXKfjw2NpaZmf/3v//xkCFDmJl5\n27ZtHBoayhs2bGBm5jNnzjAzc0ZGBnfo0IG3bdvGzM6zph33N27cyA0aNODk5GROSkri+vXr8+bN\nm3n//v0cGhrKW7ZsYWbmPn368KxZs7Ll6cUXX+RJkyYxM3PDhg155cqVzMw8btw4fvzxx5mZuXLl\nypyamsrMzOfOnWNm5p49e/KaNWuYmTkpKYnT09N9OqNZWwqKohR6HE1IjqYjZsZzzz2HRo0a4fbb\nb8eRI0dw4sSJHONZtWoVBgwYAEDcXDdq1Mh+bt68eWjatCmaNGmCHTt2YOfOnW7TtGbNGvTq1Qul\nSpVC6dKl0bt3b7uTvKLsiltFQVEUr/FXW8ETd9xxB1asWIHNmzcjJSUFzZo1AyAeUE+dOoVNmzZh\n7dq1qFSpElJTU3Odr/3792Py5MlYsWIFEhIS0KNHjzzFY1KUXXGrKCiKUugpXbo0OnXqhAceeMCp\ngzkxMRFXXnklwsPDsWrVKhw8eNBtPO3bt8eXX34JANi+fTsSEhIAABcuXECpUqUQHR2NEydOYMmS\nJfZrypQp49Ju365dOyxcuBApKSm4ePEiFixYYHfRnRsKmyvuwr0EkK/RIamKUmTp27cvevXq5TQS\nqX///rjtttvQsGFDxMXF4frrr3cbx4gRIzBkyBDUq1cP9erVs7c44uLi0KRJE1x//fWoVq0a2rZt\na79m2LBh6Nq1K6pUqYL4+Hj78aZNm2Lw4MFo2bIlAGDo0KFo0qSJW1NRTuTXFbfZCe8LCswhnq/I\nl0O8PXuAa68FatcGjAW+ixvB7BAOCO78q0M8dYhnUigd4hVK1HykKIriluASBUfzURFrISmKohQE\nwSUKERFAyZJAZiZw8WKgU6MoRYaiZmYOZvL7roJLFAA1ISlKLomKisKZM2dUGIoAzIwzZ84gKioq\nz3EE1+gjQETh6FERhapVA50aRSn0VK1aFYcPH8apU6cCnRS3pKam5qswLMo45j0qKgpV81G2Baco\nANpSUBQvCQ8PR82aNQOdDI+sXLkSTZpkW8IlKPBl3oPPfKRzFRRFUXIk+ERB3WcriqLkSPCKgrYU\nFEVRsqGioCiKothRUVAURVHsqCgoiqIodlQUFEVRFDsqCoqiKIqd4BMFnaegKIqSI8EnCjpPQVEU\nJUeCVxS0paAoipKN4BMFXVNBURQlR4JPFKKiZEtPBy5dCnRqFEVRChXBJwqAmpAURVFyQEVBURRF\nsROcoqDDUhVFUVwSnKKgLQVFURSXBLco6FwFRVEUJ/wmCkQURUR/ENE2ItpBRBNchIkkorlE9A8R\nrSeiGv5KjxPaUlAURXGJP1sKaQA6M3McgMYAuhJRqyxhHgRwjpnrAHgLwOt+TI+FioKiKIpL/CYK\nLCQbu+HGlnW22B0AZhrf5wPoQkTkrzTZUVFQFEVxiV/7FIgolIi2AjgJYDkzr88S5GoAhwCAmTMA\nJAIo7880AVBRUBRFyYEwf0bOzJkAGhNRDIAFRNSAmbfnNh4iGgZgGABUqlQJK1eudBs+OTnZbZgr\njx5FfQAn//4bOz3EVdTwlPfiTjDnP5jzDgR3/n2Zd7+KggkznyeieABdATiKwhEA1QAcJqIwANEA\nzri4/iMAHwFA8+bNuWPHjm7vt3LlSrgNY7i3uDIiAld6iKuo4THvxZxgzn8w5x0I7vz7Mu/+HH1U\n0WghgIhKALgZwO4swb4HMMj4fjeAX5gLwEudmo8URVFc4s+WQmUAM4koFCI+85h5ERG9BGAjM38P\nYAaAWUT0D4CzAO7zY3osdJ6CoiiKS/wmCsycAKCJi+MvOHxPBdDHX2nIEW0pKIqiuCS4ZzSrKCiK\nojgRnKIQFQVERABpaUBqaqBToyiKUmgITlEg0taCoiiKC4JTFAB1n60oiuKC4BUFbSkoiqJkQ0VB\nh6UqiqLYUVHQloKiKIodFQUVBUVRFDsqCioKiqIodlQUVBQURVHsqCioKCiKotgJXlHQeQqKoijZ\nCF5R0JaCoihKNlQUdJ6CoiiKHRUFbSkoiqLYUVFQUVAURbGjoqCioCiKYid4RaFkSSAsDLh0SdZV\nUBRFUYJYFBzXVNDOZkVRFADBLAqAzlVQFEXJQnCLgvYrKIqiOKGiAKj5SFEUxUBFAdCWgqIoioGK\nAqCioCiKYqCiAKgoKIqiGKgoACoKiqIoBsEtCjokVVEUxYngFgVtKSiKojihogCoKCiKohioKAA6\nT0FRFMVARQHQloKiKIqBigKgoqAoimKgogCoKCiKohh4JQpEVJuIIo3vHYnoMSKK8W/SCoDSpYGQ\nEODiRSA9PdCpURRFCTjethS+AZBJRHUAfASgGoAv3V1ARNWIKJ6IdhLRDiJ63EWYjkSUSERbje2F\nXOcgPxBZcxW0s1lRFAVhXoazMXMGEfUC8C4zv0tEWzxckwFgNDNvJqIyADYR0XJm3pkl3Gpm7pnb\nhPuMmBjg3DkxIVWoELBkKIqiFAa8bSmkE1FfAIMALDKOhbu7gJmPMfNm43sSgF0Ars5rQv2GDktV\nFEWx460oDAHQGsArzLyfiGoCmOXtTYioBoAmANa7ON2aiLYR0RIiivU2Tp+hnc2Koih2iJlzdwFR\nOQDVmDnBy/ClAfwKEZRvs5wrCzFNJRNRdwBvM3NdF3EMAzAMACpVqtRszpw5bu+ZnJyM0qVLe5Wf\n2BdeQMXVq7F9/Hic7tDBq2sKM7nJe3EkmPMfzHkHgjv/3uS9U6dOm5i5ucfImNnjBmAlgLIArgCw\nH1Ljf9OL68IBLAXwpJf3OQCggrswzZo1Y0/Ex8d7DGNnyBBmgHn6dO+vKcTkKu/FkGDOfzDnnTm4\n8+9N3gFsZC/KYW/NR9HMfAFAbwCfM/MNAG5ydwEREYAZAHYx85s5hLnKCAciagkxZ53xMk254q+/\ngO7dgTNZY1fzkaIoih1vRx+FEVFlAPcAeN7La9oCuB/An0S01Tj2HIBrAICZpwG4G8AIIsoAcAnA\nfYai+Zzhw4GVK4FbbwVWrLBGoqooKIqiWHgrCi9BzEBrmXkDEdUCsMfdBcy8BgB5CDMVwFQv05Av\nZs8G2rcHNm2SFsPSpTJ3TddUUBRFsfDKfMTMXzNzI2YeYezvY+a7/Js031KlirQQqlUD1q0DbrsN\nuHQJ2lJQFEVxwFs3F1WJaAERnTS2b4ioqr8T52uqVwd++QWoXFlMSb17A2mlrpCTu3ZJ82HXLnF7\noSiKEoR429H8KYDvAVQxth+MY0WOOnWkxVCxIvDTT8B9U9siHWFiV+raFahfX+xKFSoATZsCTzwB\n2GyBTraiKEqB4G2fQkVmdhSBz4joCX8kqCCoVw9Yvhzo2BFY+OsVGNh0Bz6u9yZO7UvCiUOXcfw4\ncPxMeRw/cxWu2fIvhrT/DtS7V6CTrSiK4ne8FYUzRDQAwFfGfl/4aehoQREXByxbBnTpAszZfC3m\nbJ6WY9jwMS/g/l53igM9RVGUYoy35qMHIMNRjwM4BhlKOthPaSowWrQAFi+WPobwcOmEbt4c6NkT\nGDoUGNQ/AwAwcv8Y7P18bYGm7fJlYOBAYObMAr2toihBjlctBWY+COB2x2OG+WiKPxJVkNx4I3D4\nsDQCsjYEmMNwcdsuzN9eD/1HXYHV/UQ8CoJVq4BZs4DVq4FBgwrmnoqiKPlZee1Jn6UiwISEuLYM\nEQEfLaqCqnQY65Pq46WHDhVYmnYaDsYPHNDBUIqiFBz5EYWgMLCXqx6NL/r/BIINr86sgtWrC+a+\nOx1Wndi1q2DuqSiKkh9R8Is7isJIhzfvwLNhk2FDKAbcc7lA5rk5CsHOrMsSKYqi+Am3okBESUR0\nwcWWBJmvEBxUrIjxw4+jJdbj3+MRGD4c8I+HJgtHIdixw7/3UhRFMXErCsxchpnLutjKMLO3w1mL\nBeHPPInZYYNRGkmYOxf4/HP/3evUKeD0aWtfRUFRlIIiP+aj4KJqVdQZfCOm4lEAwMiRwL59/rmV\n2Uq44grnfUVRFH+jopAbnnkGA+kL3E3zcfEi8OGH/rmNKQLdu8sQ2P37dQSSoigFg4pCbqhTB3Tf\nvRjOHwAQ30n+wBSFuDjg2mvl++7d/rmXoiiKIyoKuWXsWNyINSiFZCQkAEeP+v4WpijUrw/Exsp3\n7VdQFKUgUFHILY0aIfK2W9EZvwAAfnrxN58PRXIUhfr1nY8piqL4ExWFvPDee+ha+x8AwE/TDwGd\nOvms1D53Djh+HChZErjmGm0pKIpSsKgo5IVq1dB16f8BAJbTLcj4dY10AIwdm+8eYXPS2vXXi/sN\ns6WgoqAoSkGgopBHatUmXHstcJ5jsP6O14DMTOD116UU//XXPMfraDoCgLp1ZQSS+kBSFKUgUFHI\nB926yeeSBk8Bv/0GNGkC/PsvcM89QHJynuLMKgrh4TICiVlHICmK4n9UFPJB167y+dNPAG64Adiw\nQT5PngTefjtPcWYVBcfv2tmsKIq/UVHIBx06AFFRsrzziRMAQkOBV1+Vk5MmAWfP5jpOV6Kgnc2K\nohQUKgr5oEQJWecZkKU9AQCdOwM33QQkJkofQy64cAE4dAiIjARq1rSOa0tBUZSCQkUhn9j7FZY4\nHDRbC++8k6vZbWafwXXXAWEO7ga1paAoSkGhopBPzH6FZctkABIAWfy5d28gNRV4+WWv4zKHo9ar\n53y8bl0Rif37gZSU/KdZURQlJ1QU8kndukCtWsCZM8DGjQ4n/vtfmWgwfTqwd69XcbnqTwB0BJKi\nKAWHikI+IbJaC04mpHr1gIEDgYwM4IUXvIorJ1EA1ISkKErBoKLgA8x+hWxeU8ePByIigK++AhIS\nPMbjThS0s7lwcOwYsGJFoFOhKP5DRcEHdOokZf8ffzivmIbq1WFfu/P557NfmJwMzJsHjB2LlF0H\nsX+/9B3UqZM9qLYUCgcPPiiDy377LdApURT/oKLgA0qVAtq3l7J/+fIsJ59/HlyyFH5cZMNX4/8C\nnzkLzJwJ3HEHUKECcO+9wOuv469Wg8AsfRQREdnvoT6QAk96OrBypXzPhycTRSnUqCj4CJf9CgBO\n4krcfc169MSP6DfhOqy88h5g8GDg+++By5eBtm2BDh2w88LVAID6YX+7dMWtI5ACz7ZtwKVL8n3D\nhsCmRVH8hYqCjzD7FZYuBWw2+T5vntTwv90daw830fa02B/efx84cgRYswZYsQI72zwEAKj/5xyg\nf/9sJX9EhI5ACjTr1lnfVRSU4oqKgo+oVw+oVk3cHi1bBvTpI5ahM2eALl2AzXP3oFRkOpbhFmx+\nfTkwYgRQubJcHBqKXZU6AgDqR+6Tjuk2baRZ4IB2NgcWR1E4dMhwbaIoxQwVBR9BZLUWunUD5s+X\nvoYPPpB+hib31MXDI8MBuPZ+YRb09Wb/R3qat20DmjcHZs2y+8zWzmYfc/kyMHu2+BfxAlMUqlSR\nT20tKMURv4kCEVUjongi2klEO4jocRdhiIjeIaJ/iCiBiJr6Kz0FgSkKgIxI+vNPGXxEJMf+7/9k\nItr8+cCePVbYtDTgn39krtu13etIadO9uzjUGzgQuPJKoG9f1E+R2XHaUvARn38ODBggTToPS6oe\nOiRbTAzQt68cU1FQiiP+bClkABjNzPUBtAIwkoiyjsDvBqCusQ0D8IEf0+N3brsNGDdOJjH//LOz\nUzsAqFoVuP9+6XOYNMk6vmePuMioVUuc7CEmRjqip00DWreW/oU5cxA7aRAAYEf8CRn/quSPrVvl\n86efgB9+cBvUbCW0bi3e0QEVBaV44jdRYOZjzLzZ+J4EYBeAq7MEuwPA5yz8DiCGiCr7K03+JjQU\neOklGcseksOTffppaTnMnGn5ynM5aS00FHj4YSmN9u8HJk5E3YYlEIZ07EuqiJQ2N1mFmpI3/v7b\n+v7EE9bQIheYotCmjbi2AkQUPDQwFKXIEeY5SP4hohoAmgBYn+XU1QAOOewfNo4dy3L9MEhLApUq\nVcJKc7B4DiQnJ3sME0jatYvFqlUV8eST/2L48H1YvLgGgBooXfogVq7c7/qiG24AbrgBVftfxIGj\nMfgrszaqPfootv/3v07BfJX3c+fCsXDh1ejR4xiuvDIt3/EVFLnJf6uEBEQBSLviCkTu34/9I0fi\n4MCBLsMuXdoUQFmULLkV+/efR3R0G5w+HYG5c3/HVVel+iz9+aGw/+79TTDn36d5Z2a/bgBKA9gE\noLeLc4sA3OiwvwJAc3fxNWvWjD0RHx/vMUwg2bCBGWAuXZr57FnmPn1k//PPPV97990Sdlb4EPmy\ncaPTeV/lfcAAib5OHebjx30SZYHgdf5TUpiJmMPCmJcvl8xGRTHv358taHIyc2goc0gIc1KSHOvW\nTS6ZN89nSc83hf1372+COf/e5B3ARvaizPbr6CMiCgfwDYDZzPytiyBHAFRz2K9qHCvWNG8uUxWS\nk2W6gjufR1mxD0tt0l++jB/v8/QdPCijYgHpAL/1VuD8eS8unD8fGDXKrRmm0LB3r9h+atWSl3Hf\nfeLqfPTobEE3bpQ+n7g4oHRpOeZoQipQ9u6VF6QofsKfo48IwAwAu5j5zRyCfQ9goDEKqRWARGY+\nlkPYYsXYsfL59tuWafv66z1fZx+WWu5GoGRJYNEin5dM//ufFII9e8qEuW3bgNtv96Ksf/JJYOpU\n4JVXfJoev2A+9Guvlc9Jk2QM8bffZvNVsnatfLZpYx0LiChcvCg3vuEGETBF8QP+bCm0BXA/gM5E\ntNXYuhPRcCIaboRZDGAfgH8AfAzgET+mp1DRubO0GE6dEp861atLmeQJuyj8Eym1cgB48UWfpev0\naRk9BUjZvmwZcPXVwOrVMnIzPT2HC48dkzGbAPDGG4V/MkVWUahaVYaOAcBjj8kcBgPHTmYTUxQ2\nbbJmsPudP/4Azp2TWXOLFxfQTZVgw5+jj9YwMzFzI2ZubGyLmXkaM08zwjAzj2Tm2szckJk3eoq3\nuEAEPPuste+N6QiwfCDt2wekPDJG7BlLlvjMbefUqdIi6N4daNRIxGrZMuCKK2TU5oMP5lAIrncY\nQ5CeLhM0Cqy0zAN//SWfpigAMgKpbl3xI/LOOwAkC+ajbdvWClqpksxgT0qyonIFs7QK38yprZwb\nHKdUf/mlDyJUlOzojOYAcuedsh4z4L0oRERIYc0MTJtfQWq1gE/6Fi5eBN59V74/84x1vH59qZiW\nKiUTrJ980sVQTFMUHnlESsw1a4BPPsl3mvxG1pYCAERG2sUAEyYAR4/i779lDmGVKsA11zhH4Y0J\nafVqmcE+Zow0pvKFo/AvWgQkJuYzQkXJjopCAAkJkT6F666zZsl6g7ns87hxwMF7ngLKlJHqvGn8\nzsLhw2KqGjbMYR1pF0yfLgVg69ZAu3bO5264AViwQGZkv/02MHQo8PvvDuJgisKtt0oAAHjqqcLr\nIMgQBa57rfPxrl3FrXlyMjBmjJPpyJyZbuKNKHz0kXwyA998k4/0OjZZrrtOpsHnK0JFcY2KQoC5\n9VaxVjRr5v013buLw72UFGDUuBjw40/ICRd9CxkZMrBm0ybg44/FD5+rCVfp6dLBDEgrIWsBCAA3\n3yxWi5AQaQS0bg3UqAGM/j8b1v/OYEDU4557pHA9f16aFYWNs2dx+HQkhoZ+ipJ1q+D997Ocf+st\nICoK+OorrP1EbEOO/QkmnkTh7FkZkGUyd673Sdy6NUtfsmOTZcwYOTZ7tvcRKoq3eDNutTBtxWGe\ngi84coS5bFkZK//t50nM0dHMAG+eMsUp3NixEqZSJRmGDzCPG5c9vpkz5Vy9esyZme7vvXkz8xNP\nMF99tVxjbteEHuLx45kzMph53z7mEiXkxNKlvsu4Bzy9+3PnmMcOPMJRSLGnu0QJ5oMHswT89FNm\ngK/HLgaYf//ddVwAc2Qkc1pa9vNTpsj5G2+0nv2hQ57z8OWXEvbJJx0OzpghB+++W24cESHzLI4c\n8TrvxZ1gzr8v5ykEvJDP7aaiYDF1qrzBq69mvvDsq8wAn23c2H7+xx/lfGgo86pVzN99J98B5nff\nteLJzGSOjZXjn37q/f0zM5nXrmV+/KY/uQoO2wvZN980AkycKAdq1ZLJYgVATu8+NVXSdcUVloj1\nqbaOu3eX73fdlf2a00PGSKEs2gMDAAAgAElEQVSPS5x2+KTLeK+9Vq7ftMn5uM1mPdP58yV+p2fj\nhubNJWyNGhIPMzM/+KBzBL16yf7//ucx7wHlzBnmJUscMuI/CmX+CwgVBR88oOJARgZzy5byFh8f\nnsocEyM7o0fzv8t3c/nysvvaa9Y1ZoWTiHnuXDn2/fdyrGpV1zVejwwdypkgntVvsb3mvXcvM1++\nzNyggUT+3HO+yLJHXL37NWuYq1e3xKDDNft4PVowjxvHhw4xlyolx3/6yfm6RQsuS00fq5g7d2ZO\nT88Wd//+cu20D2zMf/9tb2atXcv2Ftrly/KsAeZWrdynf/16K50A8z//GCfq1WOnJsv8+bLftKnb\nvAecoUMlnS+/7PdbFcr8FxAqCj54QMWFLVssFwwbnviCGeDLCOM2WMMAc9e6ezjzrz1O17z2mrz5\n8HDx8NC2LWetdOaOhg0lgjVruF8/+dq5s1E5XLfOcifx/ffe1RgvXxb/EVOnGrYo73H17s3kNWgg\nrSfb3YZfkVmzmJn5jTdkt25daVGYPPecHH+65DvGl6ezxW2aiB686gf50qMH86VLPGiQ7D7zjIRL\nTmYuWVKOHTiQc/rN60JCDLGZxlLbzmqnunTJsh/u2pVj3gOO+fDDw5m3bfPrrQpl/gsIFQUfPKDi\nxOjRbK80bpj0Fj/daImYlXCIT8FoLjRrxvzee8wXL7LNxvz442x39wMwlyvHfOFCHm6elCQlWFgY\nc0oKnzzJXKGCxPnxx0aYESPYXvW9/nrm99+3nAg5cuwY84QJzJUrW+HHj89VcrK++1OnrHxeumQc\njIuTg+vXM7OUs2ZF/JVXrGs7dpRj373yp2V3mz/fCnDwIK+9ZTwDzI2w1Z7mczfdzSVK2Bhg3uOg\nx/feK0HeeMN12k+flnKfSPTHbtYy7YA33uh8wRDD/5XRSVTofveXL4sYmO+ycWM55icKXf4LEBUF\nHzyg4kRSEnO1avI2O3Q4YfQj2Hj1G+uY77+fuUwZ649ZsSLzyy9z5qkz9lo9wPyf/+Tx5vHxliIZ\nmB2lZcsyHz7MUv1+9VXnnunoaFGzffukNdGvn3MBUreulI4hIcwrV+YiOfFO+998I9F16mQcyMy0\nquxnz9rDrVjBTp3Oly9b/eQnT7I0owDxYvj779KDHxnJF1GCQ5HOoZTBF5evZa5QgafiEQaYu3Ry\nbuV8+62lz64wWyzdu4vZyBTrjGf/wy5bKj//zPY+G5ut8P3ud+xge6dXzZp5EvncUOjyX4CoKPjg\nARU3zH4Bc5s40eFkSgrznDlWDybAXKoUpz02hnt3u8i1axsFX14wO5NHjLAfstmYe/aUw7ff7mAx\nunxZ0tGmjXNizS0khPnOO6Wws9ks+02VKlLl94Ks737UqCxl0aFDljhm4b775FTv3pYn27p1HTJl\nVvUdt/vu47h6aab1jG0Jf3Kj0O0MMM+Je9WheSKvoXRpdu4rMMjMtMrNRYvkmLm/odnD8mXhQueL\nMjKsVtXvv/vud3/6tNwrv53D8+ZJ2m67zao8hIWJzdMPBOP/3kRFwQcPqDjSuzfba5ouh5XabFIl\nvuUWq1ALC2N+5BGXnaheYY6C+ewzp8OHDlkNFLND24k//pBWTHi4DAl65pnsbqvT060Ojx49vCqk\nsr77Ro3kcntjw2wStG2b7drDh61C2xS1QYMcAiQlWR3nbdvaO33NvtS33rI6iivQKU5FhDxrh5FX\nZsf0q68639u0ENWoYXWjPPSQETb8Bfly4kT2DP/f/zEDfGTI8zx+/Pb8D/LJzLRGL3z/ff7iesFI\n99ixsm8qdKNGeRrRsHChPPadO12fD9b/PbOKgk8eUHEkMZF59Ojd3vUNbN4sNV+zR3PmzLzdtEoV\nduzsdGTaNLZXyk+fzuH6lBT3duaDB8WG4mVPuOO7P3NGLFCRkQ4V9g8+kLgeeMDl9ZMmWXoJMH/4\nYZYAiYlS8juUvh9+KGH79bNGjo4edIr5yitl5+ab7cJgtuji4pyj7dFDjr/+unXMHLHUGT/Lwhau\n2LiRGeAO4TKw4MsvPT4i93z8sZX50aPzF5dZS/niC9lPTmauXZsd+0G8xWZjrl9fLm3RwvX4g2D9\n3zOrKPjkARVXcp33Tz5hu50klyN97KaY6GiXTZPMTKuz9v77cxe1EwsXSiTh4dLCcINj/hcsYKOf\nxSHAE09wdvuaxeXLVuEDMP/5p+fkbd4sYatVs4a37t7NYlM3hcFovqWm2ucZShiWbhVTvBytZKdO\nMRPZOAKpfLHfUNc3t9l4Z41u9vR26eI5vTly5gzbxzG76tjOLeYkjq1brWOrVklmQ0OzLRDlDkP7\n7Jur+oFP/veZmcxvv82ckJD/uAqQIrPIjlIEGDBAFprZsyd3fhgAy99Ry5YuF6UOCRHXGlFR4khv\n2bI8pvGOO8RNeHq6+Ozw0hHcr7/KZ4cODgddOcJzIDxcPMUCQPny3jkqbNBA8njokDgV7NDBcHRY\nvz4QHy8RLV4MfPwxIiPFESIAzJsnn9OmSVF3zz1AhQpWvBUqAE1iDuAyIrGmwp2ub06Ejyu/YN/9\n5Zd8rMHz/PPAmTPOfsFz9JXugdRUWaEpJMTy+giIU63HHxcnXIMGiQ8nL/j8c/ls3lw+//MfWW/I\n5/zwg6Svb195KUGIikKwEx4OPPecfP/vf917zMuKKQo33JBjkDp1pKwBgBkz8phGQBbBadIEJ/cl\n4aOb5uFCouc/rLlkbW5EAQA6dZKyYdEil1qXjfBwoHFja/+hhxxO1q8PvPeefH/qKeDQIdx7r+zO\nnStlp/lcRo7MHvdNmUsBAD9faOny3qmpwMxdUog3pAQwWwVorti0CfjwQ/HL/umnQO3a4kN9+/Y8\nRAZx6GWziSvyqCjnc6+8Isd37BAvvx5crKenW57Cp00D+vWTpA0b5odye8UK+dyxQ55JMOJNc6Iw\nbWo+ck+e8p6WZk35ddkrnAPt28s1P/zgNtiePWwNr8ylhcqR7z44whXpJAPMg9vtcRnGzP/Zs2Kl\niIhgvnjROJmWJmYLIqdRQb7A7EMtV85F1DYb8x132M1Il9Ns9m4Scz5C06Yu+tGPHOHl6MIAc5Mm\nrnuQZ882ri+1i3/CLTJCtdQxtv2wyPvBA5mZzDfcIBGZDpfM8coffJCr52Bn1iy53pX/EGYZhmwO\nQb73XudZg1kw+2Hq15dn5DgXZvp0K5xP/vfmQAKA+dFH8x9fAaHmI8W3RERYK/68/LJ3i+NkZMji\nxYDblgIgrYVatWTRsI15WEYpKUlcdd8xogpOcUUAwLw1VXDhXM6tmjVr5J/dsqWsWgoA2L9fWkLV\nq2evveaTnj3lc9QoF1ETyWLc0dHA4sUInzcbvXvLqTfekM9HHnHhmfa339AWaxEZchlbthBOncp+\nX9M197CnYtCkZRKuxmHsu3gVVt/2uiwA8cwzUmt3x2efSavvqqssT7vmO3VcPCk3mC2MBg1cn2/d\nGvjpJ3H7Pncu0KMHcOGCy6Bmy2fgQHlGFSta3tlHjwaOHs1bErNx6pSkOzRU9r/80mvzVrHCG+Uo\nTJu2FNyT57ynporzI0BmfHliqzGDt2ZNr6I3JzVPmJC7ZK1ebY3Xj4xkfmtyBreP+p0B5o+GbcgW\n3sz/k0/KNc8/73DSrHLeckvuEuElhw55GDVrdupfcQUvm3vWXiGNiXFozThiZKJLzb0y72GO8+nd\nu+X6kiVlUFR8fDw/++gFBpiHlJ1v1XgB5ltvde3q9exZq9ptjBJ65x3m66un8DQM44zrYz3mOz2d\n+euvZbSvHXNM77x57i/evFkcRElzSGa1Z0me6RDW0cOszWaN2OrVS47l+39vzqvo3Nma9e44g70Q\noy0FxfdERsq6kQDw0kuejbVe9Cc4cuut8vnTT94lJy1NktO+vVTwmzQRE+8To0Mx9N5kAMCM2ZE5\nptPsZO7Y0eGgF/0J+aFqVdfrUNgZPBi45Rbg7Fl0mjfC3qk8ZIhDa8YRY4WfmztlAAB+/tn5tLmW\ndt++QNmyxi1GlQEAzMvsjeTlv0kTq3RpYOlSoFUrqZE7NtfGjZOFudu3B/r1w9q1sirp7oMlMBwf\nouXumVi3LNlldphlvYiGDWV9j1tvdVhTyVNLwaRJE8lnnTrAli2ycMWePfbT8+bJctldusjzNSEC\nPvhAGhoLFvhovaH4ePns1EneFQDMnOmDiIsY3ihHYdq0peCefOX90iVr3kHW2bNZeeABts/Y8oLE\nRJknFxLi5F0iR/r2lehDQmRis+Ncp4unU7gsJTLAvH36b07XxcfH8/nzljum5GSHk+ZsMEe/4QXN\n/v32catTHtjGDRu6WMuBWd5FeDgzEW9cKbX/6tWtlkhqqlXBN1w42d+9OWHcPp/w9GnmZ5+1xssC\nMtX8iy/kQYWGMick8PnzMnnOrH1XizhmDz5woFWJt9nEo2yzZlZ0RA6PNilJdiIicpyDkpgok/6W\nLTMOnDhhzbivWFGmlDvk5fPPXT/O99+X85UqMX/33Wrv34MrTAdYa9ZIesLC5NkcP56/eAsAnafg\ngwdUXMl33t9+m3Pu+XTAXCxg3Tqvo+7QQS75+mv34Y4elbIqPFz+n654uMUmmV9V3dk8ER8fz4sW\nyX3atMkhAQW46I9L3n1X0nHVVTkr5Jo1EqZhQ87IsNaBMB3szZkj+3Fx1msy3705/8xpfgaz9NA+\n9ZTl1MncHn+cma3Z1s2aiQgnDx/N/8FLHBmWzoDMUH/xRWt8ASBeNt5/XwQIYG7dmq1p3Y0a5fgI\nTE+9JUsyb99uHExKsmbblynDe378iwHRMidxdyAzk7ldO7mkQoVUHjpUfl/eVDycOHbMSpBZA7n9\nds5xUkQhQ0XBBw+ouJLvvKekWDZe0wlPVhITpWoYHp6rUTyvyjpAPDSHeVgm5qziO+/MOcz65dJS\nqIgTnPb7Zvvx+Ph4fuopuf7ZZ7NcZPoJyupOo6DJzJSJYQDz4MGuw5je8R5+mJmZ+xjevs3BQJ07\ny/5771mXmO8+MdEq9/fudRH38ePiHiMqSpof58/zF19YZaI5qc48+E+XYXzbbc46Uq6czL42+0KS\nk62GyN6Jhm2+b1+XWbPZrInNgMybPH/eOJmWZp8JPa7sFAayuBpxwd9/W/1O5hYSIgI1frzMIfTI\nV1/JhTffbB0zPRg2bMhss3FKikM6vSSv3mNyi4qCDx5QccUneTc9grZs6bq1YPoPat48V9Fu2iSX\nVauWcyPEZrNGBS5YkHNcNhtzbPmjDDB/23ay/Xh8fDy3aCHXOy2ac+EC23ur8zMu1lfs3i1pMTs2\nFy92fih33innDPcjpiuN3r2lEASk4HcspBzf/YABEubFF92kITGR+cIF3rfPWprho48czptjiStV\nYrbZePFiSeq4ca4LR7Ol8d82hiMnRz/kDpi+8a6+2vJNdccdDpPiU1I484bWXAP7GGBe8aPnikdm\nJvOHH27gV1+VFlJYmCUQpUp5UZgPGyaBHZ1SpaXZZ3gfX57ANWvKc9qQfXyDS379VWavN28uefYn\nKgo+eEDFFZ/kPTlZ7LpmdTvLiBB7lX/kyFxFm5lpRZuTUzNTOMqX9+wz7c0XzjHA3BM/2N2OLlq0\nikNDxRTstGSDGXGs59E0BcZnn1ke+ABRw08/lQ4D0z2GYS/au1d2Y2Ks9TOGDHGOzvHdm161q1d3\nv+Z2erplt+/VK4tY22yW2wt3KwMZmE796pU6wDZA1n91gSke48bJazMXDHQsj39dKKOzquEgZ/a6\ny/PC4eyc/wsX5PZmN4Enk6XdJcdvzn1UPGoUpyGcb6zyj/01VazovE6GK7ZutYTW3G67LefffX5R\nUfDBAyqu+Czv77xj/ZrDwpjvuYf5l1+cJ2Ll1PvnBrNAyGmt4scek/OjRnmO6+RJ5jBK5xBk8JGB\n4olz4sRtDMhcLCdM84A5frGwcO6c2GHMDn6z1DE/HUrpWrXksDnnK2v55fjuMzOZr7lGwv3yS863\nnzBBwlSpkoPTwm7dJIAXkxovX7Y0ZCsaZfcPzmLrNxcSMq14Zh9QSIjV8Ww6Fnw2crJ8eeopj/d3\n9ds3PbtnFVAnjhyRQKVLZ+8Y37SJH8FU4xnZ7Ga72rVdO61lFgG/6ioJd9ddzP/9r6X9oaEyPDun\na/OKioIPHlBxxWd5t9mkQ/bOOy1PqoDUqEyPbn/9letozYmut96a/VxamjWixltfab1vlr6F18L+\nw3ziBPfte5ABFytnmqWfuT5mYSMtTUxF5vKVpk3FAdPC4WDmdiLrux83TsLm5Ixw3TprgvfPP+eQ\nrvHjJRJzprMHRgy5JM8/bLLL2r1Z13A03TOLmctsIe7aZbld3zljnWULyuay1hlXv/2EBLn0qqvc\nNDbMDpVu3bKd+vgjWUEvAqm8/vV4vnDBGnXVrFn2BQSPH7f6Szp3tiZqHz/OPHy49VcqUybnilFe\nUFHwwQMqrvgl74cOSeHguHJauXJ5WoTlxAm5PCrKaZkBZracocbGeh+1abKoi7/Y9tzzXL/+eQbE\nRO+E6bZhxoxcp7lAMcX4kUeyuWg151YBrkfVZn33psmpRAnpPjh1SubvPfecrERndka7rYQvWSKB\nXKw/4YrVU7eK2SfiWLZC2GazNC9rwyMz02qUmC5AWrQwTk6fzvZqtpuRY65++zabNSdz06YcLjSb\nJVnWSV271mqVfYLBdpE+ftxqtd16q9W4SEyU+XeADN5LTMx+qx07rHl9gPx+vSEjQ1rZObX6VBQ8\noKLgJ9LTpff33nvzVbiaf5ys/29zvZ6c1jDOKUlVKqSKEJS6m0NDMjkkxMUf0hwDn9MY1yKAuYZz\nqVJidcqKq3dvjsI1uyiybt26eei7OXPGUnEv1lfOnPo+X4MDDIiXbEf++EOiqlDBtaujM2ecRxFN\nnepwcuxYtlexn3tOenuz1Bxy+u2bLayXX84h0WbV3sEt+5Ejlglo1IPJIkhhYfYlCvfssax8998v\nFZxOnWS/Th3P5iHTrHXttd6tN2SOYq5SxfWAPxUFD6goFG7M/7ejReL0aamVhYTIPIXcYK7aWRt7\nnGuYJjab1euX53VHCwdr1+ZsWnP17k3LiNliaN9eLGgLF+ZiTlbduuy+qu3AyJE8Fq8yYB9Na8cs\nnN1ZorZsEf0pUSLLCqyZmdmXQ61aVZzWrVjBnJ6e42/fbIG2bu3i5L//ysmyZe3jR1NTLf+AHTsa\nWmhW76dMsV/6xx/Wct/mpL/KlWV9DE+kpTFfd51cM3my+7AHD1p9EjmNyFNR8EBRKBj9RVHI+8qV\nbDcTmZg1oa5dcx+fOXLS3MZ02excizx+XE7ExOR/3eFCTE7mk6VLRUi8qOi7xhzf+v77nsN26MAJ\naMCATLgza8FJSVbB5mnewPbtzuvy2MnMlJ7oRx5x7pg3bvbnSy+5jC8pyT45PPtS3zNnyvU9ezKz\nPC9zsv411zjUIb7+Wg5ed510wBjV9SVLrC6PmJjcrc2zeLGlRzkJtM0m6zOZndY5oaLggaJQMPqL\nopD3tDSrgDCdnJnWna++yluc7dvb7OXDD+ghdhGzybFqlZxo2dI3GSik+O3dm4rtaRaZwxDWBtdd\nlndheFWfMUOiyDbLPK9kZsrM6bFj7VXuS1demWNv8k03yf3NlUHtDB4sJ4xZy2YXSokS4qvPjqNf\nEdOcdvPNzG+8wfNe38ft2tl47drcZ8N06vfgg67Pm4PmoqPdt6B9KQrqEE8pcCIigM6d5fvSpcDO\nneKjLTpaFlnLCw8+KJ7oQsiGG6O3A0uWiKe2BQv87giv2OOtG+2TJ2Xltuho9BsYBsBaHMd03jd0\nqI/SFBIiftFfe01+QNWrI+rkSWD1apfBu3eXz8WLs5xwdIIHy5X5iy+Krz47kZGyAM+YMUBcnKxu\ntHw58PTT6PNMLaz66yq02fC2uJTPBW++KYs0ffJJ9jV9zpyRNYgAYPJkoHLlXEWdd7xRjsK0aUvB\nPUUl7++9JzWgPn2shWYeeijv8V28yNyqFXP37keZDx+2fOiYvXMAcw7mheKC3959Wpo1+9pVD7eJ\nOWOuTRvet0++lixpuUIqUyZnH0b55tln3f6I/vqL7SYt+4T2/fvZPtwpM9O+DnSZMl7MgD5xQqrx\nDz5oTQgBZHjVr7/mKuljxlitKEfr5qBBbO/X8GT11JaCUuQxXWn//DPwxRfyfdCgvMdXsiTw22/A\nU0/9BVx9tbQU3nlHVrwxV2HRlkLeiIiwqs0bNuQczsFdds2a4gU7JQXo318O9+sHlCrlpzSaN/n6\na5cL49StKyuMnj0L/PGHcdBsJbRvD4SEYPJk2X34YWm1uuXKK2W98OnTgQMHgO+/B2rWBP78U9Z/\n7d/f69V//vMfiW7dOuCrr+TYsmXitTsqShZScuuS3cf4TRSI6BMiOklELhd5JaKORJRIRFuN7QVX\n4ZTiSe3a4kL/3Dn579SpI4WIzwgJkWXQNm0CmjYV1fDpDYIMb0xIWdZQMMvpf/6RT5+ZjlwRG4vk\n2rWB8+dd2IikUM1mQnIwHR04IHoSFmaZbLyGCLjtNlnXecIEKcm//BK47jqx+6Snu708OlqsYADw\n9NNihXv4YdkfP14ErSDxZ0vhMwBdPYRZzcyNje0lP6ZFKYSYrQXAWmrR59SvLx0Wp04B1ar54QZB\nQsuW8ulOFHbskE9DFPr0sVa2jIsDmjXzY/oAnLj5Zvkye7bL806iwOwkCm+9JSu19u2bj59JiRLA\nCy9IH8eddwLJycBTTwF33SX3c8PgwfJ8jhwBWrSQxkfjxsCTT+YxLfnAb6LAzKsAnPVX/ErRx1EU\n7r/fjzciymFpM8VrHFsKrgo4ZqulEBsLQNZS7mpUCx96yP8mkBOdO8tNFi0CEhOzne/QQcrtzZuB\nY78fBA4fBsqXx9kqDTBjhoQZM8YHCalZUwY4LFkClCsH/PCDrIPthpAQsXYCwL//yv706dIJXdCE\nFfwtnWhNRNsAHAUwhpl3uApERMMADAOASpUqYeXKlW4jTU5O9himuFKU8h4eHoLY2DjUrp2MAwf2\n4MCB/MdZlPLva/yad2a0iY5GxKlT+H3OHKRmGQoTeeIEWicl4XJMDNbt3Cm1ZQCDB0cgNvYKXH/9\ncfj7tSSXKIFzjRuj3JYt2P3KKzhuNg0ciItriN9/L49PJ/yB5wCcio3Fs2MP4OLFWmje/CzOnk3w\nXTqjolBpxAjUe/VVZIwahQ2lSyOtYkW3l9x0Uz38/HMl9OnzL5KS9nmdFp++e296o/O6AagBYHsO\n58oCKG187w5gjzdx6ugj9wRz3pmDO/9+z7s5i8rVZBJzJlanTv5Ngxvi4+MtP0k5pMMc9XZXzHJm\ngFOnfGBfU2r5cj8kymazVnDr1s3jMKKUFHmUuV3yo1iMPmLmC8ycbHxfDCCciCoEKj2KonjANCG9\n/74M4XE0I2UxHQWMu+6SOQUrV4qBPgvdusnnsvMtkV69Dr4IG4wTJ8R+36WLH9JDBEybJmakJUtk\nSJEbSpSQNJp9MYEgYKJARFcRiZWRiFoaaTkTqPQoiuIBs7RavVoEonlz4OOPgYsXs408ChgxMUDP\nniJY5vhOB2pe2IZ62IkklMXqx77G5KlRAKQvwW99HpUrWx0GTzzhUqwKE/4ckvoVgN8AXEdEh4no\nQSIaTkTDjSB3A9hu9Cm8A+A+o4mjKEphpEULYPduKUHLl5ce22HDgCpVpDMVCLwoANZY2KyjkC5f\nBgYPRg/8CAAYPasxdu+W0Ub33FMAabrtNukAHzbM42ikQOLP0Ud9mbkyM4czc1VmnsHM05h5mnF+\nKjPHMnMcM7di5nX+SouiKD6iTh1g0iQZuTNrlsz9uHBBJpwAgTcfATL2NCYG2LrVGiYLAK++Cmzd\niu5XbQEgpwGpvPt9lA8R8OGHkq7Fiz2akQKJzmhWFCX3REUBAwYAa9cC27ZJyfruu1LoBZrISJkk\nAVitha1bgVdeAQC0/fxhlCkjh8uW9fOkOkeKiBlJRUFRlPzRqBHw1lvAo48GOiUWpgnpyy/F7cWg\nQeKs7tFHEXFzB9xyi5wePlyEocAYMMAyIz3wgPgBKWSoKCiKUvxo1046Cw4eBHr3BhISgFq1gIkT\nAYgF7OWXgXHjCjhdjqORli0TQS1k82pUFBRFKX6EhIjPCsBydvTJJ3aPfDVriiO60qUDkLYqVcQT\nZIMGwN694rb74YddzsIOBCoKiqIUTwYMsL6PGiV+LgoLTZuKs8YJE6SX+6OPxE+XOYorgKgoKIpS\nPGnYUNxbt29vuSEtTEREiAO9LVtk3sfRo8Dtt0sL5/TpgCVLRUFRlOLLV18Bv/7qx4UcfEBsrIzi\neustcdw4Z46Ylly4AC8IVBQURVECTWioDFP9809p2Zw4AfToAYwYITPGCxAVBUVRlMJCrVrAL7/I\nYtHh4TJSqUkTz+tj+xAVBUVRlMJEaKgszrNhg5iR9uwB2raVZdg8rOLmC1QUFEVRCiNxcSIMo0cD\nNpuMVLrxRuDSJb/eVkVBURSlsBIVJes8r1ghk/GaNBH/2n4k0CuvKYqiKJ7o1ElmZRfA+pwqCoqi\nKEWBAnI2qOYjRVEUxY6KgqIoimJHRUFRFEWxo6KgKIqi2FFRUBRFUeyoKCiKoih2VBQURVEUO8TM\ngU5DriCiUwAOeghWAUDgHJIHlmDOOxDc+Q/mvAPBnX9v8l6dmSt6iqjIiYI3ENFGZm4e6HQEgmDO\nOxDc+Q/mvAPBnX9f5l3NR4qiKIodFQVFURTFTnEVhY8CnYAAEsx5B4I7/8GcdyC48++zvBfLPgVF\nURQlbxTXloKiKIqSB4qdKBBRVyL6i4j+IaKxgU6PPyGiT4joJBFtdzh2BREtJ6I9xme5QKbRXxBR\nNSKKJ6KdRLSDiB43jgdL/qOI6A8i2mbkf4JxvCYRrTd+/3OJKCLQafUXRBRKRFuIaJGxH0x5P0BE\nfxLRViLaaBzzyW+/WAuzfY4AAATNSURBVIkCEYUCeA9ANwD1AfQlovqBTZVf+QxA1yzHxgJYwcx1\nAaww9osjGQBGM3N9AK0AjDTedbDkPw1AZ2aOA9AYQFciagXgdQBvMXMdAOcAPBjANPqbxwHsctgP\nprwDQCdmbuwwFNUnv/1iJQoAWgL4h5n3MfNlAHMA3BHgNPkNZl4F4GyWw3cAmGl8nwngzgJNVAHB\nzMeYebPxPQlSOFyN4Mk/M3OysRtubAygM4D5xvFim38iqgqgB4Dpxj4hSPLuBp/89oubKFwN4JDD\n/mHjWDBRiZmPGd+PA6gUyMQUBERUA0ATAOsRRPk3zCdbAZwEsBzAXgDnmTnDCFKcf/9TADwNwGbs\nl0fw5B2QCsAyItpERMOMYz757etynMUYZmYiKtbDy4ioNIBvADzBzBekwigU9/wzcyaAxkQUA2AB\ngOsDnKQCgYh6AjjJzJuIqGOg0xMgbmTmI0R0JYDlRLTb8WR+fvvFraVwBEA1h/2qxrFg4gQRVQYA\n4/NkgNPjN4goHCIIs5n5W+Nw0OTfhJnPA4gH0BpADBGZlb3i+vtvC+B2IjoAMRF3BvA2giPvAABm\nPmJ8noRUCFrCR7/94iYKGwDUNUYhRAC4D8D3AU5TQfM9gEHG90EAvgtgWvyGYUOeAWAXM7/pcCpY\n8l/RaCGAiEoAuBnSrxIP4G4jWLHMPzM/y8xVmbkG5D/+CzP3RxDkHQCIqBQRlTG/A7gFwHb46Ldf\n7CavEVF3iL0xFMAnzPxKgJPkN4joKwAdIR4STwB4EcBCAPMAXAPxJnsPM2ftjC7yENGNAFYD+BOW\nXfk5SL9CMOS/EaQzMRRSuZvHzC8RUS1I7fkKAFsADGDmtMCl1L8Y5qMxzNwzWPJu5HOBsRsG4Etm\nfoWIysMHv/1iJwqKoihK3ilu5iNFURQlH6goKIqiKHZUFBRFURQ7KgqKoiiKHRUFRVEUxY6KghK0\nEFGy8VmDiPr5OO7nsuyv82X8iuIvVBQUBagBIFei4DBzNiecRIGZ2+QyTYoSEFQUFAWYCKCd4Zv+\n/wxHc5OIaAMRJRDRw4BMlCKi1UT0PYCdxrGFhlOyHaZjMiKaCKCEEd9s45jZKiEj7u2GP/x7HeJe\nSUTziWg3Ec0mR0dOilJAqEM8RRG/82OYuScAGIV7IjO3IKJIAGuJaJkRtimABsy839h/gJnPGq4m\nNhDRN8w8logeZebGLu7VG7L+QRxkJvoGIlplnGsCIBbAUQBrIT5+1vg+u4qSM9pSUJTs3AJgoOGW\nej3ELXNd49wfDoIAAI8R0TYAv0OcMdaFe24E8BUzZzLzCQC/AmjhEPdhZrYB2AoxaylKgaItBUXJ\nDgEYxcxLnQ6Kn52LWfZvAtCamVOIaCWAqHzc19FPTyb0/6kEAG0pKAqQBKCMw/5SACMM19wgomsN\nb5RZiQZwzhCE6yHLgpqkm9dnYTWAe41+i4oA2gP4wye5UBQfoDURRQESAGQaZqDPIL75awDYbHT2\nnoLrpQ1/AjCciHYB+AtiQjL5CEACEW023DqbLICse7ANsnrW08x83BAVRQk46iVVURRFsaPmI0VR\nFMWOioKiKIpiR0VBURRFsaOioCiKothRUVAURVHsqCgoiqIodlQUFEVRFDsqCoqiKIqd/wdtxozz\nEMlFUQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "UQlUKtFYD5pc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Routines"
      ]
    },
    {
      "metadata": {
        "id": "VASgSU9KD-81",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Baseline Model"
      ]
    },
    {
      "metadata": {
        "id": "yNR6vmDhEBT6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model_baseline.train()\n",
        "\n",
        "model_baseline = train_model(model_baseline, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_wR099MzELCI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Pruned Model"
      ]
    },
    {
      "metadata": {
        "id": "0eNeKM87EN6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "dat = DatasetManager('cifar10', 1.0, 20.0)\n",
        "dat.ImportDataset(5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "model = train_model(model, dat, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}