\relax 
\citation{NIPS_learning_weights_pruning}
\citation{OBD}
\citation{OBS}
\citation{morphnet}
\citation{prune_transfer_learning}
\citation{prune_for_architecture}
\citation{metalearning1}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{prune_transfer_learning}
\citation{jacobgilblog}
\citation{prune_transfer_learning}
\@writefile{toc}{\contentsline {section}{\numberline {2}Formal Description}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}A Survey on Simple Pruning Algorithms}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Masking of Classifier Layers Based on a Threshold}{2}}
\newlabel{maskClass}{{2.1.1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Learned Masking of Classifier Layers}{2}}
\newlabel{maskClassL0}{{2.1.2}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Pruning Convolution Filters}{2}}
\newlabel{PruneFilter}{{2.1.3}{2}}
\newlabel{eqNp}{{1}{2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Helper functions for iterative convolution filter pruning - single dataset\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{FilterPruneBasic}{{1}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Computational functions for iterative convolution filter pruning - single dataset\relax }}{3}}
\newlabel{FilterPruneComps}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Pruning with Multiple Datasets}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Intersection Pruning of Classifier Layers Based on a Threshold}{3}}
\newlabel{intersectPruneClass}{{2.2.1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Meta-Pruning Classifier Layers}{3}}
\newlabel{metaPruneClass}{{2.2.2}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Intersection Pruning of Convolution Filters}{3}}
\newlabel{intersectPruneFilter}{{2.2.3}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Driver functions for iterative convolution filter pruning - single dataset\relax }}{4}}
\newlabel{FilterPruneDrivers}{{3}{4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Driver function for meta convolution filter pruning - multiple datasets\relax }}{4}}
\newlabel{FilterPruneDriverMeta}{{4}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Work}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Demonstration}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}A Study on Simple Pruning Algorithms}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Masking of Classifier Layers Based on a Threshold}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Learned Masking of Classifier Layers}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Pruning Convolution Filters}{5}}
\newlabel{PruneFilterRes}{{4.1.3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Pruning with Multiple Datasets}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Intersection Pruning of Classifier Layers Based on a Threshold}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Meta-Pruning Classifier Layers}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Intersection Pruning of Convolution Filters}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Limitations}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {7}List of Contributions}{5}}
\citation{NIPS_learning_weights_pruning,prune_transfer_learning,prune_entropy,prune_slimming}
\citation{prune_nisp}
\citation{prune_thinet}
\citation{prune_for_architecture}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Accuracy and loss of pre-trained VGG16 during fine-tuning. All layers were fine-tuned on 10\% of CIFAR-10 data ($4000$ training images, $1000$ validation images) for $50$ epochs. Top row: accuracy. Bottom row: loss. Left column: baseline model with no pruning. Middle column: activation-based pruning of $5$\% of all filters every $7$ epochs. Right column: weight-based pruning of $5$\% of all filters every $7$ epochs.\relax }}{6}}
\newlabel{pruneFiltersSingle}{{1}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Goals}{6}}
\citation{NIPS_learning_weights_pruning}
\citation{metalearning2}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Meta-pruning for weights\relax }}{7}}
\newlabel{alg1}{{5}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Meta-pruning for architecture\relax }}{7}}
\newlabel{alg2}{{6}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Methods}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Nice-to-haves}{7}}
\bibstyle{apalike}
\bibdata{references}
\bibcite{metalearning1}{{1}{2017}{{Finn et~al.}}{{}}}
\bibcite{jacobgilblog}{{2}{2017}{{Gildenblat}}{{}}}
\bibcite{morphnet}{{3}{2018}{{Gordon et~al.}}{{}}}
\bibcite{NIPS_learning_weights_pruning}{{4}{2015}{{Han et~al.}}{{}}}
\bibcite{OBS}{{5}{1993}{{{Hassibi} et~al.}}{{}}}
\bibcite{OBD}{{6}{1990}{{LeCun et~al.}}{{}}}
\bibcite{metalearning2}{{7}{2017}{{Li et~al.}}{{}}}
\bibcite{prune_slimming}{{8}{2017}{{Liu et~al.}}{{}}}
\bibcite{prune_for_architecture}{{9}{2019}{{Liu et~al.}}{{}}}
\bibcite{prune_entropy}{{10}{2017}{{Luo and Wu}}{{}}}
\bibcite{prune_thinet}{{11}{2017}{{Luo et~al.}}{{}}}
\bibcite{prune_transfer_learning}{{12}{2017}{{Molchanov et~al.}}{{}}}
\bibcite{prune_nisp}{{13}{2017}{{Yu et~al.}}{{}}}
