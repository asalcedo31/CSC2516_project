\relax 
\citation{NIPS_learning_weights_pruning}
\citation{OBD}
\citation{OBS}
\citation{morphnet}
\citation{prune_transfer_learning}
\citation{prune_for_architecture}
\citation{maml}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Outline of this paper.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{summaryFig}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Formal Description}{2}}
\citation{NIPS_learning_weights_pruning}
\citation{L0norm}
\citation{prune_transfer_learning}
\citation{jacobgilblog}
\citation{prune_transfer_learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}A Survey on Simple Pruning Algorithms}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Masking of Classifier Layers Based on a Threshold}{3}}
\newlabel{MaskClass}{{2.1.1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Learned Masking of Classifier Layers}{3}}
\newlabel{MaskClassL0}{{2.1.2}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Pruning Convolution Filters}{3}}
\newlabel{PruneFilter}{{2.1.3}{3}}
\newlabel{eqNp}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A. Baseline accuracy on CIFAR-10. B. Baseline loss on CIFAR-10. C. Fine-tuning accuracy of VGG-16 on fashion MNIST after training on CIFAR-10. D. Fine-tuning loss of VGG-16 on fashion MNIST after training on CIFAR 10. E. Threshold pruning accuracy on CIFAR-10. F. Threshold pruning loss on CIFAR-10. G. Fine-tuning accuracy of VGG16 on fashion MNIST after training on CIFAR-10 with threshold-pruning. H. Fine-tuning loss of VGG-16 on fashion MNIST after training on CIFAR-10 with threshold pruning.\relax }}{4}}
\newlabel{f1}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A. Iterative pruning accuracy subsampling CIFAR-10. B. Iterative pruning loss subsampling CIFAR-10. C. Iterative pruning accuracy subsampling CIFAR-10 re-initializing weights at each outer-iteration. D. Iterative pruning loss subsampling CIFAR-10 re-initializing weights at each outer-iteration. E. Iterative pruning accuracy subsampling multiple datasets. F. Iterative pruning loss subsampling multiple datasets\relax }}{5}}
\newlabel{f2}{{3}{5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Helper functions for iterative convolution filter pruning - single dataset\relax }}{6}}
\newlabel{FilterPruneBasic}{{1}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Computational functions for iterative convolution filter pruning - single dataset\relax }}{6}}
\newlabel{FilterPruneComps}{{2}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Pruning with Multiple Datasets}{6}}
\citation{reptile}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Driver functions for iterative convolution filter pruning - single dataset\relax }}{7}}
\newlabel{FilterPruneDrivers}{{3}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Driver function for meta convolution filter pruning - multiple datasets\relax }}{7}}
\newlabel{FilterPruneDriverMeta}{{4}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Intersection Pruning}{7}}
\newlabel{intersectPruneClass}{{2.2.1}{7}}
\newlabel{intersectPruneFilter}{{2.2.1}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Meta-Pruning}{7}}
\newlabel{metaPruneClass}{{2.2.2}{7}}
\citation{OBD}
\citation{OBS}
\citation{pred}
\citation{NIPS_learning_weights_pruning}
\citation{prune_transfer_learning}
\citation{NIPS_learning_weights_pruning}
\citation{L0norm}
\citation{morphnet}
\citation{maml}
\citation{reptile}
\citation{L0norm}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Work}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Demonstration}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}A Study on Simple Pruning Algorithms}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Masking of Layers Based on a Threshold}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Pruning Convolution Filters}{9}}
\newlabel{PruneFilterRes}{{4.1.2}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Filter pruning on a single dataset:} Accuracy and loss of pre-trained VGG16 during fine-tuning on $5$\% of CIFAR-10 data ($2000$ training images, $500$ validation images) for $50$ epochs. Top row: accuracy. Bottom row: loss. Left column: baseline model with no pruning. Middle column: activation-based pruning of $5$\% of all filters every $7$ epochs. Right column: weight-based pruning of $5$\% of all filters every $7$ epochs.\relax }}{10}}
\newlabel{pruneFiltersSingle}{{4}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Pruning with Multiple Datasets}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Intersection Pruning of Classifier Layers Based on a Threshold}{10}}
\citation{prune_for_architecture}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Meta-Pruning Classifier Layers}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Intersection Pruning of Convolution Filters}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A. Baseline accuracy on CIFAR-10 with pre-training. B. Baseline loss on CIFAR-10 with pre-training. C. Accuracy on CIFAR-10 with pre-training and L0 regularization. D. loss on CIFAR-10 with pre-training and L0 regularization. E. expected number of floating point operations in the classifier for CIFAR-10 with pre-training and L0 regularization. F. Final meta-pruning accuracy with L0 regularisation on CIFAR-10. G. Final expected number of floating point operations in the classifier with L0 regularisation on CIFAR-10.\relax }}{12}}
\newlabel{f3}{{5}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Iterative subsampling from CIFAR-10:} Accuracy and loss of pre-trained VGG16 during fine-tuning. Top row: accuracy. Bottom row: loss. Left column: baseline model with no pruning. Middle column: activation-based pruning of $5$\% of all filters every $7$ epochs. Right column: weight-based pruning of $5$\% of all filters every $7$ epochs.\relax }}{14}}
\newlabel{pruneFiltersIntersectionSubsample}{{6}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Limitations}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Iterative subsampling from CIFAR-10:} Accuracy and loss of pre-trained VGG16 during fine-tuning. Top row: accuracy. Bottom row: loss. Left column: baseline model with no pruning. Right column: weight-based pruning of $5$\% of all filters every $10$ epochs.\relax }}{15}}
\newlabel{pruneFiltersIntersectionSubsample2}{{7}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{15}}
\bibstyle{apalike}
\bibdata{references}
\bibcite{pred}{{1}{2013}{{Denil et~al.}}{{}}}
\bibcite{maml}{{2}{2017}{{Finn et~al.}}{{}}}
\bibcite{jacobgilblog}{{3}{2017}{{Gildenblat}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Iterative sampling from multiple datasets:} Accuracy and loss of pre-trained VGG16 during fine-tuning. Top row: accuracy. Bottom row: loss. Left column: baseline model with no pruning. Right column: weight-based pruning of $5$\% of all filters every $6$ epochs.\relax }}{16}}
\newlabel{pruneFiltersIntersectionMultiple}{{8}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {7}List of Contributions}{16}}
\bibcite{morphnet}{{4}{2018}{{Gordon et~al.}}{{}}}
\bibcite{NIPS_learning_weights_pruning}{{5}{2015}{{Han et~al.}}{{}}}
\bibcite{OBS}{{6}{1993}{{{Hassibi} et~al.}}{{}}}
\bibcite{OBD}{{7}{1990}{{LeCun et~al.}}{{}}}
\bibcite{prune_for_architecture}{{8}{2019}{{Liu et~al.}}{{}}}
\bibcite{L0norm}{{9}{2018}{{Louizos et~al.}}{{}}}
\bibcite{prune_transfer_learning}{{10}{2017}{{Molchanov et~al.}}{{}}}
\bibcite{reptile}{{11}{2018}{{Nichol et~al.}}{{}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Contributions of each author.\relax }}{17}}
\newlabel{contr}{{1}{17}}
