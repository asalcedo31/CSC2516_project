\relax 
\citation{NIPS_learning_weights_pruning}
\citation{OBD}
\citation{OBS}
\citation{morphnet}
\citation{prune_transfer_learning}
\citation{prune_for_architecture}
\citation{metalearning1}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{prune_transfer_learning}
\citation{jacobgilblog}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Outline of this paper.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{summaryFig}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Formal Description}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}A Survey on Simple Pruning Algorithms}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Masking of Classifier Layers Based on a Threshold}{2}}
\newlabel{maskClass}{{2.1.1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Learned Masking of Classifier Layers}{2}}
\newlabel{maskClassL0}{{2.1.2}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Pruning Convolution Filters}{2}}
\newlabel{PruneFilter}{{2.1.3}{2}}
\citation{prune_transfer_learning}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Helper functions for iterative convolution filter pruning - single dataset\relax }}{3}}
\newlabel{FilterPruneBasic}{{1}{3}}
\newlabel{eqNp}{{1}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Computational functions for iterative convolution filter pruning - single dataset\relax }}{4}}
\newlabel{FilterPruneComps}{{2}{4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Driver functions for iterative convolution filter pruning - single dataset\relax }}{4}}
\newlabel{FilterPruneDrivers}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Pruning with Multiple Datasets}{4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Driver function for meta convolution filter pruning - multiple datasets\relax }}{5}}
\newlabel{FilterPruneDriverMeta}{{4}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Intersection Pruning of Classifier Layers Based on a Threshold}{5}}
\newlabel{intersectPruneClass}{{2.2.1}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Meta-Pruning Classifier Layers}{5}}
\newlabel{metaPruneClass}{{2.2.2}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Intersection Pruning of Convolution Filters}{5}}
\newlabel{intersectPruneFilter}{{2.2.3}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Work}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Demonstration}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}A Study on Simple Pruning Algorithms}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Masking of Classifier Layers Based on a Threshold}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Learned Masking of Classifier Layers}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Pruning Convolution Filters}{6}}
\newlabel{PruneFilterRes}{{4.1.3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Pruning with Multiple Datasets}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Intersection Pruning of Classifier Layers Based on a Threshold}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Meta-Pruning Classifier Layers}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Intersection Pruning of Convolution Filters}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Limitations}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {7}List of Contributions}{6}}
\citation{NIPS_learning_weights_pruning,prune_transfer_learning,prune_entropy,prune_slimming}
\citation{prune_nisp}
\citation{prune_thinet}
\citation{prune_for_architecture}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Accuracy and loss of pre-trained VGG16 during fine-tuning. All layers were fine-tuned on 10\% of CIFAR-10 data ($4000$ training images, $1000$ validation images) for $50$ epochs. Top row: accuracy. Bottom row: loss. Left column: baseline model with no pruning. Middle column: activation-based pruning of $5$\% of all filters every $7$ epochs. Right column: weight-based pruning of $5$\% of all filters every $7$ epochs.\relax }}{7}}
\newlabel{pruneFiltersSingle}{{2}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Goals}{7}}
\citation{NIPS_learning_weights_pruning}
\citation{metalearning2}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Meta-pruning for weights\relax }}{8}}
\newlabel{alg1}{{5}{8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Meta-pruning for architecture\relax }}{8}}
\newlabel{alg2}{{6}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Methods}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Nice-to-haves}{8}}
\bibstyle{apalike}
\bibdata{references}
\bibcite{metalearning1}{{1}{2017}{{Finn et~al.}}{{}}}
\bibcite{jacobgilblog}{{2}{2017}{{Gildenblat}}{{}}}
\bibcite{morphnet}{{3}{2018}{{Gordon et~al.}}{{}}}
\bibcite{NIPS_learning_weights_pruning}{{4}{2015}{{Han et~al.}}{{}}}
\bibcite{OBS}{{5}{1993}{{{Hassibi} et~al.}}{{}}}
\bibcite{OBD}{{6}{1990}{{LeCun et~al.}}{{}}}
\bibcite{metalearning2}{{7}{2017}{{Li et~al.}}{{}}}
\bibcite{prune_slimming}{{8}{2017}{{Liu et~al.}}{{}}}
\bibcite{prune_for_architecture}{{9}{2019}{{Liu et~al.}}{{}}}
\bibcite{prune_entropy}{{10}{2017}{{Luo and Wu}}{{}}}
\bibcite{prune_thinet}{{11}{2017}{{Luo et~al.}}{{}}}
\bibcite{prune_transfer_learning}{{12}{2017}{{Molchanov et~al.}}{{}}}
\bibcite{prune_nisp}{{13}{2017}{{Yu et~al.}}{{}}}
